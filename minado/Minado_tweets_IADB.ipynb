{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "encouraging-column",
   "metadata": {},
   "source": [
    "# Motivación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-guinea",
   "metadata": {},
   "source": [
    "# Extracción de Tweets\n",
    "\n",
    "### El objetivo de esta notebook es revisar los pasos para realizar el minado de tweets. Para ello se tocan los siguientes puntos: \n",
    "### - Qué es la API de Twitter y cómo utilizarla\n",
    "### - Herramientas u operadores disponibles para definir queries \n",
    "### - Cómo obtener un dataset inicial de tweets \n",
    "### - Coómo refinar iterativamente los queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-prayer",
   "metadata": {},
   "source": [
    "# Requerimientos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-substitute",
   "metadata": {},
   "source": [
    "\n",
    "El minado de tweets con Python requiere de descargar previamente algunos programas así como obtener credenciales de la API de Twitter. Cabe señalar que para un minado masivo de tweets, es posible utilizar una credencial de API académica (a la que el Banco tiene acceso), pero para hacer algunas pruebas, cualquier persona puede acceder a una credencial para realizar este minado.\n",
    "\n",
    "### Credenciales para utilizar la API de Twitter\n",
    "\n",
    "Twitter provee una API (interfaz de programación de aplicaciones) para acceder a sus datos. Las API son mecanismos que permiten a dos componentes de software comunicarse entre sí mediante un conjunto de definiciones y protocolos. Por ejemplo, el sistema de software del instituto de meteorología contiene datos meteorológicos diarios. La aplicación meteorológica de su teléfono “habla” con este sistema a través de las API y le muestra las actualizaciones meteorológicas diarias en su teléfono. [Aqui una explicación de AWS](https://aws.amazon.com/es/what-is/api/).\n",
    "\n",
    "Para obtener tweets mediante la [API Oficial de Twitter](https://developer.twitter.com/en/docs/twitter-api) es necesario registrarse en el [portal para desarrolladores de Twitter](https://developer.twitter.com/en/docs/developer-portal/overview) para así dar de alta un proyecto y obtener las [credenciales necesarias](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api).\n",
    "\n",
    "Es importante señalar que **el tipo de credenciales y el método de autenticación empleado afectan la [cantidad de tweets que podremos obtener en cierto tiempo -días hacia atrás; cantidad máxima de tweets por mes](https://developer.twitter.com/en/docs/twitter-api/rate-limits#v2-limits), así como los [operadores/filtros que podemos usar](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list) y la extensión de los queries**.\n",
    "\n",
    "Por ejemplo, los límites mensuales de obtención de tweets, dependiendo de la credencial empleada, son los siguientes:\n",
    "- *Essential*: 500k tweets en un mes y 512 caracteres por query.\n",
    "- *Elevated*: 2M tweets en un mes y 512 caracteres por query.\n",
    "- *Academic*: 10M tweets en un mes y 1024 caracteres por query.\n",
    "\n",
    "Cualquier otra diferencia entre nivel de credenciales, se puede [ver aquí](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level)\n",
    "\n",
    "Notas:\n",
    "- Debido a que la webapp del Laboratorio de Migración solo necesita permisos de lectura para contenido público, nos interesa obtener la credencial de tipo **Bearer Token**, ya que es el método de autenticación que más tweets nos permite obtener en menos tiempo.\n",
    "- Se usará la versión 2 de la [API de Twitter](https://developer.twitter.com/en/docs/twitter-api).\n",
    "- El ejercicio aquí propuesto puede realizarse con las credenciales de nivel [*Elevated*](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level), pero en caso de estar trabajando en un proyecto con fines académicos o de estudio, se puede [aplicar de manera gratuita a credenciales Académicas](https://developer.twitter.com/en/products/twitter-api/academic-research) y así trabajar con límites más amplios y operadores avanzados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-portrait",
   "metadata": {},
   "source": [
    "### Dependencias de Python\n",
    "Las dependencias que se utilizan para la extracción de tweets son `pandas`, `numpy` y  `twarc`. Las primeras dos se utilizan para manipular y trabajar con los datos, mientras que [`twarc` es el paquete](https://developer.twitter.com/en/docs/twitter-api/rate-limits)  que nos facilitará el proceso de minado, ya que se encarga de obtener automáticamente todos los atributos de los tweets así como de manejar los tiempos de espera cuando se llega a los límites de minado [de la API](https://developer.twitter.com/en/docs/twitter-api/rate-limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attached-rabbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twarc==2.9.2\n",
      "  Downloading twarc-2.9.2-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 1.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: click-plugins>=1 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from twarc==2.9.2) (1.1.1)\n",
      "Collecting click-config-file>=0.6\n",
      "  Downloading click_config_file-0.6.0-py2.py3-none-any.whl (6.0 kB)\n",
      "Collecting humanize>=3.9\n",
      "  Downloading humanize-4.4.0-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click<9,>=7 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from twarc==2.9.2) (8.1.3)\n",
      "Collecting requests-oauthlib>=1.3\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from twarc==2.9.2) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.62 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from twarc==2.9.2) (4.64.1)\n",
      "Collecting configobj>=5.0.6\n",
      "  Downloading configobj-5.0.6.tar.gz (33 kB)\n",
      "Requirement already satisfied: six in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from configobj>=5.0.6->click-config-file>=0.6->twarc==2.9.2) (1.16.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from requests-oauthlib>=1.3->twarc==2.9.2) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (3.3)\n",
      "Building wheels for collected packages: configobj\n",
      "  Building wheel for configobj (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for configobj: filename=configobj-5.0.6-py3-none-any.whl size=34528 sha256=18d7be81f0d7569b76771d8ccd82b7fe5d9c25954885b1ddd735b4b06ed765a8\n",
      "  Stored in directory: /Users/aleistermontfort/Library/Caches/pip/wheels/90/cd/58/816b023d3eb0e2c9c6e0cd834a10a53e932e29d7c719298216\n",
      "Successfully built configobj\n",
      "Installing collected packages: oauthlib, configobj, requests-oauthlib, humanize, click-config-file, twarc\n",
      "Successfully installed click-config-file-0.6.0 configobj-5.0.6 humanize-4.4.0 oauthlib-3.2.2 requests-oauthlib-1.3.1 twarc-2.9.2\n",
      "Collecting pandas==1.4.1\n",
      "  Downloading pandas-1.4.1-cp310-cp310-macosx_10_9_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from pandas==1.4.1) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from pandas==1.4.1) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from pandas==1.4.1) (1.22.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.4.1) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.4\n",
      "    Uninstalling pandas-1.4.4:\n",
      "      Successfully uninstalled pandas-1.4.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "skforecast 0.4.3 requires numpy<=1.22,>=1.20, but you have numpy 1.22.2 which is incompatible.\n",
      "skforecast 0.4.3 requires pandas<=1.4,>=1.2, but you have pandas 1.4.1 which is incompatible.\n",
      "skforecast 0.4.3 requires tqdm<=4.62,>=4.57.0, but you have tqdm 4.64.1 which is incompatible.\u001b[0m\n",
      "Successfully installed pandas-1.4.1\n",
      "Collecting numpy==1.22.3\n",
      "  Downloading numpy-1.22.3-cp310-cp310-macosx_10_14_x86_64.whl (17.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.6 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.2\n",
      "    Uninstalling numpy-1.22.2:\n",
      "      Successfully uninstalled numpy-1.22.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "skforecast 0.4.3 requires numpy<=1.22,>=1.20, but you have numpy 1.22.3 which is incompatible.\n",
      "skforecast 0.4.3 requires pandas<=1.4,>=1.2, but you have pandas 1.4.1 which is incompatible.\n",
      "skforecast 0.4.3 requires tqdm<=4.62,>=4.57.0, but you have tqdm 4.64.1 which is incompatible.\n",
      "numba 0.56.2 requires setuptools<60, but you have setuptools 65.5.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.22.3\n",
      "Collecting tqdm==4.62.2\n",
      "  Downloading tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 2.0 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "skforecast 0.4.3 requires numpy<=1.22,>=1.20, but you have numpy 1.22.3 which is incompatible.\n",
      "skforecast 0.4.3 requires pandas<=1.4,>=1.2, but you have pandas 1.4.1 which is incompatible.\n",
      "skforecast 0.4.3 requires tqdm<=4.62,>=4.57.0, but you have tqdm 4.62.2 which is incompatible.\u001b[0m\n",
      "Successfully installed tqdm-4.62.2\n",
      "Requirement already satisfied: pendulum==2.1.2 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: python-dateutil<3.0,>=2.6 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from pendulum==2.1.2) (2.8.2)\n",
      "Requirement already satisfied: pytzdata>=2020.1 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from pendulum==2.1.2) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aleistermontfort/opt/anaconda3/envs/data_test/lib/python3.10/site-packages (from python-dateutil<3.0,>=2.6->pendulum==2.1.2) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Indispensables\n",
    "!pip install twarc==2.9.2\n",
    "!pip install pandas==1.4.1\n",
    "!pip install numpy==1.22.3\n",
    "\n",
    "# Para mejorar la interacción gráfica\n",
    "!pip install tqdm==4.62.2\n",
    "!pip install pendulum==2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-record",
   "metadata": {},
   "source": [
    "## Imports & Credenciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optical-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from twarc.client2 import Twarc2\n",
    "from twarc.expansions import TWEET_FIELDS\n",
    "from twarc.expansions import ensure_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-population",
   "metadata": {},
   "source": [
    "Ingresa tu propio Bearer Token o comentalo, y descomenta el resto de atributos para con tus propias API Keys y Access Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vocational-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_TWITTER = {\n",
    "    'bearer_token': \"Enter your own Bearer token\",\n",
    "\n",
    "    # 'api_key': \"Enter your own API Key\",\n",
    "    # 'api_secret_key': \"Enter your own API Secret Key\",\n",
    "    # 'access_token': \"Enter your own access_token\",\n",
    "    # 'access_token_secret': \"Enter your own access_token_secret\"\n",
    "}\n",
    "\n",
    "CREDENTIALS_TWITTER = {\n",
    "    'bearer_token': \"AAAAAAAAAAAAA0AAAAAAAAFBIPwEAAAAAxt1ACpkLKEy%2FwL4KvhyxKCLAIbA%3D1ORuBxC0mD6fY47zNqdNAXICGQboR5EQu0hxie6P3EAeEFo6ZW\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-salvation",
   "metadata": {},
   "source": [
    "## Minado de Tweets\n",
    "\n",
    "La API de Twitter permite obtener diferentes piezas de información a partir de los usuarios y los tweets que publican, según el tipo de operadores y queries utilizados. Para fines de esta notebook, nos centraremos en obtener tweets públicos en un intervalo de tiempo definido, a partir del contenido de palabras claves.\n",
    "\n",
    "Las palabras claves del tema que se desea minar se utilizan para construir queries que hacen la búsqueda más certera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-editing",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-framing",
   "metadata": {},
   "source": [
    "\n",
    "### Endpoints & Limites\n",
    "\n",
    "Los endpoints son el punto de comunicación entre el código de minado y la API de Twitter. Por medio de ellos se especifica que tipo de minado se quiere hacer, ademas de comunicar los queries y filtros que se desea utilizar para minar tweets.\n",
    "\n",
    "Para [buscar tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction) con la API de Twitter existen dos endpoints: [Recent Search](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/recent-search) y  [Full Archive](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/full-archive-search).\n",
    "- **Recent Search**: Nos permite realizar 450 requests (pedidos) a la API en una ventana de 15 minutos con un máximo de 100 tweets por request. Sin embargo, solo se pueden obtener tweets publicados en los últimos 7 días.\n",
    "- **Full Archive**: Podremos obtener tweets publicados desde el inicio de la red social, pero solo se puede acceder a 300 requests en una ventana de 15 minutos, con hasta 500 tweets por cada request. Hay que aplicar a las [credenciales academicas](https://developer.twitter.com/en/products/twitter-api/academic-research) para poder utilizar este endpoint.\n",
    "\n",
    "PREGUNTA: un request cuántos tweets te permite jalar? Un request == 1 query?\n",
    "\n",
    "**Nota**: No hay que preocuparse del código de error que aparece una vez que el minado alcanza estos límites, pues el paquete twarc se encarga de pausar la obtención de tweets una vez que se llega al limite de 450 (o 300) requests en la ventana de 15 minutos; una vez que pasa un tiempo necesario, twarc reanuda el proceso. Si se llega al límite de tweets en un mes, la función para.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-pharmaceutical",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-meditation",
   "metadata": {},
   "source": [
    "\n",
    "### Operadores\n",
    "\n",
    "En esta sección vamos a ver de manera general qué son  y cómo funcionan los operadores, pero la lista y descripción completa de los mismos se puede [encontrar aquí](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query).\n",
    "\n",
    "Existen dos tipos de operadores: \n",
    "- **Standalone**: Se pueden usar solos o junto con otros. Por ejemplo, buscar tweets con un hashtag en especifico: `#migrantes`.\n",
    "- **Conjunction-required**: Es necesario que estén junto a mínimo un operador *standalone*. Por ejemplo, buscar tweets con un hastag en especifico pero que incluyan imagenes y sean retweets: `#migrantes has:media is:retweet`\n",
    "\n",
    "Además, como se mencionó en la sección de \"Credenciales para la API de Twitter\", existen operadores *core*, que son accesibles con cualquier nivel de acceso, así como los operadores *advanced*, que solo se pueden utilizar con un acceso académico. \n",
    "\n",
    "#### Operadores Lógicos\n",
    "\n",
    "- **AND**: Obtiene tweets que cumplan con los dos operadores, se logra dejando un espacio en blanco entre ellos. \n",
    "    - Ejemplo, obtener tweets que contienen la palabra *politicos* y el hashtag *#corruptos*: `politicos #corruptos`.\n",
    "- **OR**: Obtiene tweets que cumplan con alguno de los dos operadores. Hay que añadir el string \" OR \" entre los operadores. \n",
    "    - Ejemplo, tweets que contengan la palabra *migrantes* o *inmigrantes*: `migrantes OR inmigrantes`.\n",
    "- **NOT**: Obtiene tweets que no contengan el operador o la keyword negada. Se logra añadiendo un guión medio \"-\" antes del operador. \n",
    "    - Ejemplo, obtener tweets con la palabra *politicos* pero sin la palabra *corruptos*: `politicos -corruptos`\n",
    "    - Ejemplo, obtener tweets con la palabra *migrantes pero que no sean retweets*: `migrantes -is:retweets`\n",
    "- **Grouping**: Sirve para agrupar operadores logicos, y hay que encerrar los operadores entre paréntesis. Un grupo no puede ser negado.\n",
    "    - Ejemplo, obtener tweets con la palabra *migrantes* y alguna de las palabras *llegan* o *salen*: `migrantes AND (llegan OR salen)`\n",
    "\n",
    "**Nota**: A menos de que haya paréntesis para especificar el orden de operadores, primero se resuelven aquellos que son *AND* y luego los *OR*. [Más aquí](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#boolean).\n",
    "\n",
    "#### Más Operadores\n",
    "\n",
    "- **keyword**: Hace match con tweets que contengan un string en especifico (no se pueden usar wildcards ····· DEFINIR WILDCARDS······). No es sensible a caracteres en mayúsculas o minúsculas; acentos caracteres especiales como ñ. Ejemplo: `migrantes OR inmigrantes`.\n",
    "- **\"exact phrase\"**: Parecido al anterior, permite considerar espacios y múltiples tokens. Tiene que estar entre comillas dobles. Ejemplo: `\"ola migrante\"`.\n",
    "- **#**: Hace match a tweets que tengan el hashtag incluido. Ejemplo: `#migraresunderecho`.\n",
    "- **@**: Hace match a tweets que mencionen a los usuarios incluidos. Ejemplo: `@IADB`.\n",
    "- **place_country**: Obtiene tweets que sean geolocalizables a cierto pais. Hay que pasarle el código [ISO del país](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2). Ejemplo: `place_country:MX`.\n",
    "- **lang**: Obtiene tweets que esten escrito en el lenguaje indicado y tiene que estar junto con un standalone operator. Ejemplo: `migrantes lang:es`.\n",
    "- **is:retweet**: Obtiene solo tweets que sean retweets y tiene que estar junto con un standalone operator. Ejemplo: `migrantes is:retweet`.\n",
    "\n",
    "La lista completa de operadores esta [aquí](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-suspension",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-austin",
   "metadata": {},
   "source": [
    "#### Ejercicio de Minado\n",
    "\n",
    "La siguiente función recibe una lista de queries y un intervalo de tiempo para guardar los tweets en un archivo JSONL.\n",
    "\n",
    "**Nota**: Un archivo de extension .jsonl guarda un objeto json por cada linea separados por solo el salto de linea, no comas u otro separador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(twarc_config,\n",
    "               queries_list,\n",
    "               output_file,\n",
    "               since_date, until_date,\n",
    "               is_academic=False):\n",
    "    \"\"\"Function to scrape tweets from the \n",
    "    official Twitter API, using the library \"Twarc\".\n",
    "\n",
    "    Args:\n",
    "        twarc_config (dict): Dictionary with the Twitter API credentials.\n",
    "        queries_list (list[str]): List of queries we want to scrape.\n",
    "        output_file (str): Path to the file where we store the results.\n",
    "        since_date (datetime): Initial date to scrape.\n",
    "        until_date (datetime): Last date to scrape.\n",
    "        is_academic (bool, optional): If the credentials have Research \n",
    "                                      Academic access level.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instiate the Twarc Client\n",
    "    twarc_client = Twarc2(**twarc_config)\n",
    "\n",
    "    # Make some tweaks for using the research credentials\n",
    "    max_size = 100\n",
    "    tweet_fields = TWEET_FIELDS.copy()\n",
    "    search_func = twarc_client.search_recent\n",
    "    if(is_academic):\n",
    "        search_func = twarc_client.search_all\n",
    "        max_size = 500\n",
    "\n",
    "        # Remove the context_annotations attr to\n",
    "        # scrape 500 tweets per request\n",
    "        tweet_fields.remove('context_annotations')\n",
    "\n",
    "    tweet_fields = ','.join(tweet_fields)\n",
    "\n",
    "    # Keep in memory the wanted attrs\n",
    "    return_values = []\n",
    "\n",
    "    with open(output_file, 'a') as pages_file:\n",
    "        for query in tqdm(queries_list):\n",
    "\n",
    "            search_results = search_func(query=query,\n",
    "                                         start_time=since_date,\n",
    "                                         end_time=until_date,\n",
    "                                         tweet_fields=tweet_fields,\n",
    "                                         max_results=max_size)\n",
    "\n",
    "            # Write all the obtained tweets\n",
    "            for page in search_results:\n",
    "\n",
    "                # Write one by one the tweets\n",
    "                for tweet in ensure_flattened(page):\n",
    "                    json.dump(tweet, pages_file)\n",
    "                    pages_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-tuner",
   "metadata": {},
   "source": [
    "Se declara la lista de queries, el intervalo de fecha (para fines de la prueba, es 6hrs hacia atras de la hora actual) y si las credenciales que se estan utilizando tienen un nivel de acceso academico. Esto ultimo es util para pedir que cada request tenga 500 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_queries = ['migrantes', 'inmigrantes', 'emigrantes']\n",
    "\n",
    "date_start = pendulum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-distinction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-calculator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-halifax",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCE",
   "language": "python",
   "name": "bce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
