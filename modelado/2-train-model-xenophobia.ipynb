{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de clasificación binaria para la detección de xenofobia en tweets de migración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias útiles\n",
    "\n",
    "##### Hugging Face\n",
    "Se trata de una comunidad y plataforma de ciencia de datos que ofrece una gran cantidad de herramientas para la creación y evaluación de modelos de aprendizaje profundo. Entre ellas destacan [[Omer Mahmood @ Towards Data Science](https://towardsdatascience.com/whats-hugging-face-122f4e7eb11a#:~:text=Hugging%20Face%20is%20a%20community,(OS)%20code%20and%20technologies.)]:\n",
    "* Herramientas que permiten a los usuarios construir, entrenar y desplegar modelos basados en código abierto\n",
    "* Un lugar donde una amplia comunidad de científicos de datos, ingenieros de aprendizaje profundo e investigadores pueden reunirse para compartir ideas, obtener apoyo, contribuir a los proyectos e incluso compartir sus modelos entrenados o puros.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/images/hug.png\"\n",
    "         alt=\"Hugging Face logo\"\n",
    "         style=\"width: auto; height: 100%\">\n",
    "    <figcaption>Hugging Face logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "No se encontró ningún modelo entrenado para la tarea de interés, por lo que se entrenaron y probaron 12 modelos a modo de seleccionar el mejor de ellos. La selección de estos 12 modelos consistió en tomar aquellos que estuviesen entrenados para una tarea similar a la xenofobia (en este caso fue el discurso de odio) y/o que hayan sido entrenados para comprender el idioma español.\n",
    "\n",
    "De este estudio el modelo seleccionado fue [RoBERTuito-base-uncased](https://arxiv.org/abs/2111.09453)\n",
    "\n",
    "##### PyTorch\n",
    "Es una librería de aprendizaje automático de código abierto *que acelera el camino desde la creación de prototipos de investigación hasta el despliegue de producción [[PyTorch](https://pytorch.org/)].*\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/images/pytorch.png\"\n",
    "         alt=\"PyTorch logo\"\n",
    "         style=\"width: auto; height: 100%\">\n",
    "    <figcaption>PyTorch logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "Algunos de los modelos disponibles en Hugging Face se encuentran implementados sobre esta librería. De este modo, algunas de las herramientas disponibles en PyTorch son fácilmente adaptables con Hugging Face, esto nos permitirá añadir o modificar la estructura de una red neuronal, así como su comportamiento, permitiendo al usuario implementar varias funciones personalizadas.\n",
    "\n",
    "##### Scikit learn\n",
    "Se trata de una librería de código abierto enfocada en proveer herramientas de aprendizaje de máquinas tales como modelos estadísticos y matemáticos, así como métricas de evaluación comunes en algoritmos de aprendizaje de máquinas.\n",
    "<figure>\n",
    "    <img src=\"./assets/images/scikit.png\"\n",
    "         alt=\"scikit-learn logo\"\n",
    "         style=\"width: auto; height: 50%\">\n",
    "    <figcaption>scikit-learn logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "Esta librería nos permitirá implementar de manera sencilla las métricas de evaluación del modelo de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#HuggingFace library\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments, TextClassificationPipeline\n",
    "from datasets import Dataset, Value, ClassLabel, Features\n",
    "\n",
    "#PyTorch Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#data reading\n",
    "import pandas as pd\n",
    "\n",
    "#math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#scikit-learn metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, recall_score, accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando el modelo preentrenado\n",
    "\n",
    "Para usar cualquier modelo transformer de la biblioteca Hugging Face, se debe importar la clase `AutoModelForSequenceClassification` de la biblioteca `transformers`. Esta clase se encarga de cargar el modelo preentrenado y de configurar la capa de clasificación para la tarea de clasificación de secuencias. Además debe conocerse el nombre del modelo que se desea cargar, en este caso se usará el modelo `robertuito-base-uncased` que se encuentra disponible en la [biblioteca Hugging Face](https://huggingface.co/models).\n",
    "\n",
    "La búsqueda de los modelos preentrenados se hace por lo regular a través del tipo de tarea a resolver la siguiente imagen muestra algunas tareas en hugging face.\n",
    "\n",
    "<img src=\"./assets/images/hug_tasks.png\"\n",
    "         alt=\"Algunas tareas en hugging face\"\n",
    "         style=\"width: auto; height: 60%\">\n",
    "\n",
    "Al seleccionar la tarea de interés, la página muestra los modelos disponibles como en la siguiente imagen.\n",
    "\n",
    "<img src=\"./assets/images/hug_models.png\"\n",
    "         alt=\"Algunos modelos disponibles\"\n",
    "         style=\"width: auto; height: 60%\">\n",
    "\n",
    "En caso de conocerse el nombre del modelo o bien el nombre del autor, se puede buscar directamente en la barra de búsqueda.\n",
    "\n",
    "<img src=\"./assets/images/hug_search.png\"\n",
    "         alt=\"busqueda por auutor o nombre\"\n",
    "         style=\"width: auto; height: 60%\">\n",
    "\n",
    "Como ya se mencionó se realizó la búsqueda y entrenamiento de 12 modelos relacionados con el discurso de odio en español o modelos preentrenados en español. El modelo seleccionado fue `pysentimiento/robertuito-base-uncased`. Para cargarlo se usa la clase `AutoModelForSequenceClassification` de la biblioteca `transformers` a la cual se le pasa como parámetro el nombre del modelo y la cantidad de etiquetas a utilizar (en este caso fueron 2 xenofóbico y no xenofóbico).\n",
    "\n",
    "También debe cargarse el tokenizador que se usó para entrenar originalmente el modelo. Un tokenizador es un algoritmo que divide un texto en unidades más pequeñas, llamadas tokens. Los tokenizadores son necesarios para que los modelos de lenguaje puedan procesar texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#set model name\n",
    "model_name = \"pysentimiento/robertuito-base-uncased\"\n",
    "#download model from huggingface\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2)\n",
    "\n",
    "#Load tokenizer\n",
    "#Un tokenizer es un objeto que convierte una secuencia de caracteres en una secuencia de números.\n",
    "#es una especie de filtro que prepara el texto para que el modelo lo pueda entender.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando el modelo\n",
    "\n",
    "Para agregar los tokens especiales como @usuario, url, etc agregados en el notebook anterior, se debe configurar el tokenizador para que reconozca estos tokens. Esto se realiza con la función `add_tokens` de la clase `AutoTokenizer` la cual recibe como parámetros una lista con los tokens especiales que se desean agregar. La documentación de esta función solicita que después de haber agregardo los tokens especiales se debe redimensionar el tokenizador, esto se hace con la función `resize_token_embeddings`.\n",
    "\n",
    "Puesto que el modelo fue entrenado para aceptar una longitud máxima de 128 tokens, se debe configurar el tokenizador para que acepte esta longitud máxima. Esto se hace con la función `model_max_length`.\n",
    "\n",
    "Finalmente se puede mapear las etiquetas a cadenas de texto, lo cual se realiza al modificar los atributos del modelo `id2label` y `label2id`. Si bien esta funcionalidad no es necesaria para realizar el entrenamiento, es útil para poder interpretar los resultados del modelo al momento de inferir conjuntos de texto pequeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config model: assign names to each label\n",
    "model.config.id2label = {0: 'ok', 1:'hateful'}\n",
    "id2label = {0: 'ok', 1:'hateful'}\n",
    "model.config.label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "#Add special tokens to the tokenizer\n",
    "tokenizer.add_tokens(['@usuario', 'url', 'hashtag', 'emoji'])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.model_max_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos procesados\n",
    "\n",
    "Para proceder a entrenar el modelo se debe procesar el conjunto de datos. Para esto primero se debe leer el conjunto de datos procesados en el notebook anterior. Para esto se usa la función `read_csv` de la biblioteca `pandas` la cual recibe como parámetro el nombre del archivo csv que contiene los datos procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4670</th>\n",
       "      <td>1256373527420317696</td>\n",
       "      <td>@usuario señor presidente, que más hace falta ...</td>\n",
       "      <td>2021-09-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1348643518563155968</td>\n",
       "      <td>Y TAMBIÉN LOS MIGRANTES ¡¡ ILEGALES !! url</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>1388760443087466496</td>\n",
       "      <td>@usuario @usuario Sabia usted que por toda la ...</td>\n",
       "      <td>2021-10-13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>1087215634075799552</td>\n",
       "      <td>@usuario @usuario No dijo salgan a golpear, a ...</td>\n",
       "      <td>2021-11-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>1393254552145641472</td>\n",
       "      <td>emoji ojos emoji Te invitamos a conocer esta i...</td>\n",
       "      <td>2021-08-26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "4670  1256373527420317696  @usuario señor presidente, que más hace falta ...   \n",
       "373   1348643518563155968         Y TAMBIÉN LOS MIGRANTES ¡¡ ILEGALES !! url   \n",
       "1164  1388760443087466496  @usuario @usuario Sabia usted que por toda la ...   \n",
       "4178  1087215634075799552  @usuario @usuario No dijo salgan a golpear, a ...   \n",
       "2928  1393254552145641472  emoji ojos emoji Te invitamos a conocer esta i...   \n",
       "\n",
       "            date  label  \n",
       "4670  2021-09-06      0  \n",
       "373   2021-11-30      1  \n",
       "1164  2021-10-13      0  \n",
       "4178  2021-11-17      0  \n",
       "2928  2021-08-26      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('./assets/data/train.csv')\n",
    "data_test = pd.read_csv('./assets/data/test.csv')\n",
    "data_valid = pd.read_csv('./assets/data/valid.csv')\n",
    "#una muestra aleatoria de los datos\n",
    "data_train.sample(n=5, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados los datos se debe mapear el texto a sus respectivos tokens y a su vez mapear las etiquetas a sus respectivos valores numéricos. Con esto se obtiene un conjunto de datos listo para ser utilizado en el entrenamiento, validación y/o prueba del modelo. La siguiente celda de código muestra como se realiza este proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e757950fb64328841b6d1aa82fd54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1750 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8de24468c3f4937a871f02bcdd40a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70025a915014d0ab9ecad7852eb14de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a45dc78561442fb391859e194ebe33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1762c59778e54243bf3483326bef94ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bd4d6c13b948749322171c4b5f189f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "        \"\"\"Tokenize text in current mini batch. This is a util function for get_dataset_from_dataframes function\n",
    "\n",
    "        Args:\n",
    "            batch (batched datasets.arrow_dataset.Dataset)\n",
    "        \n",
    "        Returns:\n",
    "            [datasets.arrow_dataset.Dataset]: Mapped text-label dataset\n",
    "        \"\"\"\n",
    "        return tokenizer(batch['text'], padding=False, truncation=True)\n",
    "\n",
    "def format_dataset(dataset):\n",
    "    \"\"\"Map text-label for specific dataset from pandas. This is a util function for get_dataset_from_dataframes function\n",
    "\n",
    "    Args:\n",
    "        dataset (datasets.arrow_dataset.Dataset): Dataset from pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        [datasets.arrow_dataset.Dataset]: Mapped text-label dataset\n",
    "    \"\"\"\n",
    "    def get_labels(examples):\n",
    "        return {'labels': examples['label']}\n",
    "\n",
    "    dataset = dataset.map(get_labels)\n",
    "    return dataset\n",
    "\n",
    "#Features to map insto dataset\n",
    "features = Features({\n",
    "    'text': Value('string'),\n",
    "    'label': ClassLabel(num_classes=2, names=['ok', 'hateful'])\n",
    "    })\n",
    "\n",
    "train_dataset = Dataset.from_pandas(data_train, features=features)\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=4)\n",
    "train_dataset = format_dataset(train_dataset)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(data_test, features=features)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=4)\n",
    "test_dataset = format_dataset(test_dataset)\n",
    "\n",
    "valid_dataset = Dataset.from_pandas(data_valid, features=features)\n",
    "valid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=4)\n",
    "valid_dataset = format_dataset(valid_dataset)\n",
    "\n",
    "#what happened?\n",
    "#train_dataset[8290]\n",
    "#tokenizer.decode(np.random.randint(0, 30002, size=20).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tener la posibilidad de usar un entrenamiento por lotes (pasar más de un texto a la vez al modelo) se debe instanciar un objeto DataCollatorWithPadding de la biblioteca transformers. Este objeto se encarga de agregar los tokens de relleno necesarios para que todos los textos tengan la misma longitud. La longitud máxima de los textos se obtiene de la función `model_max_length` de la clase `AutoTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be able to use batched training, we need to use a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding='longest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de evaluación\n",
    "Las métricas de evaluación o clasificación son medidas que permiten cuantificar la calidad de un modelo de clasificación, existe una gran variedad de ellas. Las más populares son: Precisión, Exahustividad, Valor-F, Exactitud y la matriz de confusión.\n",
    "\n",
    "Para entender cada una de estas es necesario comprender los conceptos de falso positivo, falso negativo, verdadero positivo y verdadero negativo en la clasificación binaria. Consideremos dos posibles clases $+$ y $-$, y una muestra con una etiqueta real $ER$ y una predicción $P$ dada por el modelo a evaluar, con esto decimos que:\n",
    "\n",
    "* Si la etiqueta real y la predicción coinciden y corresponde a la clase $+$, entonces el modelo ha predicho un verdadero positivo (VP)\n",
    "* Si la etiqueta real y la predicción coinciden y corresponde a la clase $-$, entonces el modelo ha predicho un verdadero negativo (VN)\n",
    "* Si la etiqueta real es de clase $+$ y la predicción es de clase $-$, es decir $ER \\neq P$, el modelo ha predicho un falso negativo (FN)\n",
    "* Si la etiqueta real es de clase $-$ y la predicción es de clase $+$, es decir $ER \\neq P$, el modelo ha predicho un falso positivo (FN)\n",
    "\n",
    "##### Precisión\n",
    "La métrica de precisión es una medida de cuántas de las observaciones el modelo predijo correctamente sobre el total de las predicciones correctas e incorrectas. Para calcular la precisión se usa la siguiente ecuación.\n",
    "$$precision = \\frac{VP}{VP+FP}$$\n",
    "\n",
    "##### Exhaustividad\n",
    "La Exhaustividad es la medida de cuántas observaciones predijo correctamente el modelo sobre la cantidad total de las observaciones y se obtiene usando la ecuación\n",
    "$$exhaustividad = \\frac{VP}{VP+FN}$$\n",
    "\n",
    "##### Valor-F\n",
    "El Valor-F es una métrica que combina la precisión y la exhaustividad, a modo que se establece un equilibrio entre ambas métricas. Para calcularlo es necesario conocer los valores de precisión, exhaustividad y aplicar la ecuación\n",
    "$$Valor F = 2\\cdot \\frac{precision \\cdot exhaustividad}{precision + exhaustividad}$$\n",
    "\n",
    "##### Exactitud\n",
    "La exactitud mide el porcentaje de casos que el modelo ha predicho correctamente. Esta métrica tiende a sobre estimar el rendimiento del modelo cuando se utiliza un conjunto de prueba o validación desbalanceados y se calcula a través de la siguiente ecuación.\n",
    "$$exactitud = \\frac{VP+VN}{VP+VN+FP+FN}$$\n",
    "\n",
    "##### Matriz de confusión\n",
    "Para el caso de la clasificación binaria, la matriz de confusión es una tabla con los valores de VP, VN, FP y FN en función de las etiquetas reales y de las predicciones totales del conjunto de prueba o validación. La composición se muestra a continuación\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/images/matriz_confusion.png\"\n",
    "         alt=\"Matriz de confusión\"\n",
    "         style=\"width: auto height: 50%\">\n",
    "    <figcaption>Estructura general de la matriz de confusión para clasificación binaria</figcaption>\n",
    "</figure>\n",
    "\n",
    "En la siguiente celda de código se crea una función que recibe como parámetros las etiquetas reales y las predicciones del modelo y retorna los valores de VP, VN, FP y FN. Esta función será muy útil para calcular las métricas de evaluación por cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \"\"\"Compute Accuracy, Precision, Recall and F1 metrics\n",
    "\n",
    "    Args:\n",
    "        p ([List]): List with calculated logits by model and real label per sample\n",
    "\n",
    "    Returns:\n",
    "        [dict]: dict with calculated metrics\n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    #Get class with most probability\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    #Binary recall for class 0 (not xenophobic) and class 1 (xenophobic)\n",
    "    recall_cls0 = recall_score(y_true=labels, y_pred=pred, pos_label=0, average='binary')\n",
    "    recall_cls1 = recall_score(y_true=labels, y_pred=pred, pos_label=1, average='binary')\n",
    "    #Binary precision for class 0 and class 1\n",
    "    precision_cls0 = precision_score(y_true=labels, y_pred=pred, pos_label=0, average='binary')\n",
    "    precision_cls1 = precision_score(y_true=labels, y_pred=pred, pos_label=1, average='binary')\n",
    "    #Binary F1 for class 0 and class 1\n",
    "    f1_cls0 = f1_score(y_true=labels, y_pred=pred, pos_label=0, average='binary')\n",
    "    f1_cls1 = f1_score(y_true=labels, y_pred=pred, pos_label=1, average='binary')\n",
    "\n",
    "    #F1 scores: macro (balanced data), micro and weighted (unbalanced data)\n",
    "    f1_micro = f1_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    f1_macro = f1_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1_weight = f1_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "\n",
    "    return {'accuracy': accuracy, 'precision_cls1': precision_cls1, 'precision_cls0': precision_cls0,\n",
    "            'recall_cls1': recall_cls1, 'recall_cls0': recall_cls0, 'f1_cls1': f1_cls1, 'f1_cls0': f1_cls0,\n",
    "            'f1_micro': f1_micro, 'f1_macro': f1_macro, 'f1_weigth': f1_weight}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "Para entrenar el modelo se debe instanciar un objeto de la clase `Trainer` de la biblioteca transformers. Este objeto recibe como parámetros el modelo a entrenar, el conjunto de datos de entrenamiento, el conjunto de datos de validación, la función de evaluación y el DataCollatorWithPadding.\n",
    "\n",
    "A través de la clase TrainingArguments es posible indicar al modelo cuántas épocas será entrenado, el tamaño del lote, la frecuencia de evaluación, el directorio donde se almacenarán los checkpoints, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir = './',\n",
    "        num_train_epochs = 3,\n",
    "        per_device_train_batch_size = 4,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        do_eval=True,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        bf16 = False,\n",
    "        half_precision_backend = 'amp',\n",
    "        greater_is_better = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_args = {\n",
    "        'model': model,\n",
    "        'args': training_args,\n",
    "        'train_dataset': train_dataset,\n",
    "        'eval_dataset': valid_dataset,\n",
    "        'data_collator': data_collator,\n",
    "        'tokenizer': tokenizer,\n",
    "        'compute_metrics': compute_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instanciada y configurada la clase Trainer, se procede a entrenar el modelo con el método `train()`. Por cada época se evalúa el modelo con el conjunto de datos de validación y se almacenan los checkpoints en el directorio indicado en la configuración de la clase TrainingArguments. El resultado de la evaluación se mostrará como una tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5250\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='5250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5250/5250 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Cls1</th>\n",
       "      <th>Precision Cls0</th>\n",
       "      <th>Recall Cls1</th>\n",
       "      <th>Recall Cls0</th>\n",
       "      <th>F1 Cls1</th>\n",
       "      <th>F1 Cls0</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weigth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.468800</td>\n",
       "      <td>0.420858</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.786127</td>\n",
       "      <td>0.899637</td>\n",
       "      <td>0.621005</td>\n",
       "      <td>0.952625</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.809625</td>\n",
       "      <td>0.874676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.744285</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.834586</td>\n",
       "      <td>0.875433</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>0.921117</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.775899</td>\n",
       "      <td>0.857511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.728876</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>0.942382</td>\n",
       "      <td>0.715328</td>\n",
       "      <td>0.926369</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.820849</td>\n",
       "      <td>0.880151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./checkpoint-1750\n",
      "Configuration saved in ./checkpoint-1750/config.json\n",
      "Model weights saved in ./checkpoint-1750/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1750/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./checkpoint-3500\n",
      "Configuration saved in ./checkpoint-3500/config.json\n",
      "Model weights saved in ./checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./checkpoint-5250\n",
      "Configuration saved in ./checkpoint-5250/config.json\n",
      "Model weights saved in ./checkpoint-5250/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-5250/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-5250/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint-3500 (score: 0.7442847490310669).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5250, training_loss=0.23647104081653414, metrics={'train_runtime': 188.9331, 'train_samples_per_second': 111.15, 'train_steps_per_second': 27.788, 'total_flos': 621464201650080.0, 'train_loss': 0.23647104081653414, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(**trainer_args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_name = './checkpoint-5250'\n",
    "\n",
    "#load model from model name using huggingface library\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, return_dict=True, num_labels=2)\n",
    "\n",
    "#load tokenizer and config it (based on robertuito github)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = 128\n",
    "trainer = Trainer(**trainer_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción y evaluación del modelo\n",
    "\n",
    "Para realizar predicciones con el modelo entrenado se debe instruir al modelo que no debe actualizar los pesos de los parámetros, esto se logra con el método `eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la predicción sobre un único texto se usa la clase TextClassificationPipeline, que recibe como parámetros el modelo y el tokenizador. Esta clase permite realizar la tokenización del texto, la predicción y la conversión de los resultados a un formato legible. Como en el siguiente ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'ok', 'score': 0.9999634027481079}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#single text prediction\n",
    "single_text_pred = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "text = \"Que opinas de la reformas Migratorias? emoji ojos emoji emoji ojos emoji mi buen amigo @usuario?\\nYo pienso que DEbieron facilitar la entrada de los inmigrantes emoji persona de mostrador de información emoji emoji bandera brasil emoji emoji bandera costa rica emoji emoji bandera nicaragua emoji emoji bandera méxico emoji emoji bíceps flexionado emoji hashtag yo te apoyo eexx jaja. Página oficial del BID:url\"\n",
    "single_text_pred(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que el modelo está en modo de evaluación, se procede a realizar las predicciones con el método `predict()`. Este método recibe como parámetro el conjunto de datos sobre el cual se realizarán las predicciones. Este método retorna un diccionario con las predicciones, las etiquetas reales, las métricas de evaluación, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45a6b150b81421485a3ad680c8f9c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#predict on test dataset, get metrics and logits\n",
    "test_results = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestran en una tabla los resultados de las métricas de evaluación calculadas con el conjunto de datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Métricas de evaluación en prueba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_loss</th>\n",
       "      <td>0.803517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision_cls1</th>\n",
       "      <td>0.768595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision_cls0</th>\n",
       "      <td>0.903482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall_cls1</th>\n",
       "      <td>0.638444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall_cls0</th>\n",
       "      <td>0.946257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_cls1</th>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_cls0</th>\n",
       "      <td>0.924375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_micro</th>\n",
       "      <td>0.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_macro</th>\n",
       "      <td>0.810938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_weigth</th>\n",
       "      <td>0.874803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_runtime</th>\n",
       "      <td>226.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_samples_per_second</th>\n",
       "      <td>8.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_steps_per_second</th>\n",
       "      <td>1.103000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Métricas de evaluación en prueba\n",
       "test_loss                                        0.803517\n",
       "test_accuracy                                    0.879000\n",
       "test_precision_cls1                              0.768595\n",
       "test_precision_cls0                              0.903482\n",
       "test_recall_cls1                                 0.638444\n",
       "test_recall_cls0                                 0.946257\n",
       "test_f1_cls1                                     0.697500\n",
       "test_f1_cls0                                     0.924375\n",
       "test_f1_micro                                    0.879000\n",
       "test_f1_macro                                    0.810938\n",
       "test_f1_weigth                                   0.874803\n",
       "test_runtime                                   226.608500\n",
       "test_samples_per_second                          8.826000\n",
       "test_steps_per_second                            1.103000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show metrics as dataframe\n",
    "pd.DataFrame.from_dict(test_results.metrics, orient='index', columns=['Métricas de evaluación en prueba'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se construye la matriz de confusión con los valores de VP, VN, FP y FN. Para esto se usa la función `confusion_matrix()` de la biblioteca sklearn.metrics. También se calcula la precisión, exhaustividad y valor-F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9035    0.9463    0.9244      1563\n",
      "           1     0.7686    0.6384    0.6975       437\n",
      "\n",
      "    accuracy                         0.8790      2000\n",
      "   macro avg     0.8360    0.7924    0.8109      2000\n",
      "weighted avg     0.8740    0.8790    0.8748      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Predict class using logits\n",
    "y_pred = np.argmax(test_results.predictions, axis=-1)\n",
    "#show the confusion matrix\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.heatmap(confusion_matrix(test_results.label_ids, y_pred, normalize='true'),vmin=0.0, vmax=1.0, annot=True);\n",
    "#show classification report\n",
    "print(classification_report(test_results.label_ids, y_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cad67f8f08b43f52430a6b8684b01d2984f30490bfa5827628ebc9dc8cbf0dc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
