{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracci√≥n, transformaci√≥n y carga de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias √∫tiles\n",
    "\n",
    "Una vez que se cuenta con el conjunto de datos etiquetado, √©ste debe ser dividido en tres subconjuntos que llamaremos \"entrenamiento\", \"validaci√≥n\" y \"prueba\", divididos en 70%, 10% y 20% de los datos. Esta divisi√≥n es com√∫n dentro de la ciencia de datos y est√° basada en el *principio de Pareto*.\n",
    "\n",
    "Adem√°s de la divisi√≥n es necesario transformar el texto de cada tweet, pues recordemos cada texto puede contener emoticonos üáßüá∑üá®üá∑üá≥üáÆüá≤üáΩ, hashtags *#VivaLaMigraci√≥n*, menciones a usuarios [@el_BID](https://twitter.com/el_bid?lang=es) y links a p√°ginas externas. Algunos de estos elementos pueden contener informaci√≥n √∫til para la detecci√≥n de sentimiento y/o xenofobia (como los hashtags), pero otros pueden no ser nada informativos (como los nombres de usuario).\n",
    "\n",
    "##### Scikit learn\n",
    "Se trata de una librer√≠a de c√≥digo abierto enfocada en proveer herramientas de aprendizaje de m√°quinas tales como modelos estad√≠sticos y matem√°ticos, as√≠ como m√©tricas de evaluaci√≥n comunes en algoritmos de aprendizaje de m√°quinas.\n",
    "<figure>\n",
    "    <img src=\"./assets/images/scikit.png\"\n",
    "         alt=\"scikit-learn logo\"\n",
    "         style=\"max-width: 20%; height: auto\">\n",
    "    <figcaption>scikit-learn logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "Esta librer√≠a nos permitir√° dividir el conjunto de datos en los subconjuntos mencionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#data reading\n",
    "import pandas as pd\n",
    "\n",
    "#data processing\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "import re\n",
    "\n",
    "#scikit-learn creacion de conjuntos de entrenamiento, prueba y validaci√≥n\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando los datos etiquetados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1092843025447800064</td>\n",
       "      <td>@A3Noticias Pues hombre!  un incremento de un ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>1149661755167000064</td>\n",
       "      <td>Esto suena a: mi marido me quiere pero me pega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>1175806960660369920</td>\n",
       "      <td>Ahora saldr√°n los venezolanos en Per√∫ a decir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>1356463151215219968</td>\n",
       "      <td>Tambi√©n el 19 de Enero me toc√≥ un conductor ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1297872161823229952</td>\n",
       "      <td>No hace falta decir que el crimen, sin decirlo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text\n",
       "989   1092843025447800064  @A3Noticias Pues hombre!  un incremento de un ...\n",
       "1382  1149661755167000064  Esto suena a: mi marido me quiere pero me pega...\n",
       "378   1175806960660369920  Ahora saldr√°n los venezolanos en Per√∫ a decir ...\n",
       "908   1356463151215219968  Tambi√©n el 19 de Enero me toc√≥ un conductor ex...\n",
       "368   1297872161823229952  No hace falta decir que el crimen, sin decirlo..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./assets/data/xeno_data_workshop.csv')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet marcado como xenof√≥bo:\n",
      " Una imagen dicen mil Palabras\".  Si ven la cara de Cojudo? Como que no quebrara un Plato\".... Aca en ECUADOR. Nos sirve Como un ejemplo?  LA INVASION DE EXTRANJEROS MAS VENEZOLANOS\" Son Asi.  Carita de Dios. Pero Recuerda Es mejor no conocerlos?  De lejitos\"\n",
      "Tweet marcado como NO xenof√≥bo:\n",
      " #Nacional | Alrededor de 3‚É£0‚É£ migrantes centroamericanos fueron asegurados en un operativo realizado por elementos de la Guardia Nacional. üöÉ   https://t.co/11exZf9vxG\n"
     ]
    }
   ],
   "source": [
    "print('Tweet marcado como xenof√≥bo:\\n {}\\nTweet marcado como NO xenof√≥bo:\\n {}'.format(data[data.label==1].sample(n=1).text.values[0], data[data.label==0].sample(n=1).text.values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que opinas de la reformas Migratorias?üëÄüëÄ mi buen amigo @Pepe?\n",
      "Yo pienso que DEbieron facilitar la entrada de los inmigrantes üíÅüáßüá∑üá®üá∑üá≥üáÆüá≤üáΩüí™#YoTeApoyo eeeexxxxxx jajajajaja. P√°gina oficial del BID:          https://www.iadb.org/es \n"
     ]
    }
   ],
   "source": [
    "example_text = 'Que opinas de la reformas Migratorias?üëÄüëÄ mi buen amigo @Pepe?\\nYo pienso que DEbieron facilitar la entrada de los inmigrantes üíÅüáßüá∑üá®üá∑üá≥üáÆüá≤üáΩüí™#YoTeApoyo eeeexxxxxx jajajajaja.'\n",
    "bid_url = ' P√°gina oficial del BID:          https://www.iadb.org/es '\n",
    "example_text = example_text + bid_url\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Que opinas de la reformas Migratorias? emoji ojos emoji  emoji ojos emoji  mi buen amigo @usuario?\\nYo pienso que DEbieron facilitar la entrada de los inmigrantes  emoji persona de mostrador de informaci√≥n emoji  emoji bandera brasil emoji  emoji bandera costa rica emoji  emoji bandera nicaragua emoji  emoji bandera m√©xico emoji  emoji b√≠ceps flexionado emoji hashtag yo te apoyo eexx jaja. P√°gina oficial del BID:  url'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_tweet(example_text, user_token='@usuario', url_token='url', preprocess_hashtags=True, demoji=True, shorten=2, normalize_laughter=True, hashtag_token='hashtag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el texto cambia los links por la palabra definida \"url\", los nombres de usuario por \"@usuario\", separa las palabras contenidas en un hashtag y adem√°s agrega \"hashtag\", normaliza las expresiones de risa y las letras repetidas. Describe el contenido de cada emoticono.\n",
    "\n",
    "Las palabras definidas ser√°n muy √∫tiles para el modelo, pues √©ste puede ser configurado para que considere dichas palabras como elementos estructurales y no como palabras.\n",
    "\n",
    "Sin embargo, se observa que el texto contiene espacios, mantiene los acentos, los saltos de l√≠nea y las may√∫sculas. Estas caracter√≠sticas podr√≠an ser no deseadas seg√∫n el objetivo que se tiene y el modelo que se emplear√° para clasificar los textos. Ya que existen modelos que hacen diferencia entre estas caracter√≠sticas y otros que no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#P√°gina#oficial#del#BID:#https://www.iadb.org/es#'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove extra spaces\n",
    "re.compile(r'\\s+').sub('#', bid_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#juntemos estos conceptos para crear una funci√≥n que nos permita limpiar un tweet\n",
    "def clean_tweet(tweet, user_token='@usuario', url_token='url', hashtag_token='hashtag', preprocess_hashtags=True, demoji=True, shorten=2, normalize_laughter=True):\n",
    "    tweet = str(tweet)\n",
    "    tweet = preprocess_tweet(tweet, user_token=user_token, url_token=url_token, preprocess_hashtags=preprocess_hashtags, demoji=demoji, shorten=shorten, normalize_laughter=normalize_laughter, hashtag_token=hashtag_token)\n",
    "    tweet = re.compile(r'\\s+').sub(' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "data['text'] = data.text.apply(lambda x: clean_tweet(x))\n",
    "#remove initial and final spaces\n",
    "data['text'] = data.text.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1185383493112649984,\n",
       "        '@usuario @usuario @usuario @usuario @usuario @usuario @usuario @usuario @usuario no era una ilusion ,eso era seguro, nos √≠bamos tranformar en un pais de 25 millones en 2 a√±os y la economia a 1 % o menos ,ladelincuencia desatada y todo mal,dime que hizo bachelet condeuda de chile? donde quedo la plata?bachelet paso sename,corrupcion carabienro,FFAA,inmigracion']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(n=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisi√≥n en conjuntos de entrenamiento, validaci√≥n y prueba\n",
    "\n",
    "La divisi√≥n en tres subconjuntos del conjunto de datos es de suma importancia, ya que cada uno de ellos juega un rol importante para la creaci√≥n de un modelo de aprendizaje de m√°quinas.\n",
    "\n",
    "* El conjunto de entrenamiento, es por lo regular, el que contiene m√°s muestras, ya que a partir de estos datos el modelo debe aprender y/o generalizar las caracter√≠sticas ling√º√≠sticas de los tweets marcados como xen√≥fobos y no xen√≥fobos de la mejor forma posible. De esta manera, cuando el modelo deba clasificar un texto nuevo, su etiqueta sea correcta.\n",
    "\n",
    "* El conjunto de validaci√≥n, es por lo regular, el que contiene menos muestras. Este conjunto nos permitir√° realizar m√∫ltiples evaluaciones sobre el modelo, ya sea para determinar si existen mejoras al momento de variar cualquier par√°metro involucrado con el modelo, aumentar el conjunto de entrenamiento o incluso evaluar la selecci√≥n de modelos.\n",
    "\n",
    "* El conjunto de prueba nos permite realizar la evaluaci√≥n final del modelo. Por lo regular este conjunto solo debe usarse una vez, esto para evitar posibles sesgos.\n",
    "\n",
    "En el aprendizaje de m√°quinas es com√∫n aplicar el principio de Pareto, el cual sostiene que la divisi√≥n √≥ptima es 80% de los datos para entrenamiento y el restante para tareas de evaluaci√≥n. Sin embargo, dado que se sabe que el modelo pasar√° por una serie de algoritmos de optimizaci√≥n, s√≠ se usara esta divisi√≥n podr√≠a lograrse un sesgo en cuanto al rendimiento real del modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la divisi√≥n en subconjuntos debe considerarse tambi√©n la siguiente pregunta ¬øEl conjunto de datos total est√° balanceado? Al decir balanceado nos referimos a que las etiquetas disponibles son proporcionales entre s√≠. Para nuestro caso, dado que se trabaja con √∫nicamente dos etiquetas, diremos que nuestro conjunto de datos es balanceado s√≠ y s√≥lo s√≠ aproximadamente el 50% de los datos contienen la etiqueta *xen√≥fobo*, y el restante corresponde a la etiqueta *no xenof√≥bo*.\n",
    "\n",
    "Cuando el conjunto de datos se encuentra desbalanceado (como es el caso) la divisi√≥n en subconjuntos debe hacerse con cuidado, a modo de distribuir proporcionalmente la o las clases m√°s desbalanceadas en cada uno de los tres.\n",
    "\n",
    "Un ejemplo de omitir esta distribuci√≥n puede suceder como sigue: Suponga que se tiene un conjunto de datos total de 1000 muestras, donde 780 corrresponden a la clase A y 220 a la clase B. Se hace la divisi√≥n de forma aleatoria a modo de que el conjunto de entrenamiento contiene s√≥lo 20 muestras de la clase B y 680 de la clase A. Tanto el conjunto de validaci√≥n y prueba contendr√°n 100 muestras de la clase A y 200 de la clase B. Dado que las muestras de la clase B en el conjunto de entrenamiento son pocas, el modelo no podr√° aprender lo suficiente de la clase B y por ende, su rendimiento para esta clase ser√° muy bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data before splitting\n",
    "data = data.sample(frac=1, ignore_index=True)\n",
    "\n",
    "#vamos a dividir el dataset por su id y etiqueta\n",
    "X = data.id.values\n",
    "y = data.label.values\n",
    "#generate train and test sets\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, \n",
    "                                    y, test_size=0.2, random_state=2022, stratify=y)\n",
    "#from x_test and y_test, generate validation and test sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.125, random_state=2022,\n",
    "                                                  stratify=y_train)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dataframes\n",
    "train_df = data[data.id.isin(x_train)].reset_index(drop=True)\n",
    "test_df = data[data.id.isin(x_test)].reset_index(drop=True)\n",
    "valid_df = data[data.id.isin(x_val)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5469\n",
       "1    1531\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of the labels by eye\n",
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1353357604764971008</td>\n",
       "      <td>@usuario @usuario @usuario Claro porque los ve...</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143920056621031424</td>\n",
       "      <td>El hashtag alquiler de hashtag viviendas en ha...</td>\n",
       "      <td>2021-11-21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1220702929478438912</td>\n",
       "      <td>@usuario Que raro.. total si aqu√≠ solo entran ...</td>\n",
       "      <td>2021-07-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1347148151598190592</td>\n",
       "      <td>@usuario @usuario @usuario Que est√°n en la otr...</td>\n",
       "      <td>2021-09-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1169355407757893632</td>\n",
       "      <td>Una persona como Ivanna Trump no representa a ...</td>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1353357604764971008  @usuario @usuario @usuario Claro porque los ve...   \n",
       "1  1143920056621031424  El hashtag alquiler de hashtag viviendas en ha...   \n",
       "2  1220702929478438912  @usuario Que raro.. total si aqu√≠ solo entran ...   \n",
       "3  1347148151598190592  @usuario @usuario @usuario Que est√°n en la otr...   \n",
       "4  1169355407757893632  Una persona como Ivanna Trump no representa a ...   \n",
       "\n",
       "         date  label  \n",
       "0  2021-10-20      1  \n",
       "1  2021-11-21      0  \n",
       "2  2021-07-13      1  \n",
       "3  2021-09-29      0  \n",
       "4  2021-11-12      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the final datasets\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the datasets\n",
    "train_df.to_csv('./assets/data/train.csv', index=False)\n",
    "test_df.to_csv('./assets/data/test.csv', index=False)\n",
    "valid_df.to_csv('./assets/data/valid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bid')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cad67f8f08b43f52430a6b8684b01d2984f30490bfa5827628ebc9dc8cbf0dc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
