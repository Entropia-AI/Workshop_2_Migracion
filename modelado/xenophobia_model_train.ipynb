{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de clasificaci칩n binaria para la detecci칩n de xenofobia en tweets de migraci칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias 칰tiles\n",
    "\n",
    "##### Hugging Face\n",
    "Se trata de una comunidad y plataforma de ciencia de datos que ofrece una gran cantidad de herramientas para la creaci칩n y evaluaci칩n de modelos de aprendizaje profundo. Entre ellas destacan [[Omer Mahmood @ Towards Data Science](https://towardsdatascience.com/whats-hugging-face-122f4e7eb11a#:~:text=Hugging%20Face%20is%20a%20community,(OS)%20code%20and%20technologies.)]:\n",
    "* Herramientas que permiten a los usuarios construir, entrenar y desplegar modelos basados en c칩digo abierto\n",
    "* Un lugar donde una amplia comunidad de cient칤ficos de datos, ingenieros de aprendizaje profundo e investigadores pueden reunirse para compartir ideas, obtener apoyo, contribuir a los proyectos e incluso compartir sus modelos entrenados o puros.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/images/hug.png\"\n",
    "         alt=\"Hugging Face logo\">\n",
    "    <figcaption>Hugging Face logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "No se encontr칩 ning칰n modelo entrenado para la tarea de inter칠s, por lo que se entrenaron y probaron 12 modelos a modo de seleccionar el mejor de ellos. La selecci칩n de estos 12 modelos consisti칩 en tomar aquellos que estuviesen entrenados para una tarea similar a la xenofobia (en este caso fue el discurso de odio) y/o que hayan sido entrenados para comprender el idioma espa침ol.\n",
    "\n",
    "De este estudio el modelo seleccionado fue [RoBERTuito-base-uncased](https://arxiv.org/abs/2111.09453)\n",
    "\n",
    "##### PyTorch\n",
    "Es una librer칤a de aprendizaje autom치tico de c칩digo abierto *que acelera el camino desde la creaci칩n de prototipos de investigaci칩n hasta el despliegue de producci칩n [[PyTorch](https://pytorch.org/)].*\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/images/pytorch.png\"\n",
    "         alt=\"PyTorch logo\">\n",
    "    <figcaption>PyTorch logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "Algunos de los modelos disponibles en Hugging Face se encuentran implementados sobre esta librer칤a. De este modo, algunas de las herramientas disponibles en PyTorch son f치cilmente adaptables con Hugging Face, esto nos permitir치 a침adir o modificar la estructura de una red neuronal, as칤 como su comportamiento, permitiendo al usuario implementar varias funciones personalizadas.\n",
    "\n",
    "##### Scikit learn\n",
    "Se trata de una librer칤a de c칩digo abierto enfocada en proveer herramientas de aprendizaje de m치quinas tales como modelos estad칤sticos y matem치ticos, as칤 como m칠tricas de evaluaci칩n comunes en algoritmos de aprendizaje de m치quinas.\n",
    "<figure>\n",
    "    <img src=\"./assets/images/scikit.png\"\n",
    "         alt=\"scikit-learn logo\"\n",
    "         style=\"max-width: 20%; height: auto\">\n",
    "    <figcaption>scikit-learn logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "Esta librer칤a nos permitir치 implementar de manera sencilla las m칠tricas de evaluaci칩n del modelo de inter칠s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#HuggingFace library\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import Dataset, Value, ClassLabel, Features\n",
    "\n",
    "#PyTorch Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#data reading\n",
    "import pandas as pd\n",
    "\n",
    "#math\n",
    "import numpy as np\n",
    "\n",
    "#scikit-learn metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, recall_score, accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando el modelo preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#set model name\n",
    "model_name = \"pysentimiento/robertuito-base-uncased\"\n",
    "#download model from huggingface\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2)\n",
    "\n",
    "#Load tokenizer\n",
    "#Un tokenizer es un objeto que convierte una secuencia de caracteres en una secuencia de n칰meros.\n",
    "#es una especie de filtro que prepara el texto para que el modelo lo pueda entender.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30002, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Config model: assign names to each label\n",
    "model.config.id2label = {0: 'ok', 1:'hateful'}\n",
    "id2label = {0: 'ok', 1:'hateful'}\n",
    "model.config.label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "#Add special tokens to the tokenizer\n",
    "tokenizer.add_tokens(['@usuario', 'url', 'hashtag', 'emoji'])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1186249127304349952</td>\n",
       "      <td>C칩mo #Venezolano le pido a los gobiernos de Br...</td>\n",
       "      <td>2021-09-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1340821499914160128</td>\n",
       "      <td>@elcomercio_peru Que @FSagasti cierre las fron...</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1154893184641769984</td>\n",
       "      <td>No todos los migrantes son buenas personas 游녢游낖</td>\n",
       "      <td>2021-10-14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1154971839921189888</td>\n",
       "      <td>@GaelDiputada @sebastianpinera @nmonckeberg Hi...</td>\n",
       "      <td>2021-09-29</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1284129763662210048</td>\n",
       "      <td>@ChileEnLibertad Chilenos marginales contra mi...</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "34  1186249127304349952  C칩mo #Venezolano le pido a los gobiernos de Br...   \n",
       "39  1340821499914160128  @elcomercio_peru Que @FSagasti cierre las fron...   \n",
       "43  1154893184641769984      No todos los migrantes son buenas personas 游녢游낖   \n",
       "47  1154971839921189888  @GaelDiputada @sebastianpinera @nmonckeberg Hi...   \n",
       "5   1284129763662210048  @ChileEnLibertad Chilenos marginales contra mi...   \n",
       "\n",
       "          date  label  \n",
       "34  2021-09-07    0.0  \n",
       "39  2021-08-08    0.0  \n",
       "43  2021-10-14    1.0  \n",
       "47  2021-09-29    1.0  \n",
       "5   2021-07-15    0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('xeno_train_unbalanced_retiquetado.csv')\n",
    "data_train = data_train[:80]\n",
    "data_test = pd.read_csv('xeno_test_unbalanced_retiquetado.csv')\n",
    "data_valid = pd.read_csv('xeno_valid_unbalanced_retiquetado.csv')\n",
    "data_train.dropna(subset=['text'],inplace=True)\n",
    "data_train.sample(n=5, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1181977505433029888</td>\n",
       "      <td>@MafeCarrascal Si,c칩mo no. Aqu칤 no hay desplaz...</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1136799389878560000</td>\n",
       "      <td>@ASINEG @gdehoyoswalther @lopezobrador_ @Refor...</td>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1266506225019160064</td>\n",
       "      <td>@jguaido atenci칩n Sr Guaido, nadie le echa la ...</td>\n",
       "      <td>2021-09-11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1214630095614350080</td>\n",
       "      <td>He entrado a varios restaurantes donde han con...</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1306026546004720128</td>\n",
       "      <td>@allan_parada Pienso lo mismo. A mi casa tampo...</td>\n",
       "      <td>2021-06-22</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1159894568831249920</td>\n",
       "      <td>@m__casti El tema no va por ah칤, Chile es un p...</td>\n",
       "      <td>2021-09-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1186292795537009920</td>\n",
       "      <td>@Psicovivir Y ud de verdad cree que el verdade...</td>\n",
       "      <td>2021-10-12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1345418348818010112</td>\n",
       "      <td>@CHVNoticias La demanda por viviendas en las c...</td>\n",
       "      <td>2021-08-29</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1306237767412839936</td>\n",
       "      <td>@javiernavia Claro si...los 4 a침os del pro era...</td>\n",
       "      <td>2021-08-28</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1370110551846749952</td>\n",
       "      <td>@DheysRios Esto no se trata de Vizcarra movers...</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows 칑 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0   1181977505433029888  @MafeCarrascal Si,c칩mo no. Aqu칤 no hay desplaz...   \n",
       "1   1136799389878560000  @ASINEG @gdehoyoswalther @lopezobrador_ @Refor...   \n",
       "2   1266506225019160064  @jguaido atenci칩n Sr Guaido, nadie le echa la ...   \n",
       "3   1214630095614350080  He entrado a varios restaurantes donde han con...   \n",
       "4   1306026546004720128  @allan_parada Pienso lo mismo. A mi casa tampo...   \n",
       "..                  ...                                                ...   \n",
       "75  1159894568831249920  @m__casti El tema no va por ah칤, Chile es un p...   \n",
       "76  1186292795537009920  @Psicovivir Y ud de verdad cree que el verdade...   \n",
       "77  1345418348818010112  @CHVNoticias La demanda por viviendas en las c...   \n",
       "78  1306237767412839936  @javiernavia Claro si...los 4 a침os del pro era...   \n",
       "79  1370110551846749952  @DheysRios Esto no se trata de Vizcarra movers...   \n",
       "\n",
       "          date  label  \n",
       "0   2021-10-15    1.0  \n",
       "1   2021-08-30    1.0  \n",
       "2   2021-09-11    0.0  \n",
       "3   2021-07-27    1.0  \n",
       "4   2021-06-22    1.0  \n",
       "..         ...    ...  \n",
       "75  2021-09-04    0.0  \n",
       "76  2021-10-12    1.0  \n",
       "77  2021-08-29    1.0  \n",
       "78  2021-08-28    0.0  \n",
       "79  2021-10-15    1.0  \n",
       "\n",
       "[80 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2ba501521f4440abf8a5fa256de9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fea0c6bd0a34613a5219d3849d2a6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7cfa51b06d448884d5990b76484ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce8aea1e04f4a0eb392ba2d6c1fa73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b255f11023492fb99ea22819d66995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e489c1c43a48989981f51423fae80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "        \"\"\"Tokenize text in current mini batch. This is a util function for get_dataset_from_dataframes function\n",
    "\n",
    "        Args:\n",
    "            batch (batched datasets.arrow_dataset.Dataset)\n",
    "        \n",
    "        Returns:\n",
    "            [datasets.arrow_dataset.Dataset]: Mapped text-label dataset\n",
    "        \"\"\"\n",
    "        return tokenizer(batch['text'], padding=False, truncation=True)\n",
    "\n",
    "def format_dataset(dataset):\n",
    "    \"\"\"Map text-label for specific dataset from pandas. This is a util function for get_dataset_from_dataframes function\n",
    "\n",
    "    Args:\n",
    "        dataset (datasets.arrow_dataset.Dataset): Dataset from pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        [datasets.arrow_dataset.Dataset]: Mapped text-label dataset\n",
    "    \"\"\"\n",
    "    def get_labels(examples):\n",
    "        return {'labels': examples['label']}\n",
    "\n",
    "    dataset = dataset.map(get_labels)\n",
    "    return dataset\n",
    "\n",
    "#Features to map insto dataset\n",
    "features = Features({\n",
    "    'text': Value('string'),\n",
    "    'label': ClassLabel(num_classes=2, names=['ok', 'hateful'])\n",
    "    })\n",
    "\n",
    "train_dataset = Dataset.from_pandas(data_train, features=features)\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=8)\n",
    "train_dataset = format_dataset(train_dataset)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(data_test, features=features)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=8)\n",
    "test_dataset = format_dataset(test_dataset)\n",
    "\n",
    "valid_dataset = Dataset.from_pandas(data_valid, features=features)\n",
    "valid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=8)\n",
    "valid_dataset = format_dataset(valid_dataset)\n",
    "\n",
    "#what happened?\n",
    "#train_dataset[8290]\n",
    "#tokenizer.decode(np.random.randint(0, 30002, size=20).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be able to use batched training, we need to use a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding='longest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M칠tricas de evaluaci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \"\"\"Compute Accuracy, Precision, Recall and F1 metrics\n",
    "\n",
    "    Args:\n",
    "        p ([List]): List with calculated logits by model and real label per sample\n",
    "\n",
    "    Returns:\n",
    "        [dict]: dict with calculated metrics\n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    #Get class with most probability\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    #Binary recall for class 0 (not xenophobic) and class 1 (xenophobic)\n",
    "    recall_cls0 = recall_score(y_true=labels, y_pred=pred, pos_label=0, average='binary')\n",
    "    recall_cls1 = recall_score(y_true=labels, y_pred=pred, pos_label=1, average='binary')\n",
    "    #Binary precision for class 0 and class 1\n",
    "    precision_cls0 = precision_score(y_true=labels, y_pred=pred, pos_label=0, average='binary')\n",
    "    precision_cls1 = precision_score(y_true=labels, y_pred=pred, pos_label=1, average='binary')\n",
    "    #Binary F1 for class 0 and class 1\n",
    "    f1_cls0 = f1_score(y_true=labels, y_pred=pred, pos_label=0, average='binary')\n",
    "    f1_cls1 = f1_score(y_true=labels, y_pred=pred, pos_label=1, average='binary')\n",
    "\n",
    "    #F1 scores: macro (balanced data), micro and weighted (unbalanced data)\n",
    "    f1_micro = f1_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    f1_macro = f1_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1_weight = f1_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "\n",
    "    return {'accuracy': accuracy, 'precision_cls1': precision_cls1, 'precision_cls0': precision_cls0,\n",
    "            'recall_cls1': recall_cls1, 'recall_cls0': recall_cls0, 'f1_cls1': f1_cls1, 'f1_cls0': f1_cls0,\n",
    "            'f1_micro': f1_micro, 'f1_macro': f1_macro, 'f1_weigth': f1_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir = './',\n",
    "        num_train_epochs = 3,\n",
    "        per_device_train_batch_size = 8,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        do_eval=True,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        bf16 = False,\n",
    "        half_precision_backend = 'amp',\n",
    "        greater_is_better = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_args = {\n",
    "        'model': model,\n",
    "        'args': training_args,\n",
    "        'train_dataset': train_dataset,\n",
    "        'eval_dataset': valid_dataset,\n",
    "        'data_collator': data_collator,\n",
    "        'tokenizer': tokenizer,\n",
    "        #'compute_metrics': compute_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 80\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8797038ccf23493fa404b8795834180c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pedri0/Documents/Pruebas/bid/segundo_workshop/xenophobia_model.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pedri0/Documents/Pruebas/bid/segundo_workshop/xenophobia_model.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrainer_args)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pedri0/Documents/Pruebas/bid/segundo_workshop/xenophobia_model.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/transformers/trainer.py:1422\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1421\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1422\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1424\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1425\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1426\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1427\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1428\u001b[0m ):\n\u001b[1;32m   1429\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/transformers/trainer.py:2011\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2008\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2010\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 2011\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2013\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2014\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/transformers/trainer.py:2043\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2041\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2042\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2043\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2044\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2046\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1209\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1209\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1210\u001b[0m     input_ids,\n\u001b[1;32m   1211\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1212\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1213\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1214\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1215\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1216\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1217\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1218\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1219\u001b[0m )\n\u001b[1;32m   1220\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1221\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:844\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    842\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 844\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    845\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    846\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    847\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    848\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    849\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m    850\u001b[0m )\n\u001b[1;32m    851\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    852\u001b[0m     embedding_output,\n\u001b[1;32m    853\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    862\u001b[0m )\n\u001b[1;32m    863\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:137\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    135\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 137\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embeddings(position_ids)\n\u001b[1;32m    138\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[1;32m    139\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/bid/lib/python3.9/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(**trainer_args)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bid')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cad67f8f08b43f52430a6b8684b01d2984f30490bfa5827628ebc9dc8cbf0dc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
