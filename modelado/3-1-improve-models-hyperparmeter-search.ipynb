{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de hiperparámetros: Optimización Bayesiana\n",
    "\n",
    "En muchos algoritmos de inteligencia artificial, existen dos tipos de parámetros: aquellos que son inicializados aleatoriamente y pueden ser actualizados durante el entrenamiento y aquellos que no pueden ser estimados durante el entrenamiento sino que deben ser establecidos al inicio del proceso de aprendizaje, pues son parte de la configuración del modelo.\n",
    "Los parámetros no estimables se conocen como hiperparámetros.\n",
    "\n",
    "Los hiperparámetros son importantes pues pueden afectar el desempeño del modelo. Por ejemplo, en el caso de los árboles de decisión, el número de niveles de profundidad del árbol es un hiperparámetro. Si el árbol es muy profundo, puede sobreajustarse a los datos de entrenamiento, mientras que si es muy plano, puede no ser capaz de aprender patrones complejos. Por lo tanto, es importante encontrar el número óptimo de niveles de profundidad para el árbol.\n",
    "\n",
    "En este notebook, veremos cómo podemos ajustar los hiperparámetros de un modelo de aprendizaje automático utilizando la optimización bayesiana.\n",
    "\n",
    "La optimización bayesiana es un método de optimización que se basa en la teoría de la probabilidad. En lugar de probar todas las combinaciones de hiperparámetros, la optimización bayesiana utiliza un modelo probabilístico para estimar la probabilidad de qué combinación de hiperparámetros es la mejor. El modelo probabilístico se actualiza a medida que se prueban nuevas combinaciones de hiperparámetros, lo que permite que el algoritmo se centre en las combinaciones de hiperparámetros que tienen más probabilidades de ser las mejores.\n",
    "\n",
    "### Ajuste de hiperparámetros: teoría simplificada\n",
    "El objetivo de la optimización de hipreparámetros es encontrar la combinación de hiperparámetros $\\mathbb{A}$ que maximiza alguna de las métricas explicadas con anterioridad o minimiza el error de clasificación. Puede representarse matemáticamente como:\n",
    "$$ \\mathbb{A}= arg \\hspace{0.8mm} min _{x \\in \\pi}\\hspace{0.8mm} f(x)$$\n",
    "\n",
    "donde $\\pi$ es el espacio de búsqueda de hiperparámetros y $f(x)$ es la función objetivo que se desea optimizar.\n",
    "\n",
    "En cualquier problema $\\pi$ debe ser definido a priori.\n",
    "\n",
    "### Optimización bayesiana: teoría simplificada\n",
    "En la optimización Bayesiana, se mantiene un registro de las evaluaciones de la función objetivo $f(x)$ para cada combinación de hiperparámetros $x \\in \\pi$. El registro se utiliza para construir un modelo probabilístico que se actualiza a medida que se evalúan nuevas combinaciones de hiperparámetros. El modelo probabilístico se utiliza para estimar la probabilidad de que una combinación de hiperparámetros sea la mejor. La función probabilistica para elegir las combinaciones de hiperparámetros es:\n",
    "$$ P(Puntuación | x)$$\n",
    "\n",
    "### Inconvenientes de la optimización de hiperparámetros\n",
    "\n",
    "* La optimización de hiperparámetros es un proceso costoso en términos de tiempo y recursos computacionales, ya que deben probarse una serie de combinaciones de hiperparámetros. \n",
    "\n",
    "* La mejora en el desempeño del modelo puede ser marginal y es algo del cual nunca se puede estar seguro.\n",
    "\n",
    "* La convergencia de la optimización bayesiana puede ser lenta, especialmente si el modelo probabilístico no es capaz de capturar la relación entre las combinaciones de hiperparámetros y la función objetivo.\n",
    "\n",
    "* Deben conocerse los hiperparámetros del modelo a optimizar, ya que el hecho de añadir o eliminar hiperparámetros puede afectar el proceso de optimización aumentando el tiempo de convergencia.\n",
    "\n",
    "En este notebook veremos cómo podemos ajustar los hiperparámetros del modelo de xenofobia utilizando la optimización bayesiana.\n",
    "\n",
    "### Librerias útiles\n",
    "\n",
    "##### Hugging Face\n",
    "Se trata de una comunidad y plataforma de ciencia de datos que ofrece una gran cantidad de herramientas para la creación y evaluación de modelos de aprendizaje profundo. Entre ellas destacan [[Omer Mahmood @ Towards Data Science](https://towardsdatascience.com/whats-hugging-face-122f4e7eb11a#:~:text=Hugging%20Face%20is%20a%20community,(OS)%20code%20and%20technologies.)]:\n",
    "* Herramientas que permiten a los usuarios construir, entrenar y desplegar modelos basados en código abierto\n",
    "* Un lugar donde una amplia comunidad de científicos de datos, ingenieros de aprendizaje profundo e investigadores pueden reunirse para compartir ideas, obtener apoyo, contribuir a los proyectos e incluso compartir sus modelos entrenados o puros.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/images/hug.png\"\n",
    "         alt=\"Hugging Face logo\">\n",
    "    <figcaption>Hugging Face logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "No se encontró ningún modelo entrenado para la tarea de interés, por lo que se entrenaron y probaron 12 modelos a modo de seleccionar el mejor de ellos. La selección de estos 12 modelos consistió en tomar aquellos que estuviesen entrenados para una tarea similar a la xenofobia (en este caso fue el discurso de odio) y/o que hayan sido entrenados para comprender el idioma español.\n",
    "\n",
    "De este estudio el modelo seleccionado fue [RoBERTuito-base-uncased](https://arxiv.org/abs/2111.09453)\n",
    "\n",
    "##### PyTorch\n",
    "Es una librería de aprendizaje automático de código abierto *que acelera el camino desde la creación de prototipos de investigación hasta el despliegue de producción [[PyTorch](https://pytorch.org/)].*\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/images/pytorch.png\"\n",
    "         alt=\"PyTorch logo\">\n",
    "    <figcaption>PyTorch logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "Algunos de los modelos disponibles en Hugging Face se encuentran implementados sobre esta librería. De este modo, algunas de las herramientas disponibles en PyTorch son fácilmente adaptables con Hugging Face, esto nos permitirá añadir o modificar la estructura de una red neuronal, así como su comportamiento, permitiendo al usuario implementar varias funciones personalizadas.\n",
    "\n",
    "##### Scikit learn\n",
    "Se trata de una librería de código abierto enfocada en proveer herramientas de aprendizaje de máquinas tales como modelos estadísticos y matemáticos, así como métricas de evaluación comunes en algoritmos de aprendizaje de máquinas.\n",
    "<figure>\n",
    "    <img src=\"./assets/images/scikit.png\"\n",
    "         alt=\"scikit-learn logo\"\n",
    "         style=\"max-width: 20%; height: auto\">\n",
    "    <figcaption>scikit-learn logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "Esta librería nos permitirá implementar de manera sencilla las métricas de evaluación del modelo de interés.\n",
    "\n",
    "##### Adaptive Experimentation Platform (AX)\n",
    "AX es una plataforma para optimizar cualquier tipo de experimento, incluyendo experimentos de aprendizaje automático, pruebas A/B y simulaciones [[AX](https://ax.dev/docs/why-ax.html)].\n",
    "<figure>\n",
    "    <img src=\"./assets/images/ax.png\"\n",
    "         alt=\"AX logo\"\n",
    "         style=\"max-width: 40%; height: auto\">\n",
    "    <figcaption>AX logo</figcaption>\n",
    "</figure>\n",
    "AX nos permitirá optimizar los hiperparámetros del modelo a través del métodoBayesiano, así como evaluar el desempeño del modelo en términos de métricas de evaluación (en conjunto con Scikit-Learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#HuggingFace library\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import Dataset, Value, ClassLabel, Features\n",
    "\n",
    "#PyTorch Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#data reading\n",
    "import pandas as pd\n",
    "\n",
    "#math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os, random, re\n",
    "\n",
    "#scikit-learn metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, recall_score, accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    ")\n",
    "\n",
    "#Bayesian optimization\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render\n",
    "from ax.plot.contour import plot_contour_plotly\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga y configuración del tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model name\n",
    "model_name = \"pysentimiento/robertuito-base-uncased\"\n",
    "#Load tokenizer\n",
    "#Un tokenizer es un objeto que convierte una secuencia de caracteres en una secuencia de números.\n",
    "#es una especie de filtro que prepara el texto para que el modelo lo pueda entender.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#Add special tokens to the tokenizer\n",
    "tokenizer.add_tokens(['@usuario', 'url', 'hashtag', 'emoji'])\n",
    "tokenizer.model_max_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos y construcción del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./assets/data/train.csv')\n",
    "data_valid = pd.read_csv('./assets/data/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d901d1e5e99c461a8747fa7b0d51078a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/875 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1db3a3bc56458b996ef9172bdabe3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e351407dc0c24a0c8a151f41d36f7991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66b58bb24e94bb58b3e52a8208ea8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "        \"\"\"Tokenize text in current mini batch. This is a util function for get_dataset_from_dataframes function\n",
    "\n",
    "        Args:\n",
    "            batch (batched datasets.arrow_dataset.Dataset)\n",
    "        \n",
    "        Returns:\n",
    "            [datasets.arrow_dataset.Dataset]: Mapped text-label dataset\n",
    "        \"\"\"\n",
    "        return tokenizer(batch['text'], padding=False, truncation=True)\n",
    "\n",
    "def format_dataset(dataset):\n",
    "    \"\"\"Map text-label for specific dataset from pandas. This is a util function for get_dataset_from_dataframes function\n",
    "\n",
    "    Args:\n",
    "        dataset (datasets.arrow_dataset.Dataset): Dataset from pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        [datasets.arrow_dataset.Dataset]: Mapped text-label dataset\n",
    "    \"\"\"\n",
    "    def get_labels(examples):\n",
    "        return {'labels': examples['label']}\n",
    "\n",
    "    dataset = dataset.map(get_labels)\n",
    "    return dataset\n",
    "\n",
    "#Features to map insto dataset\n",
    "features = Features({\n",
    "    'text': Value('string'),\n",
    "    'label': ClassLabel(num_classes=2, names=['ok', 'hateful'])\n",
    "    })\n",
    "\n",
    "train_dataset = Dataset.from_pandas(data_train, features=features)\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=8)\n",
    "train_dataset = format_dataset(train_dataset)\n",
    "\n",
    "valid_dataset = Dataset.from_pandas(data_valid, features=features)\n",
    "valid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=8)\n",
    "valid_dataset = format_dataset(valid_dataset)\n",
    "\n",
    "#to be able to use batched training, we need to use a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding='longest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de métricas\n",
    "Dado que solo nos concentraremos en la métrica de valor-F1 para la clase más desbalanceada, se implementará una función que nos permita calcular únicamente el valor-F1. En caso de interesarse por alguna otra métrica, debe implementarse la función correspondiente, tal y como se hizo en el notebook de entrenamiento del modelo de xenofobia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \"\"\"Compute Accuracy, Precision, Recall and F1 metrics\n",
    "\n",
    "    Args:\n",
    "        p ([List]): List with calculated logits by model and real label per sample\n",
    "\n",
    "    Returns:\n",
    "        [dict]: dict with calculated metrics\n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    #Get class with most probability\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "    f1_scr = f1_score(y_true=labels, y_pred=pred, pos_label=1, average='binary')\n",
    "\n",
    "    return {'f1_cls1': f1_scr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de la función de entrenamiento y del modelo\n",
    "Puesto que para cada conjunto de parámetros se entrenará desde cero el modelo, se implementará una función que permita entrenar el modelo con los parámetros de interés. Esta función se encargará de entrenar el modelo y evaluarlo con las métricas de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    ''' init model and config it using global variable model_name'''\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, return_dict=True, num_labels=2)\n",
    "    \n",
    "    model.config.id2label = {\n",
    "            0: 'ok',\n",
    "            1: 'hateful',\n",
    "        }\n",
    "    id2label = {\n",
    "                0: 'ok',\n",
    "                1: 'hateful',\n",
    "            }\n",
    "    label2id = {v:k for k,v in id2label.items()}\n",
    "    model.config.label2id = label2id\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos de los hiperparámetros que se pueden ajustar son:\n",
    "* Número de épocas: número de veces que se recorre el conjunto de datos de entrenamiento.\n",
    "* Tasa de aprendizaje: tasa de actualización de los pesos de la red neuronal.\n",
    "* Los pesos de \"importancia\" de cada clase: se puede ajustar la importancia de cada clase para que el modelo se enfoque en predecir mejor aquellas clases que se encuentren más desbalanceadas.\n",
    "* El warmup ratio: se puede ajustar la tasa de aprendizaje de manera que se empiece con una tasa de aprendizaje baja y se vaya incrementando hasta llegar a la tasa de aprendizaje deseada.\n",
    "* El weight decay: se puede ajustar la tasa de regularización de los pesos de la red neuronal. (un método para evitar el sobreajuste)\n",
    "\n",
    "Para poder modificar los pesos de \"importancia\" de cada clase, es necesario modificar una instancia de la clase Trainer, la cual se encarga de entrenar el modelo. Para ello, debe usarse la función *compute_loss* que se encuentra en la clase Trainer, la cual se encarga de calcular la pérdida del modelo. Se añade a la función de pérdida entropía cruzada una lista de pesos de \"importancia\" para cada clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, parameters):\n",
    "    ''' Train model'''\n",
    "    def get_optimizer_weights(parameters):\n",
    "        ''' use parameters dict to get tensor of weights for each class '''\n",
    "        weights_class0 = parameters.get('opt_weights_cls0',0.7543)\n",
    "        weights_class1 = parameters.get('opt_weights_cls1',1.0)\n",
    "        return torch.tensor([weights_class0, weights_class1]) \n",
    "    \n",
    "    class MyTrainer(Trainer):\n",
    "        ''' redefine class Trainer such that it's possible to use weights in loss function '''\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            labels = inputs.get('labels')\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get('logits')\n",
    "            loss_fct = nn.CrossEntropyLoss(weight = get_optimizer_weights(parameters).cuda())\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss    \n",
    "    \n",
    "    \"\"\"\n",
    "    Training arguments. \n",
    "    learning_rate, weight_decay, warmup_ratio, epochs, opt_weights_cls0, opt_weights_cls1,\n",
    "    adam_beta2 and adam_epsilon are parameters to optimize.\n",
    "    \"\"\" \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./robertuito/',\n",
    "        num_train_epochs=parameters.get('epochs', 3),\n",
    "        seed=3,\n",
    "        warmup_ratio=parameters.get('warm_up_ratio', 0.0),\n",
    "        evaluation_strategy='no',\n",
    "        save_strategy='no',\n",
    "        do_eval=False,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=False,\n",
    "        learning_rate = parameters.get('lr', 5e-5),\n",
    "        weight_decay = parameters.get('weight_decay', 0.0),\n",
    "        group_by_length=True,\n",
    "        )\n",
    "    \n",
    "    trainer_args = {\n",
    "        'model': model,\n",
    "        'args': training_args,\n",
    "        'train_dataset': train_dataset,\n",
    "        'eval_dataset': valid_dataset,\n",
    "        'data_collator': data_collator,\n",
    "        'tokenizer': tokenizer,\n",
    "    }\n",
    "    #instanciate new Trainer class and Train without evaluation per epoch\n",
    "    trainer = MyTrainer(**trainer_args, compute_metrics=compute_metrics)\n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "def evaluate_model(model, valid_data):\n",
    "    ''' Evaluate model using Trainer class from huggingface and validation data '''\n",
    "    return model.evaluate(valid_data)\n",
    "\n",
    "def train_evaluate(parameterization):\n",
    "    ''' Train and evaluate a bayesian optimization trial '''\n",
    "    trained_model = train_model(model=init_model(), train_data=train_dataset, parameters=parameterization)\n",
    "    metric = evaluate_model(trained_model, valid_dataset)\n",
    "    del trained_model\n",
    "    #change metric name if not interested in recall\n",
    "    return metric['f1_cls1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización bayesiana\n",
    "Para realizar la optimización se usa el método optimize de la libreria AX. Este método recibe como parámetros la función a optimizar, así como los parámetros a optimizar y sus rangos de valores. El método optimize regresa una lista de diccionarios, cada uno de los cuales contiene los valores de los parámetros que se obtuvieron al optimizar la función de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run bayesian optimization with defined parameters and its characteristics\n",
    "#see https://ax.dev/api/core.html#parameter for more info about parameters\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {'name': 'lr', 'type': 'range', 'bounds': [1e-6, 8e-5], 'log_scale': True, 'value_type':'float'},\n",
    "        {'name': 'weight_decay', 'type': 'range', 'bounds': [0.005, 0.50], 'log_scale': True, 'value_type':'float'},\n",
    "        {'name': 'warm_up_ratio', 'type': 'range', 'bounds': [0.0, 0.8], 'log_scale': False, 'value_type':'float'},\n",
    "        {'name': 'epochs', 'type': 'range', 'bounds': [1, 6], 'value_type':'int'},\n",
    "        {'name': 'opt_weights_cls0', 'type': 'range', 'bounds': [0.60, 1.0], 'log_scale': True, 'value_type':'float'},\n",
    "        {'name': 'opt_weights_cls1', 'type': 'range', 'bounds': [1.0, 3.5], 'log_scale': True, 'value_type':'float'},        \n",
    "    ],\n",
    "    evaluation_function=train_evaluate,\n",
    "    #change metric name if not interested in recall\n",
    "    objective_name='f1_cls1',\n",
    ")\n",
    "\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al terminar la optimización bayesiana, se imprime el mejor conjunto de parámetros encontrado, como en la siguiente imagen:\n",
    "<figure>\n",
    "    <img src=\"./assets/images/bayesian_opt.png\"\n",
    "         alt=\"best parameters found\"\n",
    "         style=\"max-width: 70%; height: auto\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "Para guardar los parámetros encontrados para cada iteración así como sus resultados, se usa el siguiente código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get and save data from every trial\n",
    "data = experiment.fetch_data()\n",
    "params_by_arm = pd.DataFrame([experiment.arms_by_name[i].parameters for i in experiment.arms_by_name]).reset_index()\n",
    "params_by_arm.rename(columns={'index':'trial_index'}, inplace=True)\n",
    "data = data.df.merge(params_by_arm, on='trial_index')\n",
    "data.to_csv('./assets/data/parameters_per_run_recall.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtendrá un resultado similar al siguiente\n",
    "<figure>\n",
    "    <img src=\"./assets/images/opt_results.png\"\n",
    "         alt=\"parameters and results bayesian optimization\"\n",
    "         style=\"max-width: 60%; height: auto\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La el paquete AX durante la busqueda de hiperparámetros realiza una estimación de las curvas de nivel de la función objetivo (para pares de parámetros) y del error relacionado a dicha estimación. Una [currva de nivel](https://en.wikipedia.org/wiki/Contour_line) es un conjunto de lineas que conectan puntos de igual valor de una función. En este caso, las curvas de nivel son las curvas que conectan puntos de igual valor de la función objetivo. En la siguiente imagen se muestra un ejemplo de curvas de nivel de la función objetivo para el experimento planteado en función de los pesos de importancia.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/images/curva_nivel.png\"\n",
    "         alt=\"Curvas de nivel\">\n",
    "    <figcaption>Curvas de nivel para los pesos de importancia</figcaption>\n",
    "</figure>\n",
    "\n",
    "Dado que para este experimento se buscaba maximizar la métrica de exahustividad (recall) para la clase más desbalanceada, en la curva verde, que corresponde a los promedios de cada mini experimeto, se busca que el valor sea máximo (verde oscuro). Mientras que la curva azul, que corresponde al error estándar de las estimaciones, se busca que sea mínimo (azul claro).\n",
    "\n",
    "El código para guardar las curvas de nivel por cada posible par de parámetros en formato html es el siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all 2-combinations (x, y) of every optimized parameter. This is needed for saving all posible countour plots\n",
    "w = ['lr', 'weight_decay', 'warm_up_ratio', 'epochs','opt_weights_cls0', 'opt_weights_cls1']\n",
    "all_combination = list(itertools.combinations(w, 2))\n",
    "\n",
    "#save all posible countour plots\n",
    "for combination in all_combination:\n",
    "    figure = plot_contour_plotly(model=model, param_x=str(combination[0]), param_y=str(combination[1]), \n",
    "                        metric_name='eval_recall').write_html(\"./Bayesian_opt_plots_recall/{}_{}.html\".format(\n",
    "    combination[0], combination[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien puden usarse los hiperparámetros que se imprimen al finalizar la búsqueda bayesiana, pueden usarse las curvas de nivel para determinar otro conjunto de hiperparámetros que se encuentren en una zona de alta densidad de puntos (verde oscuro) y con un error bajo (azul claro). Sí es el caso, debera comprobarse que dicho conjunto es mejor que el que se obtuvo al finalizar la búsqueda bayesiana."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cad67f8f08b43f52430a6b8684b01d2984f30490bfa5827628ebc9dc8cbf0dc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
