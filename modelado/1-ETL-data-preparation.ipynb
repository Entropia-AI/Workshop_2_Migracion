{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook \\# 1: Extracción, transformación y carga de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias útiles\n",
    "\n",
    "Una vez que se cuenta con el conjunto de datos etiquetado, éste debe ser dividido en tres subconjuntos que llamaremos \"entrenamiento\", \"validación\" y \"prueba\", divididos en 70%, 10% y 20% de los datos. Esta división es común dentro de la ciencia de datos y está basada en el *principio de Pareto*.\n",
    "\n",
    "Además de la división es necesario transformar el texto de cada tweet, pues recordemos cada texto puede contener emoticonos 🇧🇷🇨🇷🇳🇮🇲🇽, hashtags *#VivaLaMigración*, menciones a usuarios [@el_BID](https://twitter.com/el_bid?lang=es) y links a páginas externas. Algunos de estos elementos pueden contener información útil para la detección de sentimiento y/o xenofobia (como los hashtags), pero otros pueden no ser nada informativos (como los nombres de usuario).\n",
    "\n",
    "##### Scikit learn\n",
    "Se trata de una librería de código abierto enfocada en proveer herramientas de aprendizaje de máquinas tales como modelos estadísticos y matemáticos, así como métricas de evaluación comunes en algoritmos de aprendizaje de máquinas.\n",
    "<figure>\n",
    "    <img src=\"./assets/images/scikit.png\"\n",
    "         alt=\"scikit-learn logo\"\n",
    "         style=\"max-width: 40%; height: auto\">\n",
    "    <figcaption>scikit-learn logo</figcaption>\n",
    "</figure>\n",
    "\n",
    "Esta librería nos permitirá dividir el conjunto de datos en los subconjuntos mencionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#data reading\n",
    "import pandas as pd\n",
    "\n",
    "#data processing\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "import re\n",
    "\n",
    "#scikit-learn creacion de conjuntos de entrenamiento, prueba y validación\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando los datos etiquetados\n",
    "Se cargan los datos etiquetados en un DataFrame de pandas, el cual es una estructura de datos de alto rendimiento y fácil de usar, que nos permite manipular los datos de forma eficiente. Para esta ocasión solo se usan las columnas \"id\", \"text\" y \"label\" por motivos de simplicidad y confidencialidad. Los datos extraídos de Twitter se pueden recuperar a través del id del tweet y los datos guardadps en mongo DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>1386572116670271488</td>\n",
       "      <td>@OviedoJoelys Desde Perú aquí una venezolana e...</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>1206336800282816512</td>\n",
       "      <td>@yanferluis @virginiatreyes Amiga, no desmayes...</td>\n",
       "      <td>2021-09-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6465</th>\n",
       "      <td>1114601492206497792</td>\n",
       "      <td>Eso es nada. Sólo en Venezuela hay más de 5.00...</td>\n",
       "      <td>2021-11-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8149</th>\n",
       "      <td>1264945348499379968</td>\n",
       "      <td>@PeCazafantasmas @DanielaBrandon La verdad muy...</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9656</th>\n",
       "      <td>1294699845660413952</td>\n",
       "      <td>Los  migrantes pobres del mundo si están sufri...</td>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "5401  1386572116670271488  @OviedoJoelys Desde Perú aquí una venezolana e...   \n",
       "3922  1206336800282816512  @yanferluis @virginiatreyes Amiga, no desmayes...   \n",
       "6465  1114601492206497792  Eso es nada. Sólo en Venezuela hay más de 5.00...   \n",
       "8149  1264945348499379968  @PeCazafantasmas @DanielaBrandon La verdad muy...   \n",
       "9656  1294699845660413952  Los  migrantes pobres del mundo si están sufri...   \n",
       "\n",
       "            date  label  \n",
       "5401  2021-09-01      0  \n",
       "3922  2021-09-04      0  \n",
       "6465  2021-11-19      0  \n",
       "8149  2021-06-29      0  \n",
       "9656  2021-10-19      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./assets/data/xeno_data_workshop.csv')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos el texto de un tweet etiquetado como xenofóbico y otro como no xenofóbico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet marcado como xenofóbo:\n",
      " @ahope71 @lopezobrador_ Es simple, parar los migrantes antes de llegar a México o siquiera mostrar la intención de hacerlo.\n",
      "Tweet marcado como NO xenofóbo:\n",
      " Después de un 2018 de inmigrantes venezolanos tan fuerte viene una etapa más triste y de reencuentro.\n"
     ]
    }
   ],
   "source": [
    "print('Tweet marcado como xenofóbo:\\n {}\\nTweet marcado como NO xenofóbo:\\n {}'.format(data[data.label==1].sample(n=1).text.values[0], data[data.label==0].sample(n=1).text.values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de textos\n",
    "\n",
    "Como ya se mencionó, cada texto puede contener emoticonos, hashtags, menciones a usuarios y links a páginas externas. Algunos de estos elementos pueden contener información útil para la detección de sentimiento y/o xenofobia (como los hashtags), pero otros pueden ser irrelevantes (como los nombres de usuario). Por lo tanto, es necesario limpiar el texto de cada tweet para que solo contenga palabras y signos de puntuación.\n",
    "\n",
    "Para clarificar el proceso de procesamiento de texto, se muestra un texto de ejemplo, generado sintéticamente, el cual contiene emojis, hastags, expresiones de risa, links a páginas web. Este ejemplo fue generado a modo de capturar la mayor cantidad de elementos que se pueden encontrar en un tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que opinas de la reformas Migratorias?👀👀 mi buen amigo @Pepe?\n",
      "Yo pienso que DEbieron facilitar la entrada de los inmigrantes 💁🇧🇷🇨🇷🇳🇮🇲🇽💪#YoTeApoyo eeeexxxxxx jajajajaja. Página oficial del BID:          https://www.iadb.org/es \n"
     ]
    }
   ],
   "source": [
    "example_text = 'Que opinas de la reformas Migratorias?👀👀 mi buen amigo @Pepe?\\nYo pienso que DEbieron facilitar la entrada de los inmigrantes 💁🇧🇷🇨🇷🇳🇮🇲🇽💪#YoTeApoyo eeeexxxxxx jajajajaja.'\n",
    "bid_url = ' Página oficial del BID:          https://www.iadb.org/es '\n",
    "example_text = example_text + bid_url\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procesado de cada texto se deja a cargo del método `preprocess_tweet` el cual se encarga de transformar los emojis, hashtags, menciones a usuarios y links a páginas externas, normalizar las rizas y acortar palabras repetidas. Este método fue creado por el equipo de [Pysentimiento](https://github.com/pysentimiento/pysentimiento)\n",
    "\n",
    "Este método permite al usuario definir la transformación de cada elemento mencionado, por ejemplo, las menciones de usuario pueden ser modificadas a un token especial, como por ejemplo `@user`, `@usuario`, `@usr`, o pueden ser eliminadas del texto. La transformación a tokens especiales permite que el modelo aprenda a identificar este tipo de estructuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Que opinas de la reformas Migratorias?👀👀 mi buen amigo @usuario?\\nYo pienso que DEbieron facilitar la entrada de los inmigrantes 💁🇧🇷🇨🇷🇳🇮🇲🇽💪hashtag yo te apoyo eexx jaja. Página oficial del BID:  url'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_tweet(example_text, user_token='@usuario', url_token='url', preprocess_hashtags=True, demoji=False, shorten=2, normalize_laughter=True, hashtag_token='hashtag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el texto cambia los links por la palabra definida \"url\", los nombres de usuario por \"@usuario\", separa las palabras contenidas en un hashtag y además agrega \"hashtag\", normaliza las expresiones de risa y las letras repetidas. Describe el contenido de cada emoticono.\n",
    "\n",
    "Las palabras definidas serán muy útiles para el modelo, pues éste puede ser configurado para que considere dichas palabras como elementos estructurales y no como palabras.\n",
    "\n",
    "Sin embargo, se observa que el texto contiene espacios, mantiene los acentos, los saltos de línea y las mayúsculas. Estas características podrían ser no deseadas según el objetivo que se tiene y el modelo que se empleará para clasificar los textos. Ya que existen modelos que hacen diferencia entre estas características y otros que no.\n",
    "\n",
    "Para sustituir multiples espacios en blanco (incluye saltos de linea) por un solo espacio en blanco se usa la expresión regular `r'\\s+'`. Para el siguiente ejemplo se sustituyen los espacios en blanco por el símbolo `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Que#opinas#de#la#reformas#Migratorias?👀👀#mi#buen#amigo#@Pepe?#Yo#pienso#que#DEbieron#facilitar#la#entrada#de#los#inmigrantes#💁🇧🇷🇨🇷🇳🇮🇲🇽💪#YoTeApoyo#eeeexxxxxx#jajajajaja.#Página#oficial#del#BID:#https://www.iadb.org/es#'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove extra spaces\n",
    "re.compile(r'\\s+').sub('#', example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntemos estos conceptos para crear un método que nos permita limpiar el texto de cada tweet de manera automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet, user_token='@usuario', url_token='url', hashtag_token='hashtag', preprocess_hashtags=True, demoji=True, shorten=2, normalize_laughter=True):\n",
    "    \"\"\"Function to clean a tweet\n",
    "\n",
    "    Args:\n",
    "        tweet (str): text to clean\n",
    "        user_token (str, optional): token to replace user mentions. Defaults to '@usuario'.\n",
    "        url_token (str, optional): token to replace urls. Defaults to 'url'.\n",
    "        hashtag_token (str, optional): token to replace hashtags. Defaults to 'hashtag'.\n",
    "        preprocess_hashtags (bool, optional): if True, hashtags are preprocessed. Defaults to True.\n",
    "        demoji (bool, optional): if True, emojis are replaced by their description. Defaults to True.\n",
    "        shorten (int, optional): if > 0, words are shortened to the specified length. Defaults to 2.\n",
    "        normalize_laughter (bool, optional): if True, laughter is normalized. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned tweet\n",
    "    \"\"\"\n",
    "    tweet = str(tweet)\n",
    "    tweet = preprocess_tweet(tweet, user_token=user_token, url_token=url_token, preprocess_hashtags=preprocess_hashtags, demoji=demoji, shorten=shorten, normalize_laughter=normalize_laughter, hashtag_token=hashtag_token)\n",
    "    tweet = re.compile(r'\\s+').sub(' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#apply function to all tweets\n",
    "data['text'] = data.text.apply(lambda x: clean_tweet(x))\n",
    "#remove initial and final spaces\n",
    "data['text'] = data.text.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobemos que el texto se ha limpiado correctamente, a través de la selección aleatoria de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1166364172050980864,\n",
       "        '@usuario No. Considere la fuerza de trabajo en Chile total y el número de inmigrantes empleados . Ese ejercicio me dio algo como un 6.3. 12500 inmigrantes 6.6 % de la población 80% activos',\n",
       "        '2021-11-22', 0]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(n=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División en conjuntos de entrenamiento, validación y prueba\n",
    "\n",
    "La división en tres subconjuntos del conjunto de datos es de suma importancia, ya que cada uno de ellos juega un rol importante para la creación de un modelo de aprendizaje de máquinas.\n",
    "\n",
    "* El conjunto de entrenamiento, es por lo regular, el que contiene más muestras, ya que a partir de estos datos el modelo debe aprender y/o generalizar las características lingüísticas de los tweets marcados como xenófobos y no xenófobos de la mejor forma posible. De esta manera, cuando el modelo deba clasificar un texto nuevo, su etiqueta sea correcta.\n",
    "\n",
    "* El conjunto de validación, es por lo regular, el que contiene menos muestras. Este conjunto nos permitirá realizar múltiples evaluaciones sobre el modelo, ya sea para determinar si existen mejoras al momento de variar cualquier parámetro involucrado con el modelo, aumentar el conjunto de entrenamiento o incluso evaluar la selección de modelos.\n",
    "\n",
    "* El conjunto de prueba nos permite realizar la evaluación final del modelo. Por lo regular este conjunto solo debe usarse una vez, esto para evitar posibles sesgos.\n",
    "\n",
    "En el aprendizaje de máquinas es común aplicar el principio de Pareto, el cual sostiene que la división óptima es 80% de los datos para entrenamiento y el restante para tareas de evaluación. Sin embargo, dado que se sabe que el modelo pasará por una serie de algoritmos de optimización, sí se usara esta división podría lograrse un sesgo en cuanto al rendimiento real del modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la división en subconjuntos debe considerarse también la siguiente pregunta ¿El conjunto de datos total está balanceado? Al decir balanceado nos referimos a que las etiquetas disponibles son proporcionales entre sí. Para nuestro caso, dado que se trabaja con únicamente dos etiquetas, diremos que nuestro conjunto de datos es balanceado sí y sólo sí aproximadamente el 50% de los datos contienen la etiqueta *xenófobo*, y el restante corresponde a la etiqueta *no xenofóbo*.\n",
    "\n",
    "Cuando el conjunto de datos se encuentra desbalanceado (como es el caso) la división en subconjuntos debe hacerse con cuidado, a modo de distribuir proporcionalmente la o las clases más desbalanceadas en cada uno de los tres.\n",
    "\n",
    "Un ejemplo de omitir esta distribución puede suceder como sigue: Suponga que se tiene un conjunto de datos total de 1000 muestras, donde 780 corrresponden a la clase A y 220 a la clase B. Se hace la división de forma aleatoria a modo de que el conjunto de entrenamiento contiene sólo 20 muestras de la clase B y 680 de la clase A. Tanto el conjunto de validación y prueba contendrán 100 muestras de la clase A y 200 de la clase B. Dado que las muestras de la clase B en el conjunto de entrenamiento son pocas, el modelo no podrá aprender lo suficiente de la clase B y por ende, su rendimiento para esta clase será muy bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data before splitting for sanity\n",
    "data = data.sample(frac=1, ignore_index=True)\n",
    "\n",
    "#vamos a dividir el dataset por su id y etiqueta\n",
    "X = data.id.values\n",
    "y = data.label.values\n",
    "#genera los conjuntos de entrenamiento y prueba, se reparte equitativamente las etiquetas en cada conjunto\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, \n",
    "                                    y, test_size=0.2, random_state=2022, stratify=y)\n",
    "#de x_test y y_test, genera los conjuntos de prueba y validación\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.125, random_state=2022,\n",
    "                                                  stratify=y_train)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construye los conjuntos de entrenamiento, prueba y validación a partir de la división de los ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data.id.isin(x_train)].reset_index(drop=True)\n",
    "test_df = data[data.id.isin(x_test)].reset_index(drop=True)\n",
    "valid_df = data[data.id.isin(x_val)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ojo veamos la distribución de las etiquetas en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5469\n",
       "1    1531\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of the labels by eye\n",
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1256672684454359040</td>\n",
       "      <td>OJO Solidaridad emoji manos aplaudiendo emoji ...</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1086709855673629952</td>\n",
       "      <td>@usuario @usuario Paga a hacienda, vuelve a Pe...</td>\n",
       "      <td>2021-07-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1199252043774464000</td>\n",
       "      <td>@usuario Una muy buena noticia para nuestros h...</td>\n",
       "      <td>2021-11-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1308390290458320896</td>\n",
       "      <td>@usuario Cresta! Pobre gente y pobre de nosotr...</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1347265781424398336</td>\n",
       "      <td>@usuario Si fueran migrantes estarían transmit...</td>\n",
       "      <td>2021-10-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1256672684454359040  OJO Solidaridad emoji manos aplaudiendo emoji ...   \n",
       "1  1086709855673629952  @usuario @usuario Paga a hacienda, vuelve a Pe...   \n",
       "2  1199252043774464000  @usuario Una muy buena noticia para nuestros h...   \n",
       "3  1308390290458320896  @usuario Cresta! Pobre gente y pobre de nosotr...   \n",
       "4  1347265781424398336  @usuario Si fueran migrantes estarían transmit...   \n",
       "\n",
       "         date  label  \n",
       "0  2021-08-16      0  \n",
       "1  2021-07-22      0  \n",
       "2  2021-11-15      0  \n",
       "3  2021-10-20      1  \n",
       "4  2021-10-03      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the final datasets\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarda en un archivo de texto los conjuntos de entrenamiento, validación y prueba por separado, esto hará más sencillo el entrenamiento y evaluación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the datasets\n",
    "train_df.to_csv('./assets/data/train.csv', index=False)\n",
    "test_df.to_csv('./assets/data/test.csv', index=False)\n",
    "valid_df.to_csv('./assets/data/valid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bid')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cad67f8f08b43f52430a6b8684b01d2984f30490bfa5827628ebc9dc8cbf0dc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
