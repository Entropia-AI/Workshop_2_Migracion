{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recent-advancement",
   "metadata": {},
   "source": [
    "# Definir Queries √ötiles\n",
    "\n",
    "En esta notebook vamos a revisar una propuesta del proceso para definir queries que obtengan un volumen suficiente de tweets constantemente. Se usar√° de ejemplo el proceso realizado para el proyecto del Laboratorio de Migraci√≥n.\n",
    "\n",
    "Para ello, se ver√°n los siguientes puntos:\n",
    "- Identificar lenguajes y regiones a minar\n",
    "- Obtener un primer dataset\n",
    "- N-Grams para expandir queries\n",
    "- Proceso iterativo\n",
    "\n",
    "Nota: Para este proceso nos interesa obtener un volumen considerable de tweets, por lo que entre m√°s d√≠as se puedan minar, mejor. En caso de contar con acceso [*Essential* o *Elevated*](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level) se recomienda [aplicar de manera gratuita a credenciales Acad√©micas](https://developer.twitter.com/en/products/twitter-api/academic-research). Esta notebook se hacen queries dando por hecho que las credenciales son acad√©micas. En el proyecto de Laboratorio de Migraci√≥n se pudo obtener sin mayor problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "massive-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pendulum\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from twarc.client2 import Twarc2\n",
    "from twarc.expansions import TWEET_FIELDS\n",
    "from twarc.expansions import ensure_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-witch",
   "metadata": {},
   "source": [
    "Nota: Estas son funciones desarrolladas por el equipo de entropia.ai, el c√≥digo est√° en el directorio de helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "confused-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.clean_tweets import clean_tweets_dataset\n",
    "from helpers.get_n_grams import get_n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prescription-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_TWITTER_API = {\n",
    "    'bearer_token': \"Enter your own Bearer token\",\n",
    "\n",
    "    # 'api_key': \"Enter your own API Key\",\n",
    "    # 'api_secret_key': \"Enter your own API Secret Key\",\n",
    "    # 'access_token': \"Enter your own access_token\",\n",
    "    # 'access_token_secret': \"Enter your own access_token_secret\"\n",
    "}\n",
    "\n",
    "IS_ACADEMIC = False # Cambiar a True, si las credenciales son Academicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-robertson",
   "metadata": {},
   "source": [
    "Esta funci√≥n es la que se encarga de minar los tweets, dada una lista de queries y un intervalo de tiempo. La notebook 1_Fundamentos_Twitter_API.ipynb contiene m√°s detalles de qu√© es y como funciona la API de Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "liberal-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(credentials_api,\n",
    "               queries_list,\n",
    "               output_file,\n",
    "               since_date, until_date,\n",
    "               is_academic=False):\n",
    "    \"\"\"Function in charge of scrape tweets from the \n",
    "    official Twitter API, using the library named Twarc.\n",
    "\n",
    "    Args:\n",
    "        credentials_api (dict): Dictionary with the Twitter API credentials.\n",
    "        queries_list (list[str]): List of queries to scrape.\n",
    "        output_file (str): Path to the file in which to store the results.\n",
    "        since_date (datetime): Start of the time span to scrape.\n",
    "        until_date (datetime): End of the time span to scrape.\n",
    "        is_academic (bool, optional): If the credentials has Research \n",
    "                                      Academic access level.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instiate the Twarc Client\n",
    "    twarc_client = Twarc2(**credentials_api)\n",
    "\n",
    "    # Make some tweaks for using the research credentials\n",
    "    max_size = 100\n",
    "    tweet_fields = TWEET_FIELDS.copy()\n",
    "    search_func = twarc_client.search_recent\n",
    "    if(is_academic):\n",
    "        search_func = twarc_client.search_all\n",
    "        max_size = 500\n",
    "\n",
    "        # Remove the context_annotations attr to\n",
    "        # scrape 500 tweets per request\n",
    "        tweet_fields.remove('context_annotations')\n",
    "\n",
    "    tweet_fields = ','.join(tweet_fields)\n",
    "\n",
    "    with open(output_file, 'a') as pages_file:\n",
    "        for query in tqdm(queries_list):\n",
    "\n",
    "            search_results = search_func(query=query,\n",
    "                                         start_time=since_date,\n",
    "                                         end_time=until_date,\n",
    "                                         tweet_fields=tweet_fields,\n",
    "                                         max_results=max_size)\n",
    "\n",
    "            # Write all the obtained tweets\n",
    "            for page in search_results:\n",
    "\n",
    "                # Write one by one the tweets\n",
    "                for tweet in ensure_flattened(page):\n",
    "                    json.dump(tweet, pages_file, ensure_ascii=False)\n",
    "                    pages_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-distributor",
   "metadata": {},
   "source": [
    "## Primer Dataset\n",
    "\n",
    "### Identificar pa√≠ses y lenguajes de inter√©s\n",
    "Un primer paso es identificar en que lenguaje y pa√≠ses nos interesan minar, en el caso del Laboratorio de Migraci√≥n se minan tweets de los pa√≠ses hispanohablantes de LATAM (espa√±ol), pa√≠ses del caribe (ingl√©s) y Brasil (portugu√©s). Dependiendo de que tanto se habla ese idioma en el mundo y los pa√≠ses que nos interesa cubrir, van a cambiar que tan espec√≠ficos o generales pueden que ser los queries y qu√© operadores usar√≠amos.\n",
    "\n",
    "### Ejemplos proyecto Laboratorio de Migraci√≥n\n",
    "\n",
    "### Ingl√©s\n",
    "Tweets en ingl√©s se publican desde todas partes del mundo y en este caso, solo nos interesan ciertos pa√≠ses que por densidad de poblaci√≥n no pueden generar un gran volumen de tweets. Por lo que si no usamos el operador `place_country`, vamos a obtener tweets de todo el mundo y eso significar√≠a invertir m√°s tiempo y recursos en el minado, procesamiento, etc. Para evitar esto, desde ya se puede definir que usaremos el operador `place_country` para as√≠ limitar la b√∫squeda de tweets a esos pa√≠ses y esto nos dar√° la oportunidad de usar keywords con un alcance m√°s abierto. \n",
    "\n",
    "En el ejemplo de migraci√≥n se podr√≠an usar: `migrants, immigrants, emigrants, migration, immigration, emigration, migratory, immigratory y emigratory` y luego a√±adir variaciones como `pro-migrants, anti-migrants, ...`\n",
    "\n",
    "Query ejemplo: `(migrants OR immigrants OR emigrants OR migration OR immigration OR emigration OR migratory OR immigratory OR emigratory) (place_country:BB OR place_country:GY OR place_country:BZ OR place_country:JM OR place_country:HT OR place_country:SR)`\n",
    "\n",
    "### Portugu√©s\n",
    "En este caso, los tweets en portugu√©s van a ser principalmente de Brasil o de Portugal, por lo que se podr√≠a aplicar el operador `place_country` para obtener solo tweets publicados desde Brasil o minar queries sin ninguna restricci√≥n para luego filtrarlos bas√°ndose en s√≠ son geo-localizables a otro lado que no sea Brasil. \n",
    "\n",
    "Palabras claves podr√≠an ser: `migrat√≥rias, migra√ß√£o, migra√ß√µes, migrantes, migradas`\n",
    "\n",
    "Query ejemplo: `(migrat√≥rias OR migra√ß√£o OR migra√ß√µes OR migrantes OR migradas) place_country:BR`\n",
    "\n",
    "### Espa√±ol\n",
    "Para este lenguaje, exceptuando a Espa√±a, nos interesa obtener tweets del conjunto de pa√≠ses que van a generar la mayor√≠a de tweets en espa√±ol. Y aunque es buena pr√°ctica utilizar queries m√°s general junto con el operador `place_country`, esto nos dejar√≠a con un volumen muy inferior y que no terminar√≠a de reflejar las conversaciones reales de lo que se podr√≠a obtener.\n",
    "\n",
    "Por lo que aqu√≠ conviene hacer ambas cosas, tener queries con keywords generales, pero cerrado a solo los pa√≠ses de inter√©s y tener queries con keywords m√°s espec√≠ficos, pero sin ninguna limitaci√≥n de geolocalizaci√≥n, para luego tratar de extraer el pa√≠s de origen por otros atributos (Esto se va a revisar en la notebook llamada 3_Variables_Demograficas.ipynb).\n",
    "\n",
    "Query ejemplo: `(migrar OR inmigrar OR emigrar OR migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes OR migratorios OR inmigratorios OR emigratorios OR migratorias OR inmigratorias OR emigratorias) (place_country:MX OR place_country:AR OR place_country:AR)`.\n",
    "\n",
    "Esto se replicar√≠a para todos los pa√≠ses que nos interesa y nos servir√≠a como primer dataset para iterar sobre los queries. No solo para los queries con el filtro por pa√≠s, tambi√©n para aquellos que no lo tienen, ya que esos debe de ser m√°s espec√≠ficos para no traernos demasiado ruido, pero que cubran la mayor parte de la conversaci√≥n y as√≠ tener un volumen suficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-cleaners",
   "metadata": {},
   "source": [
    "Para esta notebook usaremos de ejemplo la definici√≥n de queries en espa√±ol, y como se mencion√≥ previamente, el primer paso es definir una lista preliminar de keywords y concatenarlas con el operador `OR`. Optar por palabras que de manera expl√≠cita hable del tema de inter√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "frequent-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keywords = [\"migrar\", \"inmigrar\", \"emigrar\", \n",
    "                 \"migrante\", \"inmigrante\", \"emigrante\", \n",
    "                 \"migrantes\", \"inmigrantes\", \"emigrantes\", \n",
    "                 \"migratorios\", \"inmigratorios\", \"emigratorios\", \n",
    "                 \"migratorias\", \"inmigratorias\", \"emigratorias\"]\n",
    "list_keywords = ' OR '.join(list_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-complement",
   "metadata": {},
   "source": [
    "En seguida hay que a√±adir los operadores para filtrar por pa√≠ses. Esto nos ayudar√° a encontrar keywords o expresiones que est√©n atadas al contexto de cada pa√≠s, ayudando as√≠ a filtrar un poco la conversaci√≥n migratoria.\n",
    "\n",
    "Tambi√©n se a√±ade el operador `-is:retweet` y `lang:es`, evitando traer retweets, ya que por el momento solo nos interesa obtener los tweets √∫nicos y que est√©n en espa√±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stock-simple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(migrar OR inmigrar OR emigrar OR migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes OR migratorios OR inmigratorios OR emigratorios OR migratorias OR inmigratorias OR emigratorias) (place_country:MX OR place_country:AR OR place_country:CO) lang:es -is:retweet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_queries = f\"({list_keywords}) (place_country:MX OR place_country:AR OR place_country:CO) lang:es -is:retweet\"\n",
    "lst_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-occasion",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-space",
   "metadata": {},
   "source": [
    "Definimos un intervalo de tiempo considerable para obtener un volumen √∫til de tweets. En el proyecto del Laboratorio de Migraci√≥n se empez√≥ con minar tweets de todo el a√±o 2019 y 2020, al ser tanto tiempo, podremos obtener tweets de momentos en los que ocurrieran eventos relacionados con migraci√≥n, as√≠ como en momentos donde sea un tema menos viral. \n",
    "Para este ejemplo usaremos todo lo que va del 2022.\n",
    "\n",
    "Nota: Todos los archivos que se obtengan durante el desarrollo de esta notebook se van a encontrar en el directorio: \"./files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "contrary-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_queries = [lst_queries]\n",
    "\n",
    "date_start = pendulum.datetime(year=2022, month=1, day=1)\n",
    "date_end = pendulum.datetime(year=2022, month=9, day=10)\n",
    "\n",
    "file_tweets = os.path.abspath(\"./files/1_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-embassy",
   "metadata": {},
   "source": [
    "Se ejecuta la funci√≥n para minar los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "underlying-making",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:27<00:00, 87.22s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-devon",
   "metadata": {},
   "source": [
    "Vemos cuantos tweets obtuvimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "green-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8769 ./files/1_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/1_dataset.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-chamber",
   "metadata": {},
   "source": [
    "Estos 8,593 tweets nos van a servir como primer dataset para identificar keywords nuevas que est√©n relacionadas con el fen√≥meno que queremos estudiar, ya sea que se refieran a el de manera:\n",
    "- Explicita: \"migrante\" o \"inmigrantes\" \n",
    "- Impl√≠cita: \"los mexicanos en estado unidos\" o \"las personas ilegales\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-therapist",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-cache",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "\n",
    "Estas son un concepto muy utilizado en el procesamiento de lenguaje natural (NLP, por siglas en ingl√©s), y se pueden entender como una secuencia de N palabras, por ejemplo:\n",
    "- Dream on (2-Grama)\n",
    "- Kickstart My Heart (3-Grama)\n",
    "- Hail to the King (4-Grama)\n",
    "\n",
    "Estas secuencias de texto se pueden contabilizar en un corpus de texto y obtener la frecuencia de aparici√≥n de una palabra tras la otra, y as√≠ saber qu√© palabras puede considerarse como una sola entidad. Por ejemplo, en un texto sobre la historia de la banda de rock Aerosmith, el 2-grama \"Dream On\" se puede considerar una sola palabra, dado que se refiere a la canci√≥n que sirvi√≥ como soundtrack de la pel√≠cula  Armagedon. Tambi√©n puede servir para realizar una predicci√≥n de qu√© palabras siguen, como el autocompletado que realiza Spotify para mostrar una lista de opciones de la canci√≥n original y covers cuando solo escribimos \"Kick Start...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-contrary",
   "metadata": {},
   "source": [
    "Podemos considerar los N-Grams como una herramienta √∫til para encontrar patrones entre los tweets que hablan sobre migraci√≥n y as√≠ extender la lista de keywords y/o formar queries que se apoyen en que un tweet contenga al menos dos keywords.\n",
    "\n",
    "Primero hay que preprocesar los tweets para as√≠ pasar todo a min√∫sculas y eliminar partes del texto que no sean de utilidad:\n",
    "- URLs\n",
    "- N√∫meros\n",
    "- Emojis\n",
    "- Caracteres especiales\n",
    "- Risas (Jajaj o Hahaha)\n",
    "- Espacios en blanco m√∫ltiples\n",
    "- Menci√≥n a usuarios \n",
    "- Borrar stopwords (que, como, cu√°l, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-interference",
   "metadata": {},
   "source": [
    "Para lograr hacer esto primero hay que cargar los identificadores √∫nicos (IDs) y textos de los tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spread-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_json(\"./files/1_dataset.jsonl\", lines=True)\n",
    "df_tweets = df_tweets.drop_duplicates(subset=['text']).reset_index(drop=True)\n",
    "df_tweets = df_tweets[['id', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "generous-basis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Derechos humanos, respeto, uni√≥n, pluralismo, ...</td>\n",
       "      <td>derechos humanos respeto union pluralismo dive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Nadie que no lo haya experimentado en carne pr...</td>\n",
       "      <td>experimentado carne entender realmente necesit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Un hogar lejos del hogar, ojal√° todos los migr...</td>\n",
       "      <td>hogar hogar migrantes puedan encontrar destino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Hace cuatro a√±os decid√≠ migrar. En todo este t...</td>\n",
       "      <td>anos decidi migrar argentina convertido aprend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Muy feliz d√≠a del inmigrante a todos los que f...</td>\n",
       "      <td>feliz inmigrante forjaron nacion eligen patria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>#FiestaNacionalDelInmigranteEnObera #inmigrant...</td>\n",
       "      <td>fiestanacionaldelinmigranteenobera inmigrantes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Adem√°s, entregamos un reconocimiento a la fami...</td>\n",
       "      <td>entregamos reconocimiento familia miguel balut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>#Comodoro es un crisol de razas ‚òÄÔ∏è, la tierra ...</td>\n",
       "      <td>comodoro crisol razas tierra inmigrantes eligi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>üóìÔ∏è4/09 - ùòøùôÑùòº ùòøùôÄùôá ùôÑùôâùôàùôÑùôÇùôçùòºùôâùôèùôÄ\\nDesde @Juventud_S...</td>\n",
       "      <td>saludar todxs lxs trabajadorxs migrantes forma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Desde la mesa interministerial saludamos a tod...</td>\n",
       "      <td>mesa interministerial saludamos todes migrante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Debo ser la √∫nica persona de misiones que nunc...</td>\n",
       "      <td>debo unica persona misiones fiesta inmigrante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Feliz dia para todos los inmigrantes üëèüëèüëèüëè http...</td>\n",
       "      <td>feliz inmigrantes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Feliz D√≠a del Inmigrante... https://t.co/SeGLT...</td>\n",
       "      <td>feliz inmigrante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>üü† Destacan el ‚Äòaporte hist√≥rico y cultural que...</td>\n",
       "      <td>destacan aporte historico cultural inmigrantes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Para los que vinieron y nos legaron la cultura...</td>\n",
       "      <td>vinieron legaron cultura sacrificio vienen bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>@SoherElSukaria Saludo a los inmigrantes en su...</td>\n",
       "      <td>saludo inmigrantes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>FELIZ DIA DEL \\nINMIGRANTE... https://t.co/3Uv...</td>\n",
       "      <td>feliz inmigrante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Hoy en el dia del inmigrante personalmente rin...</td>\n",
       "      <td>inmigrante personalmente rindo homenaje abuelo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Feliz d√≠a del inmigrante, fel√≠z d√≠a a todos lo...</td>\n",
       "      <td>feliz inmigrante feliz razon dejar tierra herm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>‚Å¶@ChapultepecCDMX‚Å© ahora nuestros parques con ...</td>\n",
       "      <td>parques tiendas campana migrantes frente edifi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "200  Derechos humanos, respeto, uni√≥n, pluralismo, ...   \n",
       "201  Nadie que no lo haya experimentado en carne pr...   \n",
       "202  Un hogar lejos del hogar, ojal√° todos los migr...   \n",
       "203  Hace cuatro a√±os decid√≠ migrar. En todo este t...   \n",
       "204  Muy feliz d√≠a del inmigrante a todos los que f...   \n",
       "205  #FiestaNacionalDelInmigranteEnObera #inmigrant...   \n",
       "206  Adem√°s, entregamos un reconocimiento a la fami...   \n",
       "207  #Comodoro es un crisol de razas ‚òÄÔ∏è, la tierra ...   \n",
       "208  üóìÔ∏è4/09 - ùòøùôÑùòº ùòøùôÄùôá ùôÑùôâùôàùôÑùôÇùôçùòºùôâùôèùôÄ\\nDesde @Juventud_S...   \n",
       "209  Desde la mesa interministerial saludamos a tod...   \n",
       "210  Debo ser la √∫nica persona de misiones que nunc...   \n",
       "211  Feliz dia para todos los inmigrantes üëèüëèüëèüëè http...   \n",
       "212  Feliz D√≠a del Inmigrante... https://t.co/SeGLT...   \n",
       "213  üü† Destacan el ‚Äòaporte hist√≥rico y cultural que...   \n",
       "214  Para los que vinieron y nos legaron la cultura...   \n",
       "215  @SoherElSukaria Saludo a los inmigrantes en su...   \n",
       "216  FELIZ DIA DEL \\nINMIGRANTE... https://t.co/3Uv...   \n",
       "217  Hoy en el dia del inmigrante personalmente rin...   \n",
       "218  Feliz d√≠a del inmigrante, fel√≠z d√≠a a todos lo...   \n",
       "219  ‚Å¶@ChapultepecCDMX‚Å© ahora nuestros parques con ...   \n",
       "\n",
       "                                     text_preprocessed  \n",
       "200  derechos humanos respeto union pluralismo dive...  \n",
       "201  experimentado carne entender realmente necesit...  \n",
       "202  hogar hogar migrantes puedan encontrar destino...  \n",
       "203  anos decidi migrar argentina convertido aprend...  \n",
       "204  feliz inmigrante forjaron nacion eligen patria...  \n",
       "205  fiestanacionaldelinmigranteenobera inmigrantes...  \n",
       "206  entregamos reconocimiento familia miguel balut...  \n",
       "207  comodoro crisol razas tierra inmigrantes eligi...  \n",
       "208  saludar todxs lxs trabajadorxs migrantes forma...  \n",
       "209  mesa interministerial saludamos todes migrante...  \n",
       "210      debo unica persona misiones fiesta inmigrante  \n",
       "211                                  feliz inmigrantes  \n",
       "212                                   feliz inmigrante  \n",
       "213  destacan aporte historico cultural inmigrantes...  \n",
       "214  vinieron legaron cultura sacrificio vienen bus...  \n",
       "215                                 saludo inmigrantes  \n",
       "216                                   feliz inmigrante  \n",
       "217  inmigrante personalmente rindo homenaje abuelo...  \n",
       "218  feliz inmigrante feliz razon dejar tierra herm...  \n",
       "219  parques tiendas campana migrantes frente edifi...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = clean_tweets_dataset(df_tweets, doc_column='text', clean_col='text_preprocessed')\n",
    "df_tweets[['text', 'text_preprocessed']][200:220]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-edmonton",
   "metadata": {},
   "source": [
    "Obtener bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "grateful-master",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_gram</th>\n",
       "      <th>total_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poblacion migrante</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>migrantes venezolanos</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>personas migrantes</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prov inmigrante</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brenas capital</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>capital prov</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>derechos humanos</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>migrantes refugiados</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mujeres migrantes</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lasbrenas brenas</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>forasteros inmigrantes</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>barrio grave</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fomentan sosobra</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gracias loscontenedoresdebasura</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>grave gracias</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>inmigrantes fomentan</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>inseguridad barrio</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>loscontenedoresdebasura unico</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sosobra intimidan</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>traido forasteros</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             n_gram  total_freq\n",
       "0                poblacion migrante         252\n",
       "1             migrantes venezolanos         229\n",
       "2                personas migrantes         145\n",
       "3                   prov inmigrante         133\n",
       "4                    brenas capital         132\n",
       "5                      capital prov         132\n",
       "6                  derechos humanos          99\n",
       "7              migrantes refugiados          94\n",
       "8                 mujeres migrantes          79\n",
       "9                  lasbrenas brenas          78\n",
       "10           forasteros inmigrantes          77\n",
       "11                     barrio grave          76\n",
       "12                 fomentan sosobra          76\n",
       "13  gracias loscontenedoresdebasura          76\n",
       "14                    grave gracias          76\n",
       "15             inmigrantes fomentan          76\n",
       "16               inseguridad barrio          76\n",
       "17    loscontenedoresdebasura unico          76\n",
       "18                sosobra intimidan          76\n",
       "19                traido forasteros          76"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srs_bigrams = get_n_grams(df_tweets['text_preprocessed'], \n",
    "                          n_gram = 2, \n",
    "                          n_top = 200)\n",
    "srs_bigrams[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-scotland",
   "metadata": {},
   "source": [
    "Obtener trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "operational-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_gram</th>\n",
       "      <th>total_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brenas capital prov</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>capital prov inmigrante</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lasbrenas brenas capital</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barrio grave gracias</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fomentan sosobra intimidan</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>forasteros inmigrantes fomentan</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gracias loscontenedoresdebasura unico</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grave gracias loscontenedoresdebasura</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inmigrantes fomentan sosobra</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>inseguridad barrio grave</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>loscontenedoresdebasura unico traido</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>traido forasteros inmigrantes</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>unico traido forasteros</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bogota inseguridad barrio</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>barriosdepie lasbrenas brenas</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>libresdelsur barriosdepie lasbrenas</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>acaba publicar foto</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>instituto nacional migracion</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>registro unico migrantes</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>poblacion migrante venezolana</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   n_gram  total_freq\n",
       "0                     brenas capital prov         132\n",
       "1                 capital prov inmigrante         132\n",
       "2                lasbrenas brenas capital          78\n",
       "3                    barrio grave gracias          76\n",
       "4              fomentan sosobra intimidan          76\n",
       "5         forasteros inmigrantes fomentan          76\n",
       "6   gracias loscontenedoresdebasura unico          76\n",
       "7   grave gracias loscontenedoresdebasura          76\n",
       "8            inmigrantes fomentan sosobra          76\n",
       "9                inseguridad barrio grave          76\n",
       "10   loscontenedoresdebasura unico traido          76\n",
       "11          traido forasteros inmigrantes          76\n",
       "12                unico traido forasteros          76\n",
       "13              bogota inseguridad barrio          73\n",
       "14          barriosdepie lasbrenas brenas          58\n",
       "15    libresdelsur barriosdepie lasbrenas          57\n",
       "16                    acaba publicar foto          43\n",
       "17           instituto nacional migracion          43\n",
       "18               registro unico migrantes          42\n",
       "19          poblacion migrante venezolana          40"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srs_trigramas = get_n_grams(df_tweets['text_preprocessed'], \n",
    "                            n_gram = 3, \n",
    "                            n_top = 200)\n",
    "srs_trigramas[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-superior",
   "metadata": {},
   "source": [
    "Explorando esta lista de n-gramas encontraremos que:\n",
    "- Las palabras \"migrante\", \"inmigrante\" o \"emigrante\" viene acompa√±adas de:\n",
    "  - ni√±os\n",
    "  - poblaci√≥n\n",
    "  - personas\n",
    "  - forasteros\n",
    "  - inseguridad\n",
    "  - refugiados\n",
    "  - caravana\n",
    "  - ilegales\n",
    "  - plaza\n",
    "  - venezolano\n",
    "  - hermanos\n",
    "  - familias\n",
    "  - mujer\n",
    "  - atenci√≥n\n",
    "  - grupo\n",
    "  - pa√≠ses\n",
    "  - retornados\n",
    "  - miles\n",
    "  - millones\n",
    "  - tr√°fico\n",
    "\n",
    "\n",
    "- Las palabras \"migratorio\" y \"migratoria\" vienen acompa√±adas de:\n",
    "  - leyes\n",
    "  - pol√≠tica\n",
    "  - temas\n",
    "  - pacto\n",
    "  - reformas\n",
    "  - crisis\n",
    "  - fen√≥meno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-league",
   "metadata": {},
   "source": [
    "## Primer query basado en N-Grams\n",
    "Con el grupo de palabras listadas previamente, se construye un primer query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "great-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_query = \"(migrante OR inmigrante OR emigrante) (ni√±o OR mujer OR ilegal OR muerto OR persona OR hermano OR forastero OR refugiado OR retornado OR familia OR caravana OR grupo OR poblaci√≥n OR pais OR miles OR millones OR plaza OR trafico OR inseguridad OR atenci√≥n OR venezolano)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-artist",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-fusion",
   "metadata": {},
   "source": [
    "A este primer query se le a√±ade variaciones de las palabras, por ejemplo:\n",
    "- migrante ‚®™> migrantes, inmigrante, ...\n",
    "- ni√±o -> ni√±os, ni√±a, ni√±as\n",
    "- caravana -> caravanas\n",
    "- refugiado -> refugiados, refugiada, refugiadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "governmental-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_query = \"(migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes) (ni√±o OR ni√±os OR ni√±a OR ni√±as OR mujer OR mujeres OR hombre OR hombres OR ilegal OR ilegales OR muerto OR muertos OR persona OR personas OR hermano OR hermanos OR hermana OR hermanas OR forastero OR forasteros OR forastera OR forasteras OR refugiado OR refugiados OR refugiada OR refugiadas OR familia OR familias OR caravana OR caravanas OR grupo OR grupos OR poblaci√≥n OR poblaciones OR pais OR paises OR miles OR millones OR plaza OR plazas OR trafico OR inseguridad OR atenci√≥n OR venezolano)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-score",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-invitation",
   "metadata": {},
   "source": [
    "Esto se repite para las palabras migratorio y migratoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eligible-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_query = \"(migratorio OR migratoria OR migratorios OR migratorias OR inmigratorio OR inmigratoria OR inmigratorios OR inmigratorias OR emigratorio OR emigratoria OR emigratorios OR emigratorias) (ley OR leyes OR politica OR politicas OR tema OR temas OR pacto OR pactos OR reforma OR reformas OR fenomeno OR crisis)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-american",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-feeling",
   "metadata": {},
   "source": [
    "Aprovechando que estamos trabajando con credenciales con acceso academico, podriamos incluso juntar estos dos queries con un OR para as√≠ ahorrar tiempo y requests a la API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "senior-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_query = f\"({first_query}) OR ({second_query})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-realtor",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-aluminum",
   "metadata": {},
   "source": [
    "A√±adimos el operador de `has:geo` pero negado: `-has:geo` y los operadores para evitar retweets y filtrar al espa√±ol. Con esto vamos a obtener tweets que cumplan con los keywords y que no tengan informaci√≥n de geolocalizaci√≥n. Esto porque aquellos que tengan informaci√≥n de alguna ciudad o pa√≠s, ya nos los trajimos con los queries que tienen el operador `place_country`. \n",
    "\n",
    "Para el proyecto del Laboratorio de Migraci√≥n se hizo de esta manera, y aquellos tweets que no tuvieran informaci√≥n de geolocalizaci√≥n dada por Twitter se intent√≥ inferir por medio de los atributos, *description* y *location* del usuario. Esto se ver√° en la siguiente notebook llamada \"3_Get_Location.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "consecutive-heater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes) (ni√±o OR ni√±os OR ni√±a OR ni√±as OR mujer OR mujeres OR hombre OR hombres OR ilegal OR ilegales OR muerto OR muertos OR persona OR personas OR hermano OR hermanos OR hermana OR hermanas OR forastero OR forasteros OR forastera OR forasteras OR refugiado OR refugiados OR refugiada OR refugiadas OR familia OR familias OR caravana OR caravanas OR grupo OR grupos OR poblaci√≥n OR poblaciones OR pais OR paises OR miles OR millones OR plaza OR plazas OR trafico OR inseguridad OR atenci√≥n OR venezolano)) OR ((migratorio OR migratoria OR migratorios OR migratorias OR inmigratorio OR inmigratoria OR inmigratorios OR inmigratorias OR emigratorio OR emigratoria OR emigratorios OR emigratorias) (ley OR leyes OR politica OR politicas OR tema OR temas OR pacto OR pactos OR reforma OR reformas OR fenomeno OR crisis)) -has:geo lang:es -is:retweet\n"
     ]
    }
   ],
   "source": [
    "merge_query = f\"({first_query}) OR ({second_query}) lang:es -is:retweet -has:geo\"\n",
    "print(merge_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-packing",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-action",
   "metadata": {},
   "source": [
    "Dado que minar todo lo que va del 2022 tomar√≠a mucho tiempo, aprovecharemos que la API de Twitter nos permite hacer un conteo de tweets dado un query, para as√≠ saber cuantos tweets obtendr√≠amos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "worst-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:17,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "twarc_client = Twarc2(**CREDENTIALS_TWITTER_API)\n",
    "counts_per_month = twarc_client.counts_all(query=merge_query,\n",
    "                                           start_time=date_start,\n",
    "                                           end_time=date_end,\n",
    "                                           granularity='day')\n",
    "\n",
    "df_all = []\n",
    "for month_count in tqdm(counts_per_month): \n",
    "    df_all.append(pd.DataFrame(month_count['data']))\n",
    "    \n",
    "df_all = pd.concat(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "partial-drinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si se minaran lo que va del a√±o 2022 se obtendrian 2654060 tweets\n"
     ]
    }
   ],
   "source": [
    "print(f\"Si se minaran lo que va del a√±o 2022 se obtendrian {df_all['tweet_count'].sum()} tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-sheet",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-assets",
   "metadata": {},
   "source": [
    "Pero aun as√≠ se puede hacer el ejercicio de minar un par de d√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "placed-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_queries = [merge_query]\n",
    "\n",
    "date_start = pendulum.datetime(year=2022, month=9, day=9)\n",
    "date_end = pendulum.datetime(year=2022, month=9, day=11)\n",
    "\n",
    "file_tweets = os.path.abspath(\"./files/2_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "married-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [02:12<00:00, 132.03s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "proof-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13680 ./files/2_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/2_dataset.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-scheduling",
   "metadata": {},
   "source": [
    "Con minar solo un par de d√≠as, se obtuvo mucho m√°s volumen que en todo el a√±o, con el conjunto de queries abiertos pero filtrado a ciertos pa√≠ses. Mientras que si min√°ramos todo lo que va del a√±o obtendr√≠amos 2,654,167 tweets √∫nicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-ladder",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-neighborhood",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-hobby",
   "metadata": {},
   "source": [
    "## M√°s keywords\n",
    "\n",
    "Uno de los objetivos del proyecto Laboratorio de Migraci√≥n es llevar un track de la conversaci√≥n xen√≥foba dentro de la conversaci√≥n migrante. Es decir, tweets que est√©n atacando e insultado a los migrantes, refugiados, etc. Con esto se le dar√° m√°s profundidad al estudio de la conversaci√≥n de la migraci√≥n, dado que ahora tambi√©n se podr√≠a estimar que porcentaje de ella es con un tono xen√≥fobo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-prayer",
   "metadata": {},
   "source": [
    "Para asegurar que se obtendran tweets en ese tono, se hizo el mismo ejercicio de queries abiertas y filtros por paises pero ya no solo con keywords abiertos, sino que usando operadores logicos se concatenaron con palabras que fueran racistas o xenofobas. Y se aplico el proce de N-Grams para extender esta lista de palabras.\n",
    "\n",
    "Por ejemplo, buscar tweets que contengan la palabra `migrante` y variaciones pero concatenadas con un `AND` a palabras como `ilegales, violadores, ladrones, terroristas`.\n",
    "\n",
    "Query ejemplo: `((migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes) (ilegales OR ilegal OR violadores OR violador OR ladrones OR ladron OR terroristas OR terrorista)) (place_country:MX OR place_country:AR OR place_country:CO)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-workshop",
   "metadata": {},
   "source": [
    "Primero vamos a minar nuevos tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "amateur-toronto",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((migrar OR inmigrar OR emigrar OR migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes OR migratorios OR inmigratorios OR emigratorios OR migratorias OR inmigratorias OR emigratorias) (malditos OR maldito OR ilegales OR ilegal OR violadores OR violador OR ladrones OR ladron OR terroristas OR terrorista)) (place_country:MX OR place_country:AR OR place_country:CO) lang:es -is:retweet'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query = \"(migrar OR inmigrar OR emigrar OR migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes OR migratorios OR inmigratorios OR emigratorios OR migratorias OR inmigratorias OR emigratorias) (malditos OR maldito OR ilegales OR ilegal OR violadores OR violador OR ladrones OR ladron OR terroristas OR terrorista)\"\n",
    "new_query = f\"({new_query}) (place_country:MX OR place_country:AR OR place_country:CO) lang:es -is:retweet\"\n",
    "new_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-dakota",
   "metadata": {},
   "source": [
    "Nota: Por cuestiones de volumen, se minara todo 2020, 2021 y 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "relevant-shock",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:11<00:00, 11.33s/it]\n"
     ]
    }
   ],
   "source": [
    "new_query = [new_query]\n",
    "\n",
    "date_start = pendulum.datetime(year=2020, month=1, day=1)\n",
    "date_end = pendulum.datetime(year=2022, month=9, day=10)\n",
    "\n",
    "file_tweets = os.path.abspath(\"./files/3_dataset.jsonl\")\n",
    "\n",
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = new_query,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "naughty-summer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683 ./files/3_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/3_dataset.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-excellence",
   "metadata": {},
   "source": [
    "#### Extraer N-Grams al nuevo dataset\n",
    "\n",
    "Nuevamente, se utiliza la t√©cnica de N-Grams para obtener queries que sirvan para minar tweets, ahora siendo sobre migraci√≥n y xen√≥fobos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "detailed-wireless",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_tweets = pd.read_json(\"./files/3_dataset.jsonl\", lines=True)\n",
    "df_tweets = df_tweets[['id', 'text']]\n",
    "\n",
    "df_tweets = clean_tweets_dataset(df_tweets, doc_column='text', clean_col='text_preprocessed')\n",
    "\n",
    "srs_bigrams = get_n_grams(df_tweets['text_preprocessed'], \n",
    "                          n_gram = 2, \n",
    "                          n_top = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-fruit",
   "metadata": {},
   "source": [
    "Y con esto se consiguieron nuevas palabras, por ejemplo:\n",
    "- delincuentes\n",
    "- mafiosos\n",
    "- ignorantes\n",
    "- crimen organizado\n",
    "- agresores\n",
    "- crimen\n",
    "- traficantes\n",
    "- drogadictos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-participation",
   "metadata": {},
   "source": [
    "Con estas nuevas palabras se puede armar un nuevo query para el minado sin restricci√≥n por pa√≠ses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "musical-boring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:18,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "new_query = \"(migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes) (malditos OR maldito OR ilegales OR ilegal OR violar OR violadores OR violador OR ladrones OR ladron OR terrorismo OR terroristas OR terrorista OR delincuencia OR delincuentes OR delincuente OR mafia OR mafiosos OR mafioso OR ignorantes OR ignorante OR agredir OR agresores OR agresor OR crimen OR criminal OR criminales OR trafico OR traficantes OR traficante OR drogadictos OR drogradictas OR drogradicto OR drogradicta)\"\n",
    "new_query = f\"({new_query}) -has:geo lang:es -is:retweet\"\n",
    "\n",
    "date_start = pendulum.datetime(year=2022, month=1, day=1)\n",
    "date_end = pendulum.datetime(year=2022, month=9, day=10)\n",
    "\n",
    "twarc_client = Twarc2(**CREDENTIALS_TWITTER_API)\n",
    "counts_per_month = twarc_client.counts_all(query=new_query,\n",
    "                                           start_time=date_start,\n",
    "                                           end_time=date_end,\n",
    "                                           granularity='day')\n",
    "\n",
    "df_all = []\n",
    "for month_count in tqdm(counts_per_month): \n",
    "    df_all.append(pd.DataFrame(month_count['data']))\n",
    "    \n",
    "df_all = pd.concat(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "affiliated-scenario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si se minaran lo que va del a√±o 2022 se obtendrian 142994 tweets\n"
     ]
    }
   ],
   "source": [
    "print(f\"Si se minaran lo que va del a√±o 2022 se obtendrian {df_all['tweet_count'].sum()} tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-stress",
   "metadata": {},
   "source": [
    "Este nuevo query obtendr√≠a 142,994 tweets en lo que va del a√±o. Hay que tener en cuenta que probablemente haya tweets que este query traiga tweets duplicados que existan dentro de los posibles 2 millones de tweets previamente contados, pero es muy f√°cil lidiar con tweets duplicados dado que todos tienen un ID √∫nico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-links",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-cannon",
   "metadata": {},
   "source": [
    "## Iterar el ejercicio\n",
    "\n",
    "Este ejercicio se tiene que repetir con cada dataset que vayamos generando hasta que consideramos que generamos un volumen suficiente. No hay una m√©trica escrita en piedra, por lo que es cuesti√≥n de iterar m√∫ltiples veces, a√±adiendo y quitando keywords. Algo √∫til y r√°pido para ver si nuestras keywords/operadores obtienen tweets es [utilizar la b√∫squeda avanzada dentro del portal de Twitter](https://help.twitter.com/es/using-twitter/twitter-advanced-search), con ella podremos tener una vista r√°pida de si obtiene tweets o no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-harris",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-soccer",
   "metadata": {},
   "source": [
    "### Lidiar con falsos positivos\n",
    "\n",
    "No importa cuan especificos los queries, simpre habra que lidiar con tweets que no son del tema que tratamos de estudiar. Por lo que hay que implementar algunos filtros m√°s para descartar ese subconjunto de tweets. En el caso del proyecto del Laboratorio, se opto por entrenar un modelo de clasificaci√≥n binaria para decidir si los tweets recibidos son o no sobre migraci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-military",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BID",
   "language": "python",
   "name": "bid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
