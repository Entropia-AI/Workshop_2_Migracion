{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elect-member",
   "metadata": {},
   "source": [
    "# Extracción de Tweets\n",
    "\n",
    "En esta notebook se van a revisar los pasos para lograr un buen minado de tweets. Para ello se tocan los siguientes puntos: \n",
    "- Qué es la API de Twitter y cómo utilizarla\n",
    "- Operadores para definir queries\n",
    "- Ejemplos de minado\n",
    "\n",
    "**TODO: Porque es util minar de Twitter?**\n",
    "\n",
    "Twitter provee una API (interfaz de programación de aplicaciones) para acceder a sus datos. Las API son mecanismos que permiten a dos componentes de software comunicarse entre sí mediante un conjunto de definiciones y protocolos. Por ejemplo, el sistema de software del instituto de meteorología contiene datos meteorológicos diarios. La aplicación meteorológica de su teléfono “habla” con este sistema a través de las API y le muestra las actualizaciones meteorológicas diarias en su teléfono. [Aquí una explicación de AWS](https://aws.amazon.com/es/what-is/api/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-introduction",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "\n",
    "<h3 id='credentials_api'>Credenciales para la API de Twitter</h3>\n",
    "\n",
    "Para obtener tweets con la [API Oficial de Twitter](https://developer.twitter.com/en/docs/twitter-api) es necesario registrarse en el [portal para desarrolladores de Twitter](https://developer.twitter.com/en/docs/developer-portal/overview) para así dar de alta un proyecto y que se te asignen las [credenciales necesarias](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api).\n",
    "\n",
    "Hay que aclarar que **según el tipo de credenciales y el método de autenticación serán diferente la [cantidad de tweets que podremos obtener en cierto tiempo](https://developer.twitter.com/en/docs/twitter-api/rate-limits#v2-limits), qué [operadores/filtros podemos usar](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list) y cuantos días hacia atrás podremos minar** y la extensión de los queries. \n",
    "\n",
    "Por ejemplo, los límites mensuales de obtención de tweets, dependiendo de la credencial empleada, son los siguientes:\n",
    "- *Essential*: 500k tweets en un mes y 512 caracteres por query.\n",
    "- *Elevated*: 2M tweets en un mes y 512 caracteres por query.\n",
    "- *Academic*: 10M tweets en un mes y 1024 caracteres por query.\n",
    "\n",
    "Cualquier otra diferencia entre nivel de credenciales, se puede [ver aquí](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level)\n",
    "\n",
    "Notas:\n",
    "- Debido a que la web app del Laboratorio de Migración solo necesita permisos de lectura para contenido público, nos interesa obtener la credencial de tipo Bearer Token, ya que es el método de autenticación que más tweets nos permite obtener en menos tiempo.\n",
    "- Se usará la versión 2 de la [API de Twitter](https://developer.twitter.com/en/docs/twitter-api).\n",
    "- El ejercicio aquí propuesto puede realizarse con las credenciales de nivel [*Elevated*](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level), pero en caso de estar trabajando en un proyecto con fines académicos o de estudio, se puede [aplicar de manera gratuita a credenciales Académicas](https://developer.twitter.com/en/products/twitter-api/academic-research) y así trabajar con límites más amplios y operadores avanzados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-cleaning",
   "metadata": {},
   "source": [
    "### Dependencias de Python\n",
    "Las dependencias principales son `twarc`, `pandas` y `numpy`. Las últimas dos son para manipular y trabajar con los datos, mientras que [`twarc` es el paquete](https://developer.twitter.com/en/docs/twitter-api/rate-limits) que nos facilitará el proceso de minado, ya que se encarga de obtener automáticamente todos los atributos de los tweets así como de manejar los tiempos de espera cuando se llega a los [límites de minado de la API](https://developer.twitter.com/en/docs/twitter-api/rate-limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "genetic-effects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: twarc==2.9.2 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (2.9.2)\n",
      "Requirement already satisfied: humanize>=3.9 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (3.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (2.8.2)\n",
      "Requirement already satisfied: requests-oauthlib>=1.3 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (1.3.0)\n",
      "Requirement already satisfied: tqdm>=4.62 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (4.62.2)\n",
      "Requirement already satisfied: click-config-file>=0.6 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (0.6.0)\n",
      "Requirement already satisfied: click-plugins>=1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (1.1.1)\n",
      "Requirement already satisfied: click<9,>=7 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (7.1.2)\n",
      "Requirement already satisfied: configobj>=5.0.6 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from click-config-file>=0.6->twarc==2.9.2) (5.0.6)\n",
      "Requirement already satisfied: setuptools in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from humanize>=3.9->twarc==2.9.2) (57.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from python-dateutil>=2.8->twarc==2.9.2) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests-oauthlib>=1.3->twarc==2.9.2) (2.27.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests-oauthlib>=1.3->twarc==2.9.2) (3.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (2020.12.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/noecampos/.pyenv/versions/BID/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: pandas==1.4.1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from pandas==1.4.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from pandas==1.4.1) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from pandas==1.4.1) (1.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas==1.4.1) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/noecampos/.pyenv/versions/BID/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: numpy==1.22.3 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (1.22.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/noecampos/.pyenv/versions/BID/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Indispensables\n",
    "!pip install twarc==2.9.2\n",
    "!pip install pandas==1.4.1\n",
    "!pip install numpy==1.22.3\n",
    "\n",
    "# Para mejor interacción gráfica\n",
    "!tqdm==4.62.2\n",
    "!pendulum==2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-gospel",
   "metadata": {},
   "source": [
    "## Imports & Credenciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sunrise-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pendulum\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from twarc.client2 import Twarc2\n",
    "from twarc.expansions import TWEET_FIELDS\n",
    "from twarc.expansions import ensure_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-diversity",
   "metadata": {},
   "source": [
    "Ingresa tu propio Bearer Token o coméntalo, y descomenta el resto de atributos para con tus propias API Keys y Access Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opposite-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_TWITTER_API = {\n",
    "    'bearer_token': \"Enter your own Bearer token\",\n",
    "\n",
    "    # 'api_key': \"Enter your own API Key\",\n",
    "    # 'api_secret_key': \"Enter your own API Secret Key\",\n",
    "    # 'access_token': \"Enter your own access_token\",\n",
    "    # 'access_token_secret': \"Enter your own access_token_secret\"\n",
    "}\n",
    "\n",
    "IS_ACADEMIC = False # Cambiar a True, si las credenciales son Academicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-communications",
   "metadata": {},
   "source": [
    "## Minado de Tweets\n",
    "\n",
    "La API de Twitter permite obtener diferentes piezas de información a partir de los usuarios y los tweets que publican, según el tipo de operadores y queries utilizados. Para fines de esta notebook, nos centraremos en obtener tweets públicos en un intervalo de tiempo definido, a partir del contenido de palabras claves.\n",
    "\n",
    "Las palabras claves del tema que se desea minar se utilizan para construir queries que hacen la búsqueda más certera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-thomas",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-prerequisite",
   "metadata": {},
   "source": [
    "<h3 id='endpoints_limits'>Endpoints & Límites</h3>\n",
    "\n",
    "Para [buscar tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction) hay dos endpoints, [Recent Search](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/recent-search) y  [Full Archive](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/full-archive-search).\n",
    "- **Recent Search**: Nos permite realizar 450 requests (pedidos) a la API en una ventana de 15 minutos, obteniendo máximo de 100 tweets por request. Sin embargo, solo se pueden obtener tweets publicados en los últimos 7 días.\n",
    "- **Full Archive**: Podremos obtener tweets publicados desde el inicio de la red social, pero solo se pueden realizar 300 requests en una ventana de 15 minutos, obteniendo hasta 500 tweets por cada request. Hay que aplicar a las [credenciales académicas](https://developer.twitter.com/en/products/twitter-api/academic-research) para poder utilizar este endpoint.\n",
    "\n",
    "**Nota**: No hay que preocuparse del código de error que aparece una vez que el minado alcanza estos límites, pues el paquete twarc se encarga de pausar la obtención de tweets una vez que se llega al límite de 450 (o 300) requests en la ventana de 15 minutos; una vez que pasa un tiempo necesario, twarc reanuda el proceso. Si se llega al límite de tweets en un mes, la función para."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-yellow",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-intake",
   "metadata": {},
   "source": [
    "### Operadores\n",
    "\n",
    "En esta sección vamos a ver de manera general qué son y cómo funcionan los operadores, pero la lista y descripción completa de los mismos se puede [encontrar aquí](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query).\n",
    "\n",
    "Existen dos tipos de operadores:\n",
    "- **Standalone**: Se pueden usar solos o en conjunción de otros. Por ejemplo, buscar tweets con un hashtag en específico: `#migrantes`.\n",
    "- **Conjunction-required**: Es necesario que estén junto a mínimo un operador *standalone*. Por ejemplo, buscar tweets con un hashtag en específico, pero que incluyan imágenes y sean retweets: `#migrantes has:media is:retweet`\n",
    "\n",
    "Además, como se mencionó en la sección de \"Credenciales para la API de Twitter\", existen operadores *core*, que son accesibles con cualquier nivel de acceso, así como los operadores *advanced*, que solo se pueden utilizar con un acceso académico.\n",
    "\n",
    "#### Operadores Lógicos\n",
    "\n",
    "- **AND**: Obtiene tweets que cumplan con los dos operadores, se logra dejando un espacio en blanco entre ellos. \n",
    "    - Ejemplo, obtener tweets que contienen la palabra *políticos* y el hashtag *#corruptos*: `politicos #corruptos`.\n",
    "- **OR**: Obtiene tweets que cumplan con alguno de los dos operadores. Hay que añadir el string \" OR \" entre los operadores. \n",
    "    - Ejemplo, tweets que contengan la palabra *migrantes* o *inmigrantes*: `migrantes OR inmigrantes`.\n",
    "- **NOT**: Obtiene tweets que no contengan el operador o la keyword negada. Se logra añadiendo un guion medio \"-\" antes del operador o keyword. \n",
    "    - Ejemplo, obtener tweets con la palabra *políticos* pero sin la palabra *corruptos*: `politicos -corruptos`\n",
    "    - Ejemplo, obtener tweets con la palabra *migrantes, pero que no sean retweets*: `migrantes -is:retweets`\n",
    "- **Grouping**: Sirve para agrupar operadores lógicos, y hay que encerrar los operadores entre parentesis. Un grupo no puede ser negado.\n",
    "    - Ejemplo, obtener tweets con la palabra *migrantes* y alguna de las palabras *llegan* o *salen*: `migrantes AND (llegan OR salen)`\n",
    "\n",
    "**Nota**: A menos de que haya paréntesis para especificar el orden de operadores, primero se resuelven aquellos que son *AND* y luego los *OR*. [Más aquí](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#boolean).\n",
    "\n",
    "#### Más Operadores\n",
    "\n",
    "- **keyword**: Hace match con tweets que contengan un string en específico. No es sensible a caracteres en mayúsculas o minúsculas; acentos o caracteres especiales como ñ. Ejemplo: `migrantes OR inmigrantes`.\n",
    "- **\"exact phrase\"**: Parecido al anterior, permite considerar espacios y múltiples tokens. Tiene que estar entre comillas dobles. Ejemplo: `\"ola migrante\"`.\n",
    "- **#**: Hace match a tweets que tengan el hashtag incluido. Ejemplo: `#migraresunderecho`.\n",
    "- **@**: Hace match a tweets que mencionen a los usuarios incluidos. Ejemplo: `@IADB`.\n",
    "- **place_country**: Obtiene tweets que sean geo-localizable a cierto país. Hay que pasarle el código [ISO del país](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2). Ejemplo: `place_country:MX`.\n",
    "- **lang**: Obtiene tweets que estén escrito en el lenguaje indicado y tiene que estar junto con un standalone operator. Ejemplo: `migrantes lang:es`.\n",
    "- **is:retweet**: Obtiene solo tweets que sean retweets y tiene que estar junto con un standalone operator. Ejemplo: `migrantes is:retweet`.\n",
    "\n",
    "La lista completa de operadores se puede ver [aquí](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-millennium",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-exchange",
   "metadata": {},
   "source": [
    "### Ejercicio de Minado\n",
    "\n",
    "La siguiente función recibe una lista de queries y un intervalo de tiempo para guardar los tweets en un archivo JSONL.\n",
    "\n",
    "**Nota**: Un archivo de extension .jsonl guarda un objeto json por cada línea separados por solo el salto de línea, no comas u otro separador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "criminal-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(credentials_api,\n",
    "               queries_list,\n",
    "               output_file,\n",
    "               since_date, until_date,\n",
    "               is_academic=False):\n",
    "    \"\"\"Function in charge of scrape tweets from the \n",
    "    official Twitter API, using the library named Twarc.\n",
    "\n",
    "    Args:\n",
    "        credentials_api (dict): Dictionary with the Twitter API credentials.\n",
    "        queries_list (list[str]): List of queries to scrape.\n",
    "        output_file (str): Path to the file in which to store the results.\n",
    "        since_date (datetime): Start of the time span to scrape.\n",
    "        until_date (datetime): End of the time span to scrape.\n",
    "        is_academic (bool, optional): If the credentials has Research \n",
    "                                      Academic access level.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instiate the Twarc Client\n",
    "    twarc_client = Twarc2(**credentials_api)\n",
    "\n",
    "    # Make some tweaks for using the research credentials\n",
    "    max_size = 100\n",
    "    tweet_fields = TWEET_FIELDS.copy()\n",
    "    search_func = twarc_client.search_recent\n",
    "    if(is_academic):\n",
    "        search_func = twarc_client.search_all\n",
    "        max_size = 500\n",
    "\n",
    "        # Remove the context_annotations attr to\n",
    "        # scrape 500 tweets per request\n",
    "        tweet_fields.remove('context_annotations')\n",
    "\n",
    "    tweet_fields = ','.join(tweet_fields)\n",
    "\n",
    "    with open(output_file, 'a') as pages_file:\n",
    "        for query in tqdm(queries_list):\n",
    "\n",
    "            search_results = search_func(query=query,\n",
    "                                         start_time=since_date,\n",
    "                                         end_time=until_date,\n",
    "                                         tweet_fields=tweet_fields,\n",
    "                                         max_results=max_size)\n",
    "\n",
    "            # Write all the obtained tweets\n",
    "            for page in search_results:\n",
    "\n",
    "                # Write one by one the tweets\n",
    "                for tweet in ensure_flattened(page):\n",
    "                    json.dump(tweet, pages_file, ensure_ascii=False)\n",
    "                    pages_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-integration",
   "metadata": {},
   "source": [
    "Se declara la lista de queries, el intervalo de fecha (para fines de este notebook, es 6 h hacia atrás de la hora actual) y si las credenciales que se están utilizando tienen un nivel de acceso académico. Esto último es útil para pedir que cada request tenga 500 tweets.\n",
    "\n",
    "\n",
    "Para este ejemplo, se minarán tweets que:\n",
    "- Contengan \"migrantes\" y \"migración\", o alguna de sus variantes\n",
    "- Haya sido publicado en las últimas 3 horas\n",
    "\n",
    "Y se va a guardar en un archivo llamado `1_tweets_test.jsonl`.\n",
    "\n",
    "Algo a tomar en cuenta, es que la API de Twitter no acepta expresiones regulares o wildcards, es decir, no podemos pasarle un string como `(in|e)?migrantes` esperando que obtenga tweets con las palabras \"inmigrantes\", \"emigrantes\" o \"migrantes\". Hay que pasarle todas las variaciones de la keyword que esperamos obtengan información útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "everyday-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_queries = ['migrante', 'inmigrante', 'emigrante', \n",
    "               'migrantes', 'inmigrantes', 'emigrantes',\n",
    "               'migración', 'inmigracion', 'emigracion']\n",
    "\n",
    "# De las ultimas 24 hrs\n",
    "date_end = pendulum.today()\n",
    "date_start = date_end.subtract(hours=3)\n",
    "\n",
    "# # O un rango de fechas definido\n",
    "# date_start = pendulum.datetime(year=2022, month=11, day=14)\n",
    "# date_end = pendulum.datetime(year=2022, month=11, day=13)\n",
    "\n",
    "file_tweets = os.path.abspath(\"./files/1_tweets_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handy-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrar el archivo si ya existe\n",
    "if(os.path.exists(file_tweets)):\n",
    "    os.remove(file_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "african-match",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:43<00:00,  4.81s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-grass",
   "metadata": {},
   "source": [
    "Con el siguiente comando (de bash) se puede revisar cuantos tweets obtuvimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "representative-thomson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3388 ./files/1_tweets_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/1_tweets_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-neighbor",
   "metadata": {},
   "source": [
    "Es posible que existan tweets que contengan dos o más de las keywords, por lo que al hacer búsquedas diferentes, obtendremos N veces el mismo tweet. Por ejemplo, aquellos tweets que contengan las palabras \"migrante\" y \"migrantes\" los obtendremos dos veces. \n",
    "\n",
    "Esto se puede optimizar usando el operador lógicos `OR`, así podremos hacer request que aprovechen la longitud máxima de los queries, reducir el número de request y tiempo de minado, ya que estaremos obteniendo más tweets por request. Los límites de la longitud en caracteres de los queries se vieron en la sección [Credenciales para la API de Twitter](#credentials_api) y los request/tiempos en [Endpoints & Límites](#endpoints_limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "asian-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes OR migración OR inmigracion OR emigracion'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Juntar las keywords en un solo string\n",
    "new_query = ' OR '.join(lst_queries)\n",
    "new_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-marsh",
   "metadata": {},
   "source": [
    "En lugar de 9 queries ahora tenemos solo 1 compuesto por 80 caracteres (de los 512 o 1024 posibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "occasional-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que poner el nuevo query dentro de una lista dado que es lo que espera la funcion\n",
    "lst_queries_2 = [new_query]\n",
    "\n",
    "# Nuevo archivo\n",
    "file_tweets = os.path.abspath(\"./files/2_tweets_test.jsonl\")\n",
    "\n",
    "if(os.path.exists(file_tweets)):\n",
    "    os.remove(file_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parliamentary-starter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:35<00:00, 35.23s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries_2,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "parental-while",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3243 ./files/2_tweets_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/2_tweets_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-election",
   "metadata": {},
   "source": [
    "Esta optimización con operadores lógicos se puede extender a cuando necesitamos tweets que contengan dos o más posibles palabras. Por ejemplo, obtener los tweets que contengan alguna de las keywords: \"migrantes\", \"inmigrantes\" y la keyword \"bienvenidos\".\n",
    "\n",
    "El query sería `bienvenidos (migrantes OR inmigrantes)`, recordando que:\n",
    "- Los paréntesis agrupan el OR \n",
    "- El espacio entre la primera keyword y los parentesis es un `AND`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-clinton",
   "metadata": {},
   "source": [
    "Si se quiere añadir otra keyword opción a \"bienvenidos\", sería `(hermanos OR bienvenidos) (migrantes OR inmigrantes)`. Las combinaciones que forma son: `hermanos AND migrantes`, \"hermanos inmigrantes\", \"bienvenidos migrantes\" y \"bienvenidos inmigrantes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-possibility",
   "metadata": {},
   "source": [
    "### Ejemplo final de queries\n",
    "\n",
    "Como un ejemplo más completo, vamos a obtener tweets que:\n",
    "- Contengan alguna de las siguientes palabras: \"migrantes\", \"inmigrantes\" o \"emigrantes\"\n",
    "- O que tengan alguno de estos hashtags: #migraresunderecho,  #todossomomigrantes o #heramanomigrante\n",
    "- Que estén en español\n",
    "- Que hayan sido publicados por alguien en México o en Argentina\n",
    "- Y fuesen publicados en los últimos 5 días\n",
    "\n",
    "\n",
    "Nota: Dado que se están añadiendo filtros de lenguajes y sobre todo de país, se espera obtener un volumen menor de tweets a que si solo fueran los keywords y hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "earned-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_part = \"(migrantes OR inmigrantes OR emigrantes)\"\n",
    "hashtags_part = \"(#migraresunderecho OR #todossomomigrantes OR #hermanomigrante)\"\n",
    "language_part = \"lang:es\"\n",
    "country_part = \"(place_country:MX OR place_country:AR)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daily-basis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((migrantes OR inmigrantes OR emigrantes) OR (#migraresunderecho OR #todossomomigrantes OR #hermanomigrante)) lang:es (place_country:MX OR place_country:AR)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_query = f\"({keywords_part} OR {hashtags_part}) {language_part} {country_part}\"\n",
    "final_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "third-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que poner el nuevo query dentro de una lista dado que es lo que espera la funcion\n",
    "lst_queries_3 = [final_query]\n",
    "\n",
    "# De las ultimas 5 dias\n",
    "date_end = pendulum.today()\n",
    "date_start = date_end.subtract(days=5)\n",
    "\n",
    "# Nuevo archivo\n",
    "file_tweets = os.path.abspath(\"./files/3_tweets_test.jsonl\")\n",
    "\n",
    "if(os.path.exists(file_tweets)):\n",
    "    os.remove(file_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "small-wells",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.06s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries_3,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "convertible-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 ./files/3_tweets_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/3_tweets_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-voltage",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-postcard",
   "metadata": {},
   "source": [
    "### Objeto Tweet\n",
    "Antes de terminar esta notebook, hay que revisar que campos contiene el [objeto tweet](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet).\n",
    "\n",
    "Los campos más importantes son:\n",
    "- **id**: Identificador unico de cada tweet\n",
    "- **conversation_id**: Identificador unico del tweet que inicio la conversación\n",
    "- **created_at**: Fecha y hora en la que se publicó el tweet\n",
    "- **text**: Texto publicado en el tweet\n",
    "- **possibly_sensitive**: Si el texto incluye links a contenido posiblemente sensible\n",
    "- **lang**: Idioma en el que está escrito el tweet\n",
    "- **source**: Sí fue publicado desde un iPhone, Android, un buscador, etc.\n",
    "- **geo**: Objeto que contiene el lugar, país y coordenadas que se asignaron al tweet\n",
    "- **author_id**: Identificador único del autor\n",
    "- **author**: Objeto que incluye atributos del autor\n",
    "- **public_metrics**: Objeto que tiene el conteo de retweets, replies, likes y quotes\n",
    "\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"1589089368777850880\",\n",
    "    \"conversation_id\": \"1589089368777850880\",\n",
    "    \"created_at\": \"2022-11-06T02:56:44.000Z\",\n",
    "    \"text\": \"Primer vuelo humanitario para repatriar a venezolanos que estaban en #México \\n\\n140 personas regresaron voluntariamente en un vuelo coordinado por @INAMI_mx \\n\\nSegún las autoridades mexicanas continuarán apoyando el regreso de migrantes a su país. https://t.co/JuUjRB3oQk\",\n",
    "    \"possibly_sensitive\": false,\n",
    "    \"lang\": \"es\",\n",
    "    \"source\": \"Twitter for iPhone\",\n",
    "\n",
    "    \"geo\": {\n",
    "        \"place_id\": \"2a376531dff3d76a\",\n",
    "        \"full_name\": \"Tlalpan, Distrito Federal\",\n",
    "        \"name\": \"Tlalpan\",\n",
    "        \"place_type\": \"city\",\n",
    "        \"id\": \"2a376531dff3d76a\",\n",
    "        \"country\": \"México\",\n",
    "        \"geo\": {\n",
    "            \"type\": \"Feature\",\n",
    "            \"bbox\": [-99.315748, 19.087511, -99.1009804, 19.311459],\n",
    "            \"properties\": {}\n",
    "        },\n",
    "        \"country_code\": \"MX\"\n",
    "    },\n",
    "\n",
    "    \"author_id\": \"114573824\",\n",
    "    \"author\": {\n",
    "        \"verified\": false,\n",
    "        \"protected\": false,\n",
    "        \"profile_image_url\": \"https://pbs.twimg.com/profile_images/1569749213822570500/Xf8yCPws_normal.jpg\",\n",
    "        \"created_at\": \"2010-02-15T21:50:44.000Z\",\n",
    "        \"pinned_tweet_id\": \"1588664226729832448\",\n",
    "        \"url\": \"https://t.co/E1sEHNlBkT\",\n",
    "        \"description\": \"Periodista mexicano 🗞 @aztecanoticias\",\n",
    "        \"public_metrics\": {\n",
    "            \"followers_count\": 11893,\n",
    "            \"following_count\": 732,\n",
    "            \"tweet_count\": 24530,\n",
    "            \"listed_count\": 25\n",
    "        },\n",
    "        \"entities\": {\n",
    "            \"url\": {\n",
    "                \"urls\": [{\n",
    "                    \"start\": 0,\n",
    "                    \"end\": 23,\n",
    "                    \"url\": \"https://t.co/E1sEHNlBkT\",\n",
    "                    \"expanded_url\": \"http://www.facebook.com/OtonielMartínez\",\n",
    "                    \"display_url\": \"facebook.com/OtonielMartínez\"\n",
    "                }]\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"mentions\": [{\n",
    "                    \"start\": 22,\n",
    "                    \"end\": 37,\n",
    "                    \"username\": \"aztecanoticias\"\n",
    "                }]\n",
    "            }\n",
    "        },\n",
    "        \"name\": \"OTONIEL MARTÍNEZ\",\n",
    "        \"username\": \"_otomartinez\",\n",
    "        \"location\": \"MÉXICO\",\n",
    "        \"id\": \"114573824\"\n",
    "    },\n",
    "\n",
    "    \"public_metrics\": {\n",
    "        \"retweet_count\": 4,\n",
    "        \"reply_count\": 1,\n",
    "        \"like_count\": 19,\n",
    "        \"quote_count\": 0\n",
    "    },\n",
    "    \"reply_settings\": \"everyone\",\n",
    "    \"attachments\": {\n",
    "        \"media_keys\": [\"3_1589089086119333889\", \"3_1589089086245146626\"],\n",
    "        \"media\": [{\n",
    "            \"type\": \"photo\",\n",
    "            \"media_key\": \"3_1589089086119333889\",\n",
    "            \"width\": 774,\n",
    "            \"url\": \"https://pbs.twimg.com/media/Fg2UGr6X0AEKzBx.jpg\",\n",
    "            \"height\": 1024\n",
    "        }, {\n",
    "            \"type\": \"photo\",\n",
    "            \"media_key\": \"3_1589089086245146626\",\n",
    "            \"width\": 731,\n",
    "            \"url\": \"https://pbs.twimg.com/media/Fg2UGsYXkAIC8S4.jpg\",\n",
    "            \"height\": 1024\n",
    "        }]\n",
    "    },\n",
    "\n",
    "    \"entities\": {\n",
    "        \"annotations\": [{\n",
    "            \"start\": 70,\n",
    "            \"end\": 75,\n",
    "            \"probability\": 0.9717,\n",
    "            \"type\": \"Organization\",\n",
    "            \"normalized_text\": \"México\"\n",
    "        }],\n",
    "        \"urls\": [{\n",
    "            \"start\": 246,\n",
    "            \"end\": 269,\n",
    "            \"url\": \"https://t.co/JuUjRB3oQk\",\n",
    "            \"expanded_url\": \"https://twitter.com/_otomartinez/status/1589089368777850880/photo/1\",\n",
    "            \"display_url\": \"pic.twitter.com/JuUjRB3oQk\",\n",
    "            \"media_key\": \"3_1589089086119333889\"\n",
    "        }, {\n",
    "            \"start\": 246,\n",
    "            \"end\": 269,\n",
    "            \"url\": \"https://t.co/JuUjRB3oQk\",\n",
    "            \"expanded_url\": \"https://twitter.com/_otomartinez/status/1589089368777850880/photo/1\",\n",
    "            \"display_url\": \"pic.twitter.com/JuUjRB3oQk\",\n",
    "            \"media_key\": \"3_1589089086245146626\"\n",
    "        }],\n",
    "        \"hashtags\": [{\n",
    "            \"start\": 69,\n",
    "            \"end\": 76,\n",
    "            \"tag\": \"México\"\n",
    "        }],\n",
    "        \"mentions\": [{\n",
    "            \"start\": 146,\n",
    "            \"end\": 155,\n",
    "            \"username\": \"INAMI_mx\",\n",
    "            \"id\": \"1300283125\",\n",
    "            \"verified\": true,\n",
    "            \"protected\": false,\n",
    "            \"profile_image_url\": \"https://pbs.twimg.com/profile_images/1542852123821424640/exw6RvmZ_normal.jpg\",\n",
    "            \"created_at\": \"2013-03-25T17:02:40.000Z\",\n",
    "            \"pinned_tweet_id\": \"1589088702302945280\",\n",
    "            \"url\": \"https://t.co/aMyjqh7MV3\",\n",
    "            \"description\": \"Instituto Nacional de Migración\",\n",
    "            \"public_metrics\": {\n",
    "                \"followers_count\": 52464,\n",
    "                \"following_count\": 656,\n",
    "                \"tweet_count\": 27656,\n",
    "                \"listed_count\": 334\n",
    "            },\n",
    "            \"entities\": {\n",
    "                \"url\": {\n",
    "                    \"urls\": [{\n",
    "                        \"start\": 0,\n",
    "                        \"end\": 23,\n",
    "                        \"url\": \"https://t.co/aMyjqh7MV3\",\n",
    "                        \"expanded_url\": \"https://www.gob.mx/inm\",\n",
    "                        \"display_url\": \"gob.mx/inm\"\n",
    "                    }]\n",
    "                }\n",
    "            },\n",
    "            \"name\": \"INM\",\n",
    "            \"location\": \"México\"\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-watch",
   "metadata": {},
   "source": [
    "Como manipular el JSONL a un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BID",
   "language": "python",
   "name": "bid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
