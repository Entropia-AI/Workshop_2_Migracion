{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "commercial-pressing",
   "metadata": {},
   "source": [
    "# Variables Demograficas\n",
    "\n",
    "- [ ] El tener multiples caracterisiticas demograficas permite hacer cortes y el sesgo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "encouraging-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "\n",
    "import helpers.preprocess_text as pre_text\n",
    "import helpers.preprocess_location as pre_loc\n",
    "from helpers.here_api import normalize_user_location_here_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-campus",
   "metadata": {},
   "source": [
    "## Geolocalizaci√≥n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-negotiation",
   "metadata": {},
   "source": [
    "### Objeto geo del tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar tweets del dataset ejemplo\n",
    "df_tweets = pd.read_json(\"./files/1_dataset.jsonl\", lines=True)\n",
    "df_tweets = df_tweets[['id', 'author_id', 'geo']]\n",
    "\n",
    "# Cargar los objetos anidados geo\n",
    "df_geo = pd.DataFrame(list(df_tweets['geo']))\n",
    "df_geo = df_geo.drop(columns=['id'])\n",
    "# df_geo.columns = list(map(lambda x: f\"geo_{x}\", df_geo.columns))\n",
    "\n",
    "df_tweets = df_tweets.drop(columns=['geo'])\n",
    "df_tweets = df_tweets.join(df_geo)\n",
    "\n",
    "del df_geo\n",
    "\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-federation",
   "metadata": {},
   "source": [
    "- [ ] Describir la funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_twitter_geolocation(df_data):\n",
    "\n",
    "    # TODO: Make this a wrapper(?)\n",
    "    df_countries = pd.read_csv(f\"./resources/countries_data.csv\",\n",
    "                               usecols=['ISO_2', 'ISO_3', 'name_short_es'],\n",
    "                               sep='\\t')\n",
    "\n",
    "    ISO_codes = dict(df_countries.iloc[:, :2].values)\n",
    "    countries_names = dict(df_countries.iloc[:, [0, 2]].values)\n",
    "\n",
    "    # Normalize author country data\n",
    "    srs_country_codes = df_data['country_code']\n",
    "    df_data['country_code'] = srs_country_codes.map(ISO_codes)\n",
    "    df_data['country_name'] = srs_country_codes.map(\n",
    "        countries_names)\n",
    "\n",
    "    # Normalize federal divisions\n",
    "    admin_index = (df_data['place_type'] == 'admin')\n",
    "    df_data['federal_division'] = df_data[admin_index]['name']\n",
    "\n",
    "    # Normalize cities\n",
    "    city_index = (df_data['place_type'] == 'city')\n",
    "    df_data['city'] = df_data[city_index]['name']\n",
    "    \n",
    "    # Get bbox centroid\n",
    "    df_data['geo_type'] = df_data['geo'].apply(lambda x: x['type'])\n",
    "    df_data['geo_bbox'] = df_data['geo'].apply(lambda x: x['bbox'])\n",
    "    df_data['latitude'] = df_data['geo_bbox'].apply(lambda x: (x[1]+x[3])/2)\n",
    "    df_data['longitude'] = df_data['geo_bbox'].apply(lambda x: (x[0]+x[2])/2)\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = df_tweets[['id', 'place_id']]\n",
    "df_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocs = df_tweets.iloc[:, 1:].dropna(subset=['place_id']) \\\n",
    "                      .drop_duplicates(subset=['place_id'])\n",
    "df_geolocs = normalize_twitter_geolocation(df_geolocs)\n",
    "df_geolocs = df_geolocs[['place_id', 'country_code', \n",
    "                         'country_name', 'federal_division', 'city', \n",
    "                         'latitude', 'longitude']]\n",
    "df_geolocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = df_ids.merge(df_geolocs, on='place_id', how='left')\n",
    "df_geo = df_geo[~df_geo['country_code'].isna()]\n",
    "df_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-patrol",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-chrome",
   "metadata": {},
   "source": [
    "### Atributo *location* del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar tweets del dataset ejemplo\n",
    "df_tweets = pd.read_json(\"./files/2_dataset.jsonl\", lines=True)\n",
    "df_tweets = df_tweets[['id', 'author_id', 'author']]\n",
    "\n",
    "# Cargar los objetos anidados geo\n",
    "df_authors = pd.DataFrame(list(df_tweets['author']))\n",
    "df_authors = df_authors[['location', 'description']]\n",
    "df_authors.columns = list(map(lambda x: f\"author_{x}\", df_authors.columns))\n",
    "\n",
    "df_tweets = df_tweets.drop(columns=['author'])\n",
    "df_tweets = df_tweets.join(df_authors)\n",
    "\n",
    "del df_authors\n",
    "\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dejar solo los textos de locations sin duplicados ni nulos\n",
    "df_unique_locs = df_tweets[['author_location']].drop_duplicates() \\\n",
    "                                        .dropna() \\\n",
    "                                        .reset_index(drop=True) \\\n",
    "                                        .copy()\n",
    "df_unique_locs['author_location_processed'] = df_unique_locs['author_location']\n",
    "\n",
    "# Preprocesar los strings de locations\n",
    "df_unique_locs = pre_loc.preprocessing_location_string(df_unique_locs)\n",
    "df_unique_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_locs = df_unique_locs[['author_location_processed']].drop_duplicates() \\\n",
    "                                                              .reset_index(drop=True).copy()\n",
    "\n",
    "# Validar locations ya preprocesadas\n",
    "df_unique_locs = pre_loc.validate_author_locations(df_unique_locs.copy())\n",
    "df_unique_locs = df_unique_locs[df_unique_locs['author_location_is_valid']].reset_index(drop=True)\n",
    "df_unique_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev =  pd.read_csv('./resources/user_location_none.csv',\n",
    "                       usecols=['a_proc_loc'])\n",
    "df_prev = df_prev.append(pd.read_csv('./resources/user_location_invalids.csv',\n",
    "                                     usecols=['a_proc_loc']))\n",
    "df_prev = df_prev.append(pd.read_csv('./resources/user_location_valids.csv',\n",
    "                                     usecols=['a_proc_loc']))\n",
    "df_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_locs = df_unique_locs[~df_unique_locs['author_location_processed'].isin(df_prev['a_proc_loc'])]\n",
    "df_unique_locs = df_unique_locs.reset_index(drop=True)\n",
    "df_unique_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-sunday",
   "metadata": {},
   "source": [
    "- [ ] Explicar que es la API de HERE y de que nos ayuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_HERE_API = \"<API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_locs, df_invalids_locs, df_none_locs = normalize_user_location_here_api(df_unique_locs, CREDENTIALS_HERE_API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-rates",
   "metadata": {},
   "source": [
    "### Store the new normalized locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_prev_valid = pd.read_csv('./resources/user_location_valids.csv')\n",
    "    df_data_locs_tmp = df_data_locs[['author_location_processed', 'norm_country_code', \n",
    "                        'norm_country', 'norm_fed_division', 'norm_fed_division_code', \n",
    "                        'norm_county', \n",
    "                        'norm_city', 'norm_district', 'norm_postal_code', \n",
    "                        'norm_latitude', 'norm_longitude',\n",
    "                        'here_query_score', 'here_field_score', 'valid_regions']]\n",
    "    df_data_locs_tmp.columns = df_prev_valid.columns\n",
    "    df_prev_valid = pd.concat([df_prev_valid, df_data_locs_tmp])\n",
    "    df_prev_valid = df_prev_valid.drop_duplicates('a_proc_loc', keep='last')\n",
    "    \n",
    "    \n",
    "    \n",
    "except FileNotFoundError:\n",
    "    df_prev_valid = df_data_locs.copy()\n",
    "\n",
    "df_prev_valid.to_csv('./resources/user_location_valids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-banking",
   "metadata": {},
   "source": [
    "### Assign the normalized author locations to the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv('./resources/user_location_valids.csv',\n",
    "                       usecols=['a_proc_loc', 'a_code', 'a_country',\n",
    "                                'a_fdiv', 'a_county', 'a_city',\n",
    "                                'a_lat', 'a_lng'])\n",
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_tweets[['id', 'author_location']].copy()\n",
    "df_data = df_data.dropna(subset='author_location') \\\n",
    "                 .reset_index(drop=True)\n",
    "df_data['author_location_processed'] = df_data['author_location'].copy()\n",
    "df_data = pre_loc.preprocessing_location_string(df_data)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = df_data.merge(df_valid, left_on='author_location_processed',\n",
    "                        right_on='a_proc_loc', how='left')\n",
    "df_norm = df_norm.dropna(subset=['a_code'])\n",
    "df_norm = df_norm.reset_index(drop=True) \\\n",
    "                 .drop(columns=['a_proc_loc', \n",
    "                                'author_location_processed'])\n",
    "df_norm.columns = ['id', 'author_location',\n",
    "                   'country_code', 'country', 'federal_division', 'county', \n",
    "                   'city', 'latitude', 'longitude']\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = df_norm[df_norm['country_code'].isin(['MEX', 'ARG', 'COL'])]\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm['country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-dollar",
   "metadata": {},
   "source": [
    "- [ ] Graficas juntando ambos metodos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-olympus",
   "metadata": {},
   "source": [
    "- [ ] Ideas para extraer m√°s locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-stick",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-watson",
   "metadata": {},
   "source": [
    "## Inferir g√©nero\n",
    "\n",
    "El poder asignar un g√©nero a los tweets nos permite hacer un estudio de como los diferentes g√©neros abordan el tema de estudio, en este caso el de Migraci√≥n. Twitter no recolecta esa informaci√≥n al hacer la cuenta, por lo que no podemos encontrar un atributo dado por la API, pero al igual que la geolocalizaci√≥n, podemos inferirla a partir de atributos como el nombre, la descripci√≥n o incluso de la foto de perfil.\n",
    "\n",
    "En esta notebook se van a revisar algunas opciones para asignar g√©nero al autor:\n",
    "- Buscar nombres t√≠picamente masculinos o femeninos en los campos de username, descripci√≥n\n",
    "- Buscar pronombres (She/Her o He/His) en la descripci√≥n del usuario\n",
    "- Buscar la nacionalidad (e.g.: Mexicano / Mexicana) en los campos descripci√≥n\n",
    "\n",
    "Tambi√©n se podr√≠a trabajar con adjetivos, profesiones, oficios o t√≠tulos.\n",
    "\n",
    "Nota: Dado que estos campos son abiertos y el nivel de veracidad recae completamente en cada usuario, se puede esperar que estos m√©todos tengan falsos positivos o negativos. Igualmente, tampoco se puede esperar que se termine asignando un g√©nero a todos los autores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-volleyball",
   "metadata": {},
   "source": [
    "## Cargar nombres, pronombres, nacionalidades y g√©neros\n",
    "\n",
    "En el directorio llamado ./resources hay un archivo llamado names_genders.csv que contiene una columna con nombres, una con el g√©nero que t√≠picamente tienen las personas llamadas as√≠ y una √∫ltima columna de si es un nombre compuesto como; Maria Jose o Jose Guadalupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "verbal-description",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>is_double</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guadalupe</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jose</td>\n",
       "      <td>M</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sofia</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angel</td>\n",
       "      <td>M</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jesus</td>\n",
       "      <td>M</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>aron</td>\n",
       "      <td>M</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>jose guadalupe</td>\n",
       "      <td>M</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>jose maria</td>\n",
       "      <td>M</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>maria guadalupe</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>maria jose</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name gender  is_double\n",
       "0          guadalupe      F      False\n",
       "1               jose      M      False\n",
       "2              sofia      F      False\n",
       "3              angel      M      False\n",
       "4              jesus      M      False\n",
       "..               ...    ...        ...\n",
       "555             aron      M      False\n",
       "556   jose guadalupe      M       True\n",
       "557       jose maria      M       True\n",
       "558  maria guadalupe      F       True\n",
       "559       maria jose      F       True\n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names = pd.read_csv('./resources/names_genders.csv')\n",
    "\n",
    "pre_text.remove_duplicated_chars(df_names, 'name')\n",
    "\n",
    "# Names composed\n",
    "df_names_double = df_names[df_names['is_double']]\n",
    "regex_names_double = f\"\\\\b((?:{'|'.join(df_names_double['name'])}))\\\\b\"\n",
    "regex_names_double = re.compile(regex_names_double)\n",
    "\n",
    "# Single names\n",
    "df_names_single = df_names[~df_names['is_double']]\n",
    "regex_names_single = f\"\\\\b((?:{'|'.join(df_names_single['name'])}))\\\\b\"\n",
    "regex_names_single = re.compile(regex_names_single)\n",
    "\n",
    "# Names to gender\n",
    "dict_names_gender = df_names.set_index('name')['gender'].to_dict()\n",
    "df_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-reduction",
   "metadata": {},
   "source": [
    "Despues hay un diccionario con los posibles strings conteniendo pronombres y el genero que identifican. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solar-vinyl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'\\b((?:she her|her she|he him|him he))\\b', re.UNICODE)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pronouns_gender = {\n",
    "    \"she her\": 'F',\n",
    "    \"her she\": 'F',\n",
    "    \"he him\": 'M',\n",
    "    \"him he\": 'M'\n",
    "}\n",
    "\n",
    "regex_pronous = f\"\\\\b((?:{'|'.join(dict_pronouns_gender.keys())}))\\\\b\"\n",
    "regex_pronous = re.compile(regex_pronous)\n",
    "regex_pronous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-robert",
   "metadata": {},
   "source": [
    "Un diccionario con las posibles nacionalidades en las diferentes variaciones segun el genero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "german-laptop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'\\b((?:mexicano|mexicana|colombiano|colombiana|argentino|argentina))\\b',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_nationalities_gender = {\n",
    "    'mexicano': 'M',\n",
    "    'mexicana': 'F',\n",
    "    'colombiano': 'M',\n",
    "    'colombiana': 'F',\n",
    "    'argentino': 'M',\n",
    "    'argentina': 'F'\n",
    "}\n",
    "\n",
    "regex_nationalities = f\"\\\\b((?:{'|'.join(dict_nationalities_gender.keys())}))\\\\b\"\n",
    "regex_nationalities = re.compile(regex_nationalities)\n",
    "regex_nationalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-gravity",
   "metadata": {},
   "source": [
    "## Cargar tweets\n",
    "\n",
    "Primero hay que cargar los tweets, espec√≠ficamente el ID del tweet, el ID del usuario, el nombre del autor, su descripci√≥n. Los √∫ltimos tres campos son abiertos, es decir, que el usuario puede ingresar lo que desee a excepci√≥n de algunos caracteres especiales. Esto implica que antes de todo hay que pre-procesarlos para as√≠ reducir el n√∫mero de variantes para una misma nacionalidad y/o nombre.\n",
    "\n",
    "Nota: Usaremos de ejemplo el primer dataset que se obtuvo en la notebook \"2_Definiendo_Queries.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "seven-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_json(\"./files/1_dataset.jsonl\", lines=True)\n",
    "df_tweets = df_tweets[['id', 'author_id', 'author']]\n",
    "\n",
    "df_tweets['author_name'] = df_tweets['author'].apply(lambda x: x['name'])\n",
    "df_tweets['author_description'] = df_tweets['author'].apply(lambda x: x['description'])\n",
    "\n",
    "df_tweets = df_tweets.drop(columns=['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-failure",
   "metadata": {},
   "source": [
    "Para evitar trabajar con autores duplicados, trabajaremos solo con una copia √∫nica de los autores, asignaremos g√©neros y luego haremos JOIN a los tweets usando como llave la columna `author_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "potential-doctor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1361302057945333760</td>\n",
       "      <td>Activa</td>\n",
       "      <td>Somos una empresa industrial y comercial de la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1391458758459600896</td>\n",
       "      <td>Jorge Araya</td>\n",
       "      <td>ver peliculas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1149464602452877312</td>\n",
       "      <td>Eduardo Cardoza Mata üá∏üáªüá¶üá∑</td>\n",
       "      <td>Embajador de El Salvador en Argentina. Salvado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119160105</td>\n",
       "      <td>daniel tejeda</td>\n",
       "      <td>#C√≥rdoba Tierra de Santos... https://t.co/eEp1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>805051335687081984</td>\n",
       "      <td>üá≤üáΩ MUCMAM-IMU MX üèçÔ∏èüõµ</td>\n",
       "      <td>pero es el comportamiento del usuario quien te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>377788360</td>\n",
       "      <td>Eduardo D√≠atriü§çüíô‚ù§Ô∏è‚òÆÔ∏èüíõüíô</td>\n",
       "      <td>Productor y asociado para espect√°culos mayores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4635</th>\n",
       "      <td>397706715</td>\n",
       "      <td>FUVADIS INTERNACIONAL üè≥Ô∏è‚Äçüåà</td>\n",
       "      <td>Trabajamos por la inclusi√≥n, el respeto y la i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4636</th>\n",
       "      <td>1176580744036196352</td>\n",
       "      <td>GaelAlejandro</td>\n",
       "      <td>¬°Estoy vivooo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>579360940</td>\n",
       "      <td>Juan Farre</td>\n",
       "      <td>#11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>326540841</td>\n",
       "      <td>YUNNYS L.</td>\n",
       "      <td>Ing. Industrial~UNEXPO.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4639 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                author_id                 author_name  \\\n",
       "0     1361302057945333760                      Activa   \n",
       "1     1391458758459600896                 Jorge Araya   \n",
       "2     1149464602452877312   Eduardo Cardoza Mata üá∏üáªüá¶üá∑   \n",
       "3               119160105               daniel tejeda   \n",
       "4      805051335687081984        üá≤üáΩ MUCMAM-IMU MX üèçÔ∏èüõµ   \n",
       "...                   ...                         ...   \n",
       "4634            377788360      Eduardo D√≠atriü§çüíô‚ù§Ô∏è‚òÆÔ∏èüíõüíô   \n",
       "4635            397706715  FUVADIS INTERNACIONAL üè≥Ô∏è‚Äçüåà   \n",
       "4636  1176580744036196352               GaelAlejandro   \n",
       "4637            579360940                  Juan Farre   \n",
       "4638            326540841                   YUNNYS L.   \n",
       "\n",
       "                                     author_description  \n",
       "0     Somos una empresa industrial y comercial de la...  \n",
       "1                                         ver peliculas  \n",
       "2     Embajador de El Salvador en Argentina. Salvado...  \n",
       "3     #C√≥rdoba Tierra de Santos... https://t.co/eEp1...  \n",
       "4     pero es el comportamiento del usuario quien te...  \n",
       "...                                                 ...  \n",
       "4634  Productor y asociado para espect√°culos mayores...  \n",
       "4635  Trabajamos por la inclusi√≥n, el respeto y la i...  \n",
       "4636                                     ¬°Estoy vivooo!  \n",
       "4637                                                #11  \n",
       "4638                            Ing. Industrial~UNEXPO.  \n",
       "\n",
       "[4639 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users = df_tweets.drop_duplicates(subset=['author_id'], \n",
    "                                  keep='last').reset_index(drop=True)\n",
    "df_users = df_users.iloc[:, 1:]\n",
    "df_users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-swiss",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "La siguiente funci√≥n se encarga de pasar todo a min√∫sculas, eliminar acentos, URLs, emails, n√∫meros, caracteres que no sean letras y espacios en blancos duplicados. Esto nos ayudar√° a que al buscar los nombres o nacionalidades en los campos, se tenga m√°s posibilidades de encontrarlos, dado que no habr√° variaciones del tipo: \"NOE\", \"NoE\", \"Noe\", \"No√©\". Permitiendo que todas esas variaciones sean validas y encontradas con la cadena de texto \"noe\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "impressive-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_names(df_data, new_col, flg_remove_two_chars=True):\n",
    "\n",
    "    at_least_3_letter_reg = re.compile(r'\\b\\w{1,2}\\b')\n",
    "    at_least_3_letter_reg_capture = re.compile(r'\\b(\\w{2})\\b')\n",
    "\n",
    "    pre_text.initial_preprocessing(df_data, new_col,\n",
    "                                   flg_remove_emojis=True,\n",
    "                                   flg_lower=True)\n",
    "\n",
    "    pre_text.remove_urls(df_data, new_col)\n",
    "    pre_text.remove_emails(df_data, new_col)\n",
    "    pre_text.remove_urls(df_data, new_col)\n",
    "    pre_text.remove_numbers(df_data, new_col, \n",
    "                            replace_char=' ')\n",
    "\n",
    "    pre_text.remove_duplicated_chars(df_data, new_col)\n",
    "    pre_text.remove_non_alphanumeric(df_data, new_col)\n",
    "\n",
    "    pre_text.remove_single_letters(df_data, new_col)\n",
    "    \n",
    "    if(flg_remove_two_chars):\n",
    "        df_data[new_col] = df_data[new_col].str.replace(at_least_3_letter_reg, \n",
    "                                                        ' ', regex=True)\n",
    "\n",
    "    pre_text.remove_multiple_blank_spaces(df_data, new_col)\n",
    "    df_data[new_col] = df_data[new_col].str.strip()\n",
    "\n",
    "    valid_texts = ((~df_data[new_col].isna()) *\n",
    "                   (df_data[new_col] != '') *\n",
    "                   (~df_data[new_col].str.contains(r'^\\w$', regex=True)))\n",
    "    df_data = df_data[valid_texts]\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-interstate",
   "metadata": {},
   "source": [
    "Se le aplica la funci√≥n de limpieza a ambas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "overall-humidity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name_og</th>\n",
       "      <th>author_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Activa</td>\n",
       "      <td>activa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jorge Araya</td>\n",
       "      <td>jorge araya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eduardo Cardoza Mata üá∏üáªüá¶üá∑</td>\n",
       "      <td>eduardo cardoza mata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daniel tejeda</td>\n",
       "      <td>daniel tejeda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üá≤üáΩ MUCMAM-IMU MX üèçÔ∏èüõµ</td>\n",
       "      <td>mucmam imu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>Alfredo Marquez T.</td>\n",
       "      <td>alfredo marquez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>Eduardo D√≠atriü§çüíô‚ù§Ô∏è‚òÆÔ∏èüíõüíô</td>\n",
       "      <td>eduardo diatri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4635</th>\n",
       "      <td>FUVADIS INTERNACIONAL üè≥Ô∏è‚Äçüåà</td>\n",
       "      <td>fuvadis internacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4636</th>\n",
       "      <td>GaelAlejandro</td>\n",
       "      <td>gaelalejandro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>YUNNYS L.</td>\n",
       "      <td>yunys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3931 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author_name_og            author_name\n",
       "0                         Activa                 activa\n",
       "1                    Jorge Araya            jorge araya\n",
       "2      Eduardo Cardoza Mata üá∏üáªüá¶üá∑   eduardo cardoza mata\n",
       "3                  daniel tejeda          daniel tejeda\n",
       "4           üá≤üáΩ MUCMAM-IMU MX üèçÔ∏èüõµ             mucmam imu\n",
       "...                          ...                    ...\n",
       "4633          Alfredo Marquez T.        alfredo marquez\n",
       "4634      Eduardo D√≠atriü§çüíô‚ù§Ô∏è‚òÆÔ∏èüíõüíô         eduardo diatri\n",
       "4635  FUVADIS INTERNACIONAL üè≥Ô∏è‚Äçüåà  fuvadis internacional\n",
       "4636               GaelAlejandro          gaelalejandro\n",
       "4638                   YUNNYS L.                  yunys\n",
       "\n",
       "[3931 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users['author_name_og'] = df_users['author_name']\n",
    "df_users = preprocess_for_names(df_users.copy(), 'author_name')\n",
    "df_users = preprocess_for_names(df_users.copy(), 'author_description',\n",
    "                                flg_remove_two_chars=False)\n",
    "df_users[['author_name_og', 'author_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-middle",
   "metadata": {},
   "source": [
    "## Buscar los nombres en la columna del nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "substantial-cliff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>extracted_name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jorge araya</td>\n",
       "      <td>jorge</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eduardo cardoza mata</td>\n",
       "      <td>eduardo</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daniel tejeda</td>\n",
       "      <td>daniel</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fabian neiman</td>\n",
       "      <td>fabian</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rodolfo alcide</td>\n",
       "      <td>rodolfo</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4629</th>\n",
       "      <td>agustin milan gomez</td>\n",
       "      <td>agustin</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>agustin basilio</td>\n",
       "      <td>agustin</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>jorge gomez monge</td>\n",
       "      <td>jorge</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>alfredo marquez</td>\n",
       "      <td>alfredo</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>eduardo diatri</td>\n",
       "      <td>eduardo</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1811 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_name extracted_name gender\n",
       "1              jorge araya          jorge      M\n",
       "2     eduardo cardoza mata        eduardo      M\n",
       "3            daniel tejeda         daniel      M\n",
       "5            fabian neiman         fabian      M\n",
       "11          rodolfo alcide        rodolfo      M\n",
       "...                    ...            ...    ...\n",
       "4629   agustin milan gomez        agustin      M\n",
       "4630       agustin basilio        agustin      M\n",
       "4632     jorge gomez monge          jorge      M\n",
       "4633       alfredo marquez        alfredo      M\n",
       "4634        eduardo diatri        eduardo      M\n",
       "\n",
       "[1811 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copia de la tabla de usuarios\n",
    "df_by_names = preprocess_for_names(df_users.copy(), 'author_name')\n",
    "\n",
    "# Buscar nombres\n",
    "df_tmp2 = df_by_names['author_name'].str.extract(regex_names_double).dropna()\n",
    "\n",
    "df_tmp1 = df_by_names[~df_by_names.index.isin(df_tmp2.index)]['author_name']\n",
    "df_tmp1 = df_tmp1.str.extract(regex_names_single).dropna()\n",
    "\n",
    "# Asignar g√©nero en base al nombre encontrado\n",
    "df_by_names['extracted_name'] = pd.concat([df_tmp1, df_tmp2])[0]\n",
    "df_by_names['gender'] = df_by_names['extracted_name'].map(dict_names_gender)\n",
    "\n",
    "df_by_names = df_by_names.dropna()\n",
    "df_by_names[['author_name', 'extracted_name', 'gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-shape",
   "metadata": {},
   "source": [
    "## Buscar los probombres en la descripci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comic-martial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>extracted_pronoun</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>celia mendoza</td>\n",
       "      <td>she her</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>vino barato</td>\n",
       "      <td>he him</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>gabe itch</td>\n",
       "      <td>he him</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>karol suarez</td>\n",
       "      <td>she her</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>edecan telcel taylor version</td>\n",
       "      <td>he him</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>lencha huasteca uta stan</td>\n",
       "      <td>she her</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>pistolita</td>\n",
       "      <td>she her</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>alain rodriguez castro</td>\n",
       "      <td>he him</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>paz</td>\n",
       "      <td>she her</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>raf juguito mengou</td>\n",
       "      <td>he him</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>tifa fracica</td>\n",
       "      <td>she her</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>ophelia pastrana</td>\n",
       "      <td>she her</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4557</th>\n",
       "      <td>sour dipa</td>\n",
       "      <td>she her</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       author_name extracted_pronoun gender\n",
       "526                  celia mendoza           she her      F\n",
       "752                    vino barato            he him      M\n",
       "1612                     gabe itch            he him      M\n",
       "1670                  karol suarez           she her      F\n",
       "1698  edecan telcel taylor version            he him      M\n",
       "1768      lencha huasteca uta stan           she her      F\n",
       "2206                     pistolita           she her      F\n",
       "2351        alain rodriguez castro            he him      M\n",
       "2692                           paz           she her      F\n",
       "3634            raf juguito mengou            he him      M\n",
       "4131                  tifa fracica           she her      F\n",
       "4366              ophelia pastrana           she her      F\n",
       "4557                     sour dipa           she her      F"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descartar usuarios a los que ya se les asigno un g√©nero\n",
    "df_pronouns = df_users[~df_users['author_id'].isin(df_by_names['author_id'])].copy()\n",
    "\n",
    "# Buscar los probombres en la descripci√≥n\n",
    "df_pronouns['extracted_pronoun'] = df_pronouns['author_description'].str.extract(regex_pronous).dropna()\n",
    "\n",
    "# Asignar un g√©nero en base a los pronombres encontrados\n",
    "df_pronouns['gender'] = df_pronouns.dropna()['extracted_pronoun'].map(dict_pronouns_gender)\n",
    "\n",
    "df_pronouns = df_pronouns.dropna()\n",
    "df_pronouns[['author_name', 'extracted_pronoun', 'gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-remark",
   "metadata": {},
   "source": [
    "### Nota\n",
    "Los siguientes m√©todos necesitan un mayor preprocesamiento del realizado en esta notebook, debido a que puede haber nombres de pa√≠ses que se confundan con la nacionalidad (e.g. Argentina) o los usuarios sean de una organizaci√≥n y/o negocio, pero mencionen a alguna nacionalidad. Se deber√≠a considerar a√±adir un paso de preprocesamiento donde se intente identificar que cuentas no son controladas por un solo individuo y representan a un conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-recall",
   "metadata": {},
   "source": [
    "## Buscar nacionalidades en la descripci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "formed-grave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_description</th>\n",
       "      <th>extracted_nationality</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>donde el el tengan paz un vago lector sexoso i...</td>\n",
       "      <td>colombiano</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>argentina es inviable por los argentinos</td>\n",
       "      <td>argentina</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>cuenta oficial de la secretaria de la juventud...</td>\n",
       "      <td>argentina</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>hincha de river puxa asturias puxa bolos cuatr...</td>\n",
       "      <td>argentina</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>real embajada de noruega ante la republica arg...</td>\n",
       "      <td>argentina</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4460</th>\n",
       "      <td>trabajamos para defender los derechos de las l...</td>\n",
       "      <td>argentina</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>colombiano democrata amante de la naturaleza</td>\n",
       "      <td>colombiano</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>astrologa poetisa viajera colombiana amo los a...</td>\n",
       "      <td>colombiana</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>tucuman argentina dios patria familia lic admi...</td>\n",
       "      <td>argentina</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4536</th>\n",
       "      <td>asociacion mexicana de psicologia desarolo com...</td>\n",
       "      <td>mexicana</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     author_description extracted_nationality  \\\n",
       "49    donde el el tengan paz un vago lector sexoso i...            colombiano   \n",
       "52             argentina es inviable por los argentinos             argentina   \n",
       "90    cuenta oficial de la secretaria de la juventud...             argentina   \n",
       "96    hincha de river puxa asturias puxa bolos cuatr...             argentina   \n",
       "112   real embajada de noruega ante la republica arg...             argentina   \n",
       "...                                                 ...                   ...   \n",
       "4460  trabajamos para defender los derechos de las l...             argentina   \n",
       "4525       colombiano democrata amante de la naturaleza            colombiano   \n",
       "4528  astrologa poetisa viajera colombiana amo los a...            colombiana   \n",
       "4529  tucuman argentina dios patria familia lic admi...             argentina   \n",
       "4536  asociacion mexicana de psicologia desarolo com...              mexicana   \n",
       "\n",
       "     gender  \n",
       "49        M  \n",
       "52        F  \n",
       "90        F  \n",
       "96        F  \n",
       "112       F  \n",
       "...     ...  \n",
       "4460      F  \n",
       "4525      M  \n",
       "4528      F  \n",
       "4529      F  \n",
       "4536      F  \n",
       "\n",
       "[99 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descartar usuarios a los que ya se les asigno un g√©nero\n",
    "df_nationalities = df_users.loc[~(df_users['author_id'].isin(df_by_names['author_id']) + \n",
    "                                  df_users['author_id'].isin(df_pronouns['author_id']))].copy()\n",
    "\n",
    "# Buscar los probombres en la descripci√≥n\n",
    "df_nationalities['extracted_nationality'] = df_nationalities['author_description'].str.extract(regex_nationalities).dropna()\n",
    "\n",
    "# # Asignar un g√©nero en base a los pronombres encontrados\n",
    "df_nationalities['gender'] = df_nationalities.dropna()['extracted_nationality'].map(dict_nationalities_gender)\n",
    "\n",
    "df_nationalities = df_nationalities.dropna()\n",
    "df_nationalities[['author_description', 'extracted_nationality', 'gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-opposition",
   "metadata": {},
   "source": [
    "## Buscar nombres en la descripci√≥n del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "working-marathon",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Descartar usuarios a los que ya se les asigno un g√©nero\n",
    "# df_description = df_users.loc[~(df_users['author_id'].isin(df_by_names['author_id']) + \n",
    "#                                 df_users['author_id'].isin(df_pronouns['author_id']) + \n",
    "#                                 df_users['author_id'].isin(df_nationalities['author_id']))].copy()\n",
    "\n",
    "# # Extraer nombres de la descripci√≥n\n",
    "# df_tmp2 = df_description['author_description'].str.extract(regex_names_double).dropna()\n",
    "\n",
    "# df_tmp1 = df_description[~df_description.index.isin(df_tmp2.index)]['author_description']\n",
    "# df_tmp1 = df_tmp1.str.extract(regex_names_single).dropna()\n",
    "\n",
    "# # Asignar genero a los nombres\n",
    "# df_description['extracted_name'] = pd.concat([df_tmp1, df_tmp2])[0]\n",
    "# df_description['gender'] = df_description['extracted_name'].map(dict_names_gender)\n",
    "\n",
    "# df_description = df_description.dropna()\n",
    "# df_description[['author_description', 'extracted_name', 'gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-glasgow",
   "metadata": {},
   "source": [
    "## A√±adir la columna gender a cada tweet\n",
    "\n",
    "Primero se juntan las tablas de los usuarios con g√©nero obtenidos con los diferentes m√©todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "compatible-secretary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1391458758459600896</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1149464602452877312</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119160105</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>705239258148290560</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1385219885891047424</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4460</th>\n",
       "      <td>1166728119434301440</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>317321263</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>1402075690019786752</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>1146517522927280128</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4536</th>\n",
       "      <td>937840963862503424</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1923 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                author_id author_gender\n",
       "1     1391458758459600896             M\n",
       "2     1149464602452877312             M\n",
       "3               119160105             M\n",
       "5      705239258148290560             M\n",
       "11    1385219885891047424             M\n",
       "...                   ...           ...\n",
       "4460  1166728119434301440             F\n",
       "4525            317321263             M\n",
       "4528  1402075690019786752             F\n",
       "4529  1146517522927280128             F\n",
       "4536   937840963862503424             F\n",
       "\n",
       "[1923 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_genders = pd.concat([df_by_names, df_pronouns, df_nationalities])\n",
    "df_genders = df_genders.drop(columns=['extracted_name', 'extracted_pronoun', ])\n",
    "\n",
    "df_genders = df_genders[['author_id', 'gender']]\n",
    "df_genders.columns = ['author_id', 'author_gender']\n",
    "df_genders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-roman",
   "metadata": {},
   "source": [
    "Se hace el merge de las tablas, el equivalente a un JOIN de SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "substantial-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.merge(df_genders, on='author_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "competent-closing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='author_gender'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seaborn.barplot(x=df_tweets['author_gender'].value_counts().index, \n",
    "                y=df_tweets['author_gender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "postal-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['author_gender'] = df_tweets['author_gender'].fillna('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "elder-condition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    5121\n",
       "M      2559\n",
       "F      1089\n",
       "Name: author_gender, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['author_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-progressive",
   "metadata": {},
   "source": [
    "## M√°s ideas\n",
    "- Buscar profesiones y sus abstracciones en el nombre del la cuenta. E.g.: Doctor/Dr. y Doctora/Dra.\n",
    "- Buscar titulos como \"Pap√°\", \"Mam√°\", \"Coordinador\", \"Coordinadora\"\n",
    "- Buscar adjetivos calificativos en la descripci√≥n\n",
    "- Identificar cuentas que representen a Secretarias Publicas, Oraganizaci√≥n, Clubs, Empresas, etc.\n",
    "- Aplicar un NER para hacer las busquedas de manera m√°s inteligentes y focalizada"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BID",
   "language": "python",
   "name": "bid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
