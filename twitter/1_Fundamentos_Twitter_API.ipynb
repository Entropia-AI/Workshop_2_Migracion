{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a5099e",
   "metadata": {},
   "source": [
    "# Laboratorio de Percepciones sobre Migración\n",
    "## Workshop 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144cb98a",
   "metadata": {},
   "source": [
    "### preparado por Entropía AI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3fc43",
   "metadata": {},
   "source": [
    "**Nota:** Esta es la primera de tres notebooks con los contenidos que se verán en el wokshop 2 organizado por el área de migración del BID. El objetivo de este workshop es aprender a utilizar la API de Twitter para construir una base de datos robusta para temas diversos de opinión pública, susceptible de ser analizado con las herramientas de Twitter.\n",
    "\n",
    "En esta primera notebook se analiza el funcionamiento de la API de Twitter y solo se necesita entender que."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-berkeley",
   "metadata": {},
   "source": [
    "# 1. Extracción de Tweets\n",
    "\n",
    "En esta notebook se van a revisar los pasos para lograr un minado de tweets que produzca una base de datos robusta. Para ello se tocarán los siguientes puntos: \n",
    "- ¿Qué es la API de Twitter y cómo utilizarla?\n",
    "- Operadores para definir queries\n",
    "- Ejemplos de minado\n",
    "\n",
    "Twitter provee una API (interfaz de programación de aplicaciones) para acceder a sus datos. \n",
    "\n",
    "Las API son mecanismos que permiten a dos componentes de software comunicarse entre sí mediante un conjunto de definiciones y protocolos. Por ejemplo, el sistema de software del instituto de meteorología contiene datos meteorológicos diarios. La aplicación meteorológica de su teléfono “habla” con este sistema a través de las API y  muestra las actualizaciones meteorológicas diarias en su teléfono. [Aquí una explicación de AWS](https://aws.amazon.com/es/what-is/api/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-finish",
   "metadata": {},
   "source": [
    "## 1a. Requerimientos\n",
    "\n",
    "<h3 id='credentials_api'>Credenciales para la API de Twitter</h3>\n",
    "\n",
    "Para obtener tweets mediante la [API Oficial de Twitter](https://developer.twitter.com/en/docs/twitter-api) es necesario registrarse en el [portal para desarrolladores de Twitter](https://developer.twitter.com/en/docs/developer-portal/overview), darse de alta en un proyecto y así obtener las [credenciales necesarias](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api) para llevar a cabo el minado.\n",
    "\n",
    "Hay que aclarar que **el tipo de credenciales y el método de autenticación utilizado inciden sobre la [cantidad de tweets que podremos obtener en cierto tiempo](https://developer.twitter.com/en/docs/twitter-api/rate-limits#v2-limits), qué [operadores/filtros podemos usar](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list) y cuantos días hacia atrás podremos minar**, así como sobre la extensión de los queries. \n",
    "\n",
    "Por ejemplo, los límites mensuales de obtención de tweets, dependiendo de la credencial empleada, son los siguientes:\n",
    "- *Essential*: 500k tweets en un mes y 512 caracteres por query.\n",
    "- *Elevated*: 2M tweets en un mes y 512 caracteres por query.\n",
    "- *Academic*: 10M tweets en un mes y 1024 caracteres por query.\n",
    "\n",
    "Cualquier otra diferencia entre nivel de credenciales, se puede [consultar aquí](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level)\n",
    "\n",
    "Notas:\n",
    "- Debido a que la web app del Laboratorio de Percepciones sobre Migración solo necesita permisos de lectura para contenido público, nos interesa obtener la credencial de tipo Bearer Token, ya que es el método de autenticación que más tweets nos permite obtener en menos tiempo.\n",
    "- Se usará la versión 2 de la [API de Twitter](https://developer.twitter.com/en/docs/twitter-api).\n",
    "- El ejercicio aquí propuesto puede realizarse con las credenciales de nivel [*Elevated*](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level), pero en caso de estar trabajando en un proyecto con fines académicos o de estudio, se puede [aplicar de manera gratuita a credenciales Académicas](https://developer.twitter.com/en/products/twitter-api/academic-research) y así trabajar con límites más amplios y operadores avanzados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-complaint",
   "metadata": {},
   "source": [
    "### Dependencias de Python\n",
    "Las dependencias principales que se utilizaron para el proyecto del Laboratorio de Percepciones sobre Migración son `twarc` y `pandas`. La última se utiliza para manipular y analizar los datos, mientras que [`twarc` es el paquete](https://developer.twitter.com/en/docs/twitter-api/rate-limits) que nos facilitará el proceso de minado, ya que se encarga de obtener automáticamente todos los atributos de los tweets así como de manejar los tiempos de espera cuando se llega a los [límites de minado de la API](https://developer.twitter.com/en/docs/twitter-api/rate-limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "foreign-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Indispensables\n",
    "# !pip install twarc==2.9.2\n",
    "# !pip install pandas==1.4.1\n",
    "\n",
    "# # Para mejor interacción gráfica\n",
    "# !tqdm==4.62.2\n",
    "# !pendulum==2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-joseph",
   "metadata": {},
   "source": [
    "## Imports & Credenciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absent-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pendulum\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from twarc.client2 import Twarc2\n",
    "from twarc.expansions import TWEET_FIELDS\n",
    "from twarc.expansions import ensure_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-parish",
   "metadata": {},
   "source": [
    "Ingresa tu propio Bearer Token o coméntalo, y descomenta el resto de atributos para con tus propias API Keys y Access Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "welcome-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_TWITTER_API = {\n",
    "    'bearer_token': \"Enter your own Bearer token\",\n",
    "\n",
    "    # 'api_key': \"Enter your own API Key\",\n",
    "    # 'api_secret_key': \"Enter your own API Secret Key\",\n",
    "    # 'access_token': \"Enter your own access_token\",\n",
    "    # 'access_token_secret': \"Enter your own access_token_secret\"\n",
    "}\n",
    "\n",
    "IS_ACADEMIC = False # Cambiar a True, si las credenciales son Academicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-lingerie",
   "metadata": {},
   "source": [
    "## 2. Minado de Tweets\n",
    "\n",
    "A fin de obtener una base de datos que contenga la información de interés, es necesario construir queries con las palabras clave relevantes, junto con los distintos operadores que ofrece la API de Twitter.\n",
    "\n",
    "La API de Twitter permite obtener diferentes piezas de información de los usuarios y los tweets que publican, según el tipo de operadores y queries utilizados. Para fines de esta notebook, nos centraremos en obtener tweets públicos en un intervalo de tiempo definido, a partir del contenido de palabras claves.\n",
    "\n",
    "Las palabras claves del tema que se desea minar se utilizan para construir queries que hacen la búsqueda más certera y mediante un proceso iterativo, estos se enriquecen hasta conseguir una base de datos de tamaño suficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-toolbox",
   "metadata": {},
   "source": [
    "<h3 id='endpoints_limits'>2a. Endpoints & Límites</h3>\n",
    "\n",
    "Para [buscar tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction) existen dos tipos de endpoint (canales de comunicación entre el cliente y servidor): [Recent Search](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/recent-search) y  [Full Archive](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/full-archive-search).\n",
    "- **Recent Search**: Permite realizar 450 requests (pedidos) a la API en una ventana de 15 minutos, obteniendo un máximo de 100 tweets por request. Sin embargo, solo se pueden obtener tweets publicados en los últimos 7 días.\n",
    "- **Full Archive**: Permite obtener tweets publicados desde el inicio de la red social, pero solo se pueden realizar 300 requests en una ventana de 15 minutos, obteniendo hasta 500 tweets por cada request. Es necesario solicitar [credenciales académicas](https://developer.twitter.com/en/products/twitter-api/academic-research) para poder utilizar este endpoint.\n",
    "\n",
    "**Nota**: No hay que preocuparse del código de error que aparece una vez que el minado alcanza estos límites, pues el paquete twarc se encarga de pausar la obtención de tweets una vez que se llega al límite de 450 (o 300) requests en la ventana de 15 minutos; una vez que pasa el tiempo necesario, twarc reanuda el proceso. Si se llega al límite de tweets en un mes, la función para."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-salad",
   "metadata": {},
   "source": [
    "### 2b. Operadores para la construcción de queries\n",
    "\n",
    "A fin de construir queries útiles que permitan elaborar una base de datos, se pueden utilizar distintos operadores que modifiquen el comportamiento de dicho query. En esta sección vamos a ver, de manera general, qué son y cómo funcionan los operadores. La lista y descripción completa de los mismos se puede [encontrar aquí](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query).\n",
    "\n",
    "Existen dos tipos de operadores:\n",
    "- **Standalone**: Se pueden usar solos o en conjunción de otros. Por ejemplo, buscar tweets con un hashtag en específico: `#migrantes`\n",
    "    - **keyword**: Hace match con tweets que contengan un string o conjunto de palabras o caracteres en específico. No es sensible a caracteres en mayúsculas o minúsculas; ni a acentos o caracteres especiales como ñ. Ejemplo: `migrantes OR inmigrantes`\n",
    "    - **\"exact phrase\"**: Parecido al anterior, permite considerar espacios y múltiples tokens. Tiene que estar entre comillas dobles. Ejemplo: `\"ola migrante\"`\n",
    "    - **#**: Hace match a tweets que tengan el hashtag incluido. Ejemplo: `#migraresunderecho`\n",
    "    - **@**: Hace match a tweets que mencionen a los usuarios incluidos. Ejemplo: `@IADB`\n",
    "\n",
    "\n",
    "- **Conjunction-required**: Es necesario que estén junto a mínimo un operador *standalone*. Por ejemplo, buscar tweets con un hashtag en específico, pero que incluyan imágenes y sean retweets: `#migrantes has:media is:retweet`\n",
    "    - **place_country**: Obtiene tweets que sean geo-localizable a cierto país. Hay que pasarle el código [ISO del país](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2). Ejemplo: `place_country:MX`\n",
    "    - **lang**: Obtiene tweets que estén escritos en el lenguaje indicado y tiene que estar junto con un standalone operator. Ejemplo: `migrantes lang:es`\n",
    "    - **is:retweet**: Obtiene solo tweets que sean retweets y tiene que estar junto con un standalone operator. Ejemplo: `migrantes is:retweet`\n",
    "\n",
    "<br>\n",
    "\n",
    "Además, como se mencionó en la sección de \"Credenciales para la API de Twitter\", existen operadores *core*, que son accesibles con cualquier nivel de acceso, así como los operadores *advanced*, que solo se pueden utilizar con un acceso académico.\n",
    "\n",
    "#### Operadores Lógicos\n",
    "\n",
    "- **AND**: Obtiene tweets que cumplan con los dos operadores, se logra dejando un espacio en blanco entre ellos. \n",
    "    - Ejemplo, obtener tweets que contienen la palabra *políticos* y el hashtag *#corruptos*: `politicos #corruptos`.\n",
    "- **OR**: Obtiene tweets que cumplan con alguno de los dos operadores. Hay que añadir el string \" OR \" entre los operadores. \n",
    "    - Ejemplo, tweets que contengan la palabra *migrantes* o *inmigrantes*: `migrantes OR inmigrantes`.\n",
    "- **NOT**: Obtiene tweets que no contengan el operador o la keyword negada. Se logra añadiendo un guion medio \"-\" antes del operador o keyword. \n",
    "    - Ejemplo, obtener tweets con la palabra *políticos* pero sin la palabra *corruptos*: `politicos -corruptos`\n",
    "    - Ejemplo, obtener tweets con la palabra *migrantes, pero que no sean retweets*: `migrantes -is:retweets`\n",
    "- **Grouping**: Sirve para agrupar operadores lógicos, y hay que encerrar los operadores entre parentesis. Un grupo no puede ser negado.\n",
    "    - Ejemplo, obtener tweets con la palabra *migrantes* y alguna de las palabras *llegan* o *salen*: `migrantes AND (llegan OR salen)`\n",
    "\n",
    "**Nota**: A menos de que haya paréntesis para especificar el orden de operadores, primero se resuelven aquellos que son *AND* y luego los *OR*. [Más aquí](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#boolean).\n",
    "\n",
    "La lista completa de operadores se puede ver [aquí](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-invitation",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-stroke",
   "metadata": {},
   "source": [
    "### 2c. Ejercicio de Minado\n",
    "\n",
    "La siguiente función recibe una lista de queries y un intervalo de tiempo para guardar los tweets en un archivo JSONL.\n",
    "\n",
    "**Nota**: Un archivo de extension .jsonl guarda un objeto json por cada línea separados por solo el salto de línea, no comas u otro separador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fitted-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(credentials_api,\n",
    "               queries_list,\n",
    "               output_file,\n",
    "               since_date, until_date,\n",
    "                  is_academic=False):\n",
    "    \"\"\"Function in charge of scrape tweets from the \n",
    "    official Twitter API, using the library named Twarc.\n",
    "\n",
    "    Args:\n",
    "        credentials_api (dict): Dictionary with the Twitter API credentials.\n",
    "        queries_list (list[str]): List of queries to scrape.\n",
    "        output_file (str): Path to the file in which to store the results.\n",
    "        since_date (datetime): Start of the time span to scrape.\n",
    "        until_date (datetime): End of the time span to scrape.\n",
    "        is_academic (bool, optional): If the credentials has Research \n",
    "                                      Academic access level.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instiate the Twarc Client\n",
    "    twarc_client = Twarc2(**credentials_api)\n",
    "\n",
    "    # Make some tweaks for using the research credentials\n",
    "    max_size = 100\n",
    "    tweet_fields = TWEET_FIELDS.copy()\n",
    "    search_func = twarc_client.search_recent\n",
    "    if(is_academic):\n",
    "        search_func = twarc_client.search_all\n",
    "        max_size = 500\n",
    "\n",
    "        # Remove the context_annotations attr to\n",
    "        # scrape 500 tweets per request\n",
    "        tweet_fields.remove('context_annotations')\n",
    "\n",
    "    tweet_fields = ','.join(tweet_fields)\n",
    "\n",
    "    with open(output_file, 'a') as pages_file:\n",
    "        for query in tqdm(queries_list):\n",
    "\n",
    "            search_results = search_func(query=query,\n",
    "                                         start_time=since_date,\n",
    "                                         end_time=until_date,\n",
    "                                         tweet_fields=tweet_fields,\n",
    "                                         max_results=max_size)\n",
    "\n",
    "            # Write all the obtained tweets\n",
    "            for page in search_results:\n",
    "\n",
    "                # Write one by one the tweets\n",
    "                for tweet in ensure_flattened(page):\n",
    "                    json.dump(tweet, pages_file, ensure_ascii=False)\n",
    "                    pages_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-giant",
   "metadata": {},
   "source": [
    "Se declara una lista de queries, el intervalo de fecha (para fines de este notebook, es 6 h hacia atrás de la hora actual) y las credenciales relevantes. En este caso se utiliza la credencial académica para que el request sea de 500 tweets.\n",
    "\n",
    "\n",
    "Para este ejemplo, se minarán tweets que:\n",
    "- Contengan \"migrantes\" y \"migración\", o alguna de sus variantes\n",
    "- Haya sido publicado en las últimas 3 horas\n",
    "\n",
    "Se guarda en un archivo llamado `1_tweets_test.jsonl`.\n",
    "\n",
    "Es importante señalar que la API de Twitter no acepta expresiones regulares o wildcards, es decir, no podemos pasarle un string como `(in|e)?migrantes` esperando que obtenga tweets con las palabras \"inmigrantes\", \"emigrantes\" o \"migrantes\". Por lo tanto, es necesario pasarle todas las variaciones de la keyword que esperamos se útil para obtener la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smooth-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_queries = ['migrante', 'inmigrante', 'emigrante', \n",
    "               'migrantes', 'inmigrantes', 'emigrantes',\n",
    "               'migración', 'inmigracion', 'emigracion']\n",
    "\n",
    "# De las ultimas 24 hrs\n",
    "date_end = pendulum.today()\n",
    "date_start = date_end.subtract(hours=3)\n",
    "\n",
    "# # O un rango de fechas definido\n",
    "# date_start = pendulum.datetime(year=2022, month=11, day=14)\n",
    "# date_end = pendulum.datetime(year=2022, month=11, day=13)\n",
    "\n",
    "file_tweets = os.path.abspath(\"./files/1_tweets_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "powerful-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrar el archivo si ya existe\n",
    "if(os.path.exists(file_tweets)):\n",
    "    os.remove(file_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "consecutive-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:25<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-battlefield",
   "metadata": {},
   "source": [
    "Con el siguiente comando (de bash) se puede revisar cuantos tweets obtuvimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "entitled-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653 ./files/1_tweets_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/1_tweets_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-decision",
   "metadata": {},
   "source": [
    "Es posible que existan tweets que contengan dos o más de las keywords, por lo que al hacer búsquedas diferentes, obtendremos N veces el mismo tweet. Por ejemplo, aquellos tweets que contengan las palabras \"migrante\" y \"migrantes\" los obtendremos dos veces. \n",
    "\n",
    "Esto se puede optimizar usando el operador lógico `OR`, así podremos hacer requests que aprovechen la longitud máxima de los queries, y reducir el número total de requests y el tiempo de minado, ya que estaremos obteniendo más tweets por request. Los límites de la longitud en caracteres de los queries se vieron en la sección [Credenciales para la API de Twitter](#credentials_api) y los request/tiempos en [Endpoints & Límites](#endpoints_limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minute-revolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes OR migración OR inmigracion OR emigracion'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Juntar las keywords en un solo string\n",
    "new_query = ' OR '.join(lst_queries)\n",
    "new_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-amazon",
   "metadata": {},
   "source": [
    "En lugar de 9 queries ahora tenemos solo 1 compuesto por 80 caracteres (de los 512 o 1024 posibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "every-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que poner el nuevo query dentro de una lista dado que es lo que espera la funcion\n",
    "lst_queries_2 = [new_query]\n",
    "\n",
    "# Nuevo archivo\n",
    "file_tweets = os.path.abspath(\"./files/2_tweets_test.jsonl\")\n",
    "\n",
    "if(os.path.exists(file_tweets)):\n",
    "    os.remove(file_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unsigned-stress",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.27s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries_2,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "quality-poetry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593 ./files/2_tweets_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/2_tweets_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-twelve",
   "metadata": {},
   "source": [
    "Esta optimización con operadores lógicos se puede extender a cuando necesitamos tweets que contengan dos o más posibles palabras. Por ejemplo, obtener los tweets que contengan alguna de las keywords: \"migrantes\", \"inmigrantes\" y la keyword \"bienvenidos\".\n",
    "\n",
    "El query sería `bienvenidos (migrantes OR inmigrantes)`, recordando que:\n",
    "- Los paréntesis agrupan el OR \n",
    "- El espacio entre la primera keyword y los paréntesis es un `AND`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-complaint",
   "metadata": {},
   "source": [
    "Si se quiere añadir otra keyword opcional a \"bienvenidos\", sería `(hermanos OR bienvenidos) (migrantes OR inmigrantes)`. Las combinaciones que forma son: `hermanos AND migrantes`, \"hermanos inmigrantes\", \"bienvenidos migrantes\" y \"bienvenidos inmigrantes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-psychiatry",
   "metadata": {},
   "source": [
    "### 2d. Ejemplo final de queries\n",
    "\n",
    "Para tener un ejemplo más completo, vamos a obtener tweets que:\n",
    "- Contengan alguna de las siguientes palabras: \"migrantes\", \"inmigrantes\" o \"emigrantes\"\n",
    "- O que tengan alguno de estos hashtags: #migraresunderecho,  #todossomomigrantes o #heramanomigrante\n",
    "- Que estén en español\n",
    "- Que hayan sido publicados por alguien en México o en Argentina\n",
    "- Y fuesen publicados en los últimos 5 días\n",
    "\n",
    "\n",
    "Nota: Dado que se están añadiendo filtros de lenguajes y sobre todo de país, se espera obtener un volumen menor de tweets a que si solo fueran los keywords y hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "apart-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_part = \"(migrantes OR inmigrantes OR emigrantes)\"\n",
    "hashtags_part = \"(#migraresunderecho OR #todossomomigrantes OR #hermanomigrante)\"\n",
    "language_part = \"lang:es\"\n",
    "country_part = \"(place_country:MX OR place_country:AR)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aggressive-importance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((migrantes OR inmigrantes OR emigrantes) OR (#migraresunderecho OR #todossomomigrantes OR #hermanomigrante)) lang:es (place_country:MX OR place_country:AR)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_query = f\"({keywords_part} OR {hashtags_part}) {language_part} {country_part}\"\n",
    "final_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "specified-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que poner el nuevo query dentro de una lista dado que es lo que espera la funcion\n",
    "lst_queries_3 = [final_query]\n",
    "\n",
    "# De las ultimas 5 dias\n",
    "date_end = pendulum.today()\n",
    "date_start = date_end.subtract(days=5)\n",
    "\n",
    "# Nuevo archivo\n",
    "file_tweets = os.path.abspath(\"./files/3_tweets_test.jsonl\")\n",
    "\n",
    "if(os.path.exists(file_tweets)):\n",
    "    os.remove(file_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "contained-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries_3,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "actual-venture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 ./files/3_tweets_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/3_tweets_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-switzerland",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-setting",
   "metadata": {},
   "source": [
    "### 2e. Objeto Tweet\n",
    "Antes de terminar esta notebook, hay que revisar los campos contenidos en el [objeto tweet](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet).\n",
    "\n",
    "Los campos más importantes son:\n",
    "- **id**: Identificador unico de cada tweet\n",
    "- **conversation_id**: Identificador unico del tweet que inicio la conversación\n",
    "- **created_at**: Fecha y hora en la que se publicó el tweet\n",
    "- **text**: Texto publicado en el tweet\n",
    "- **possibly_sensitive**: Si el texto incluye links a contenido posiblemente sensible\n",
    "- **lang**: Idioma en el que está escrito el tweet\n",
    "- **source**: Sí fue publicado desde un iPhone, Android, un buscador, etc.\n",
    "- **geo**: Objeto que contiene el lugar, país y coordenadas que se asignaron al tweet\n",
    "- **author_id**: Identificador único del autor\n",
    "- **author**: Objeto que incluye atributos del autor\n",
    "- **public_metrics**: Objeto que tiene el conteo de retweets, replies, likes y quotes\n",
    "\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"1589089368777850880\",\n",
    "    \"conversation_id\": \"1589089368777850880\",\n",
    "    \"created_at\": \"2022-11-06T02:56:44.000Z\",\n",
    "    \"text\": \"Primer vuelo humanitario para repatriar a venezolanos que estaban en #México \\n\\n140 personas regresaron voluntariamente en un vuelo coordinado por @INAMI_mx \\n\\nSegún las autoridades mexicanas continuarán apoyando el regreso de migrantes a su país. https://t.co/JuUjRB3oQk\",\n",
    "    \"possibly_sensitive\": false,\n",
    "    \"lang\": \"es\",\n",
    "    \"source\": \"Twitter for iPhone\",\n",
    "\n",
    "    \"geo\": {\n",
    "        \"place_id\": \"2a376531dff3d76a\",\n",
    "        \"full_name\": \"Tlalpan, Distrito Federal\",\n",
    "        \"name\": \"Tlalpan\",\n",
    "        \"place_type\": \"city\",\n",
    "        \"id\": \"2a376531dff3d76a\",\n",
    "        \"country\": \"México\",\n",
    "        \"geo\": {\n",
    "            \"type\": \"Feature\",\n",
    "            \"bbox\": [-99.315748, 19.087511, -99.1009804, 19.311459],\n",
    "            \"properties\": {}\n",
    "        },\n",
    "        \"country_code\": \"MX\"\n",
    "    },\n",
    "\n",
    "    \"author_id\": \"114573824\",\n",
    "    \"author\": {\n",
    "        \"verified\": false,\n",
    "        \"protected\": false,\n",
    "        \"profile_image_url\": \"https://pbs.twimg.com/profile_images/1569749213822570500/Xf8yCPws_normal.jpg\",\n",
    "        \"created_at\": \"2010-02-15T21:50:44.000Z\",\n",
    "        \"pinned_tweet_id\": \"1588664226729832448\",\n",
    "        \"url\": \"https://t.co/E1sEHNlBkT\",\n",
    "        \"description\": \"Periodista mexicano 🗞 @aztecanoticias\",\n",
    "        \"public_metrics\": {\n",
    "            \"followers_count\": 11893,\n",
    "            \"following_count\": 732,\n",
    "            \"tweet_count\": 24530,\n",
    "            \"listed_count\": 25\n",
    "        },\n",
    "        \"entities\": {\n",
    "            \"url\": {\n",
    "                \"urls\": [{\n",
    "                    \"start\": 0,\n",
    "                    \"end\": 23,\n",
    "                    \"url\": \"https://t.co/E1sEHNlBkT\",\n",
    "                    \"expanded_url\": \"http://www.facebook.com/OtonielMartínez\",\n",
    "                    \"display_url\": \"facebook.com/OtonielMartínez\"\n",
    "                }]\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"mentions\": [{\n",
    "                    \"start\": 22,\n",
    "                    \"end\": 37,\n",
    "                    \"username\": \"aztecanoticias\"\n",
    "                }]\n",
    "            }\n",
    "        },\n",
    "        \"name\": \"OTONIEL MARTÍNEZ\",\n",
    "        \"username\": \"_otomartinez\",\n",
    "        \"location\": \"MÉXICO\",\n",
    "        \"id\": \"114573824\"\n",
    "    },\n",
    "\n",
    "    \"public_metrics\": {\n",
    "        \"retweet_count\": 4,\n",
    "        \"reply_count\": 1,\n",
    "        \"like_count\": 19,\n",
    "        \"quote_count\": 0\n",
    "    },\n",
    "    \"reply_settings\": \"everyone\",\n",
    "    \"attachments\": {\n",
    "        \"media_keys\": [\"3_1589089086119333889\", \"3_1589089086245146626\"],\n",
    "        \"media\": [{\n",
    "            \"type\": \"photo\",\n",
    "            \"media_key\": \"3_1589089086119333889\",\n",
    "            \"width\": 774,\n",
    "            \"url\": \"https://pbs.twimg.com/media/Fg2UGr6X0AEKzBx.jpg\",\n",
    "            \"height\": 1024\n",
    "        }, {\n",
    "            \"type\": \"photo\",\n",
    "            \"media_key\": \"3_1589089086245146626\",\n",
    "            \"width\": 731,\n",
    "            \"url\": \"https://pbs.twimg.com/media/Fg2UGsYXkAIC8S4.jpg\",\n",
    "            \"height\": 1024\n",
    "        }]\n",
    "    },\n",
    "\n",
    "    \"entities\": {\n",
    "        \"annotations\": [{\n",
    "            \"start\": 70,\n",
    "            \"end\": 75,\n",
    "            \"probability\": 0.9717,\n",
    "            \"type\": \"Organization\",\n",
    "            \"normalized_text\": \"México\"\n",
    "        }],\n",
    "        \"urls\": [{\n",
    "            \"start\": 246,\n",
    "            \"end\": 269,\n",
    "            \"url\": \"https://t.co/JuUjRB3oQk\",\n",
    "            \"expanded_url\": \"https://twitter.com/_otomartinez/status/1589089368777850880/photo/1\",\n",
    "            \"display_url\": \"pic.twitter.com/JuUjRB3oQk\",\n",
    "            \"media_key\": \"3_1589089086119333889\"\n",
    "        }, {\n",
    "            \"start\": 246,\n",
    "            \"end\": 269,\n",
    "            \"url\": \"https://t.co/JuUjRB3oQk\",\n",
    "            \"expanded_url\": \"https://twitter.com/_otomartinez/status/1589089368777850880/photo/1\",\n",
    "            \"display_url\": \"pic.twitter.com/JuUjRB3oQk\",\n",
    "            \"media_key\": \"3_1589089086245146626\"\n",
    "        }],\n",
    "        \"hashtags\": [{\n",
    "            \"start\": 69,\n",
    "            \"end\": 76,\n",
    "            \"tag\": \"México\"\n",
    "        }],\n",
    "        \"mentions\": [{\n",
    "            \"start\": 146,\n",
    "            \"end\": 155,\n",
    "            \"username\": \"INAMI_mx\",\n",
    "            \"id\": \"1300283125\",\n",
    "            \"verified\": true,\n",
    "            \"protected\": false,\n",
    "            \"profile_image_url\": \"https://pbs.twimg.com/profile_images/1542852123821424640/exw6RvmZ_normal.jpg\",\n",
    "            \"created_at\": \"2013-03-25T17:02:40.000Z\",\n",
    "            \"pinned_tweet_id\": \"1589088702302945280\",\n",
    "            \"url\": \"https://t.co/aMyjqh7MV3\",\n",
    "            \"description\": \"Instituto Nacional de Migración\",\n",
    "            \"public_metrics\": {\n",
    "                \"followers_count\": 52464,\n",
    "                \"following_count\": 656,\n",
    "                \"tweet_count\": 27656,\n",
    "                \"listed_count\": 334\n",
    "            },\n",
    "            \"entities\": {\n",
    "                \"url\": {\n",
    "                    \"urls\": [{\n",
    "                        \"start\": 0,\n",
    "                        \"end\": 23,\n",
    "                        \"url\": \"https://t.co/aMyjqh7MV3\",\n",
    "                        \"expanded_url\": \"https://www.gob.mx/inm\",\n",
    "                        \"display_url\": \"gob.mx/inm\"\n",
    "                    }]\n",
    "                }\n",
    "            },\n",
    "            \"name\": \"INM\",\n",
    "            \"location\": \"México\"\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
