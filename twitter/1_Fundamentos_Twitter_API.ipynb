{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reasonable-nature",
   "metadata": {},
   "source": [
    "# Extracci√≥n de Tweets\n",
    "\n",
    "En esta notebook se van a revisar los pasos para lograr un buen minado de tweets. Para ello se tocan los siguientes puntos: \n",
    "- Qu√© es la API de Twitter y c√≥mo utilizarla\n",
    "- Operadores para definir queries\n",
    "- Ejemplos de minado\n",
    "\n",
    "**TODO: Porque es util minar de Twitter?**\n",
    "\n",
    "Twitter provee una API (interfaz de programaci√≥n de aplicaciones) para acceder a sus datos. Las API son mecanismos que permiten a dos componentes de software comunicarse entre s√≠ mediante un conjunto de definiciones y protocolos. Por ejemplo, el sistema de software del instituto de meteorolog√≠a contiene datos meteorol√≥gicos diarios. La aplicaci√≥n meteorol√≥gica de su tel√©fono ‚Äúhabla‚Äù con este sistema a trav√©s de las API y le muestra las actualizaciones meteorol√≥gicas diarias en su tel√©fono. [Aqu√≠ una explicaci√≥n de AWS](https://aws.amazon.com/es/what-is/api/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-cylinder",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "\n",
    "<h3 id='credentials_api'>Credenciales para la API de Twitter</h3>\n",
    "\n",
    "Para obtener tweets con la [API Oficial de Twitter](https://developer.twitter.com/en/docs/twitter-api) es necesario registrarse en el [portal para desarrolladores de Twitter](https://developer.twitter.com/en/docs/developer-portal/overview) para as√≠ dar de alta un proyecto y que se te asignen las [credenciales necesarias](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api).\n",
    "\n",
    "Hay que aclarar que **seg√∫n el tipo de credenciales y el m√©todo de autenticaci√≥n ser√°n diferente la [cantidad de tweets que podremos obtener en cierto tiempo](https://developer.twitter.com/en/docs/twitter-api/rate-limits#v2-limits), qu√© [operadores/filtros podemos usar](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list) y cuantos d√≠as hacia atr√°s podremos minar** y la extensi√≥n de los queries. \n",
    "\n",
    "Por ejemplo, los l√≠mites mensuales de obtenci√≥n de tweets, dependiendo de la credencial empleada, son los siguientes:\n",
    "- *Essential*: 500k tweets en un mes y 512 caracteres por query.\n",
    "- *Elevated*: 2M tweets en un mes y 512 caracteres por query.\n",
    "- *Academic*: 10M tweets en un mes y 1024 caracteres por query.\n",
    "\n",
    "Cualquier otra diferencia entre nivel de credenciales, se puede [ver aqu√≠](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level)\n",
    "\n",
    "Notas:\n",
    "- Debido a que la web app del Laboratorio de Migraci√≥n solo necesita permisos de lectura para contenido p√∫blico, nos interesa obtener la credencial de tipo Bearer Token, ya que es el m√©todo de autenticaci√≥n que m√°s tweets nos permite obtener en menos tiempo.\n",
    "- Se usar√° la versi√≥n 2 de la [API de Twitter](https://developer.twitter.com/en/docs/twitter-api).\n",
    "- El ejercicio aqu√≠ propuesto puede realizarse con las credenciales de nivel [*Elevated*](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level), pero en caso de estar trabajando en un proyecto con fines acad√©micos o de estudio, se puede [aplicar de manera gratuita a credenciales Acad√©micas](https://developer.twitter.com/en/products/twitter-api/academic-research) y as√≠ trabajar con l√≠mites m√°s amplios y operadores avanzados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-layer",
   "metadata": {},
   "source": [
    "### Dependencias de Python\n",
    "Las dependencias principales son `twarc`, `pandas` y `numpy`. Las √∫ltimas dos son para manipular y trabajar con los datos, mientras que [`twarc` es el paquete](https://developer.twitter.com/en/docs/twitter-api/rate-limits) que nos facilitar√° el proceso de minado, ya que se encarga de obtener autom√°ticamente todos los atributos de los tweets as√≠ como de manejar los tiempos de espera cuando se llega a los [l√≠mites de minado de la API](https://developer.twitter.com/en/docs/twitter-api/rate-limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "serial-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: twarc==2.9.2 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (2.9.2)\n",
      "Requirement already satisfied: humanize>=3.9 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (3.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (2.8.2)\n",
      "Requirement already satisfied: requests-oauthlib>=1.3 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (1.3.0)\n",
      "Requirement already satisfied: tqdm>=4.62 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (4.62.2)\n",
      "Requirement already satisfied: click-config-file>=0.6 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (0.6.0)\n",
      "Requirement already satisfied: click-plugins>=1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (1.1.1)\n",
      "Requirement already satisfied: click<9,>=7 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from twarc==2.9.2) (7.1.2)\n",
      "Requirement already satisfied: configobj>=5.0.6 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from click-config-file>=0.6->twarc==2.9.2) (5.0.6)\n",
      "Requirement already satisfied: setuptools in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from humanize>=3.9->twarc==2.9.2) (57.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from python-dateutil>=2.8->twarc==2.9.2) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests-oauthlib>=1.3->twarc==2.9.2) (2.27.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests-oauthlib>=1.3->twarc==2.9.2) (3.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc==2.9.2) (2020.12.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/noecampos/.pyenv/versions/BID/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: pandas==1.4.1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from pandas==1.4.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from pandas==1.4.1) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from pandas==1.4.1) (1.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas==1.4.1) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/noecampos/.pyenv/versions/BID/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: numpy==1.22.3 in /home/noecampos/.pyenv/versions/BID/lib/python3.8/site-packages (1.22.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/noecampos/.pyenv/versions/BID/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Indispensables\n",
    "!pip install twarc==2.9.2\n",
    "!pip install pandas==1.4.1\n",
    "!pip install numpy==1.22.3\n",
    "\n",
    "# Para mejor interacci√≥n gr√°fica\n",
    "!tqdm==4.62.2\n",
    "!pendulum==2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-header",
   "metadata": {},
   "source": [
    "## Imports & Credenciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uniform-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pendulum\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from twarc.client2 import Twarc2\n",
    "from twarc.expansions import TWEET_FIELDS\n",
    "from twarc.expansions import ensure_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-mongolia",
   "metadata": {},
   "source": [
    "Ingresa tu propio Bearer Token o com√©ntalo, y descomenta el resto de atributos para con tus propias API Keys y Access Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "binary-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_TWITTER_API = {\n",
    "    'bearer_token': \"Enter your own Bearer token\",\n",
    "\n",
    "    # 'api_key': \"Enter your own API Key\",\n",
    "    # 'api_secret_key': \"Enter your own API Secret Key\",\n",
    "    # 'access_token': \"Enter your own access_token\",\n",
    "    # 'access_token_secret': \"Enter your own access_token_secret\"\n",
    "}\n",
    "\n",
    "IS_ACADEMIC = False # Cambiar a True, si las credenciales son Academicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-article",
   "metadata": {},
   "source": [
    "## Minado de Tweets\n",
    "\n",
    "La API de Twitter permite obtener diferentes piezas de informaci√≥n a partir de los usuarios y los tweets que publican, seg√∫n el tipo de operadores y queries utilizados. Para fines de esta notebook, nos centraremos en obtener tweets p√∫blicos en un intervalo de tiempo definido, a partir del contenido de palabras claves.\n",
    "\n",
    "Las palabras claves del tema que se desea minar se utilizan para construir queries que hacen la b√∫squeda m√°s certera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-digit",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-retailer",
   "metadata": {},
   "source": [
    "<h3 id='endpoints_limits'>Endpoints & L√≠mites</h3>\n",
    "\n",
    "Para [buscar tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction) hay dos endpoints, [Recent Search](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/recent-search) y  [Full Archive](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/full-archive-search).\n",
    "- **Recent Search**: Nos permite realizar 450 requests (pedidos) a la API en una ventana de 15 minutos, obteniendo m√°ximo de 100 tweets por request. Sin embargo, solo se pueden obtener tweets publicados en los √∫ltimos 7 d√≠as.\n",
    "- **Full Archive**: Podremos obtener tweets publicados desde el inicio de la red social, pero solo se pueden realizar 300 requests en una ventana de 15 minutos, obteniendo hasta 500 tweets por cada request. Hay que aplicar a las [credenciales acad√©micas](https://developer.twitter.com/en/products/twitter-api/academic-research) para poder utilizar este endpoint.\n",
    "\n",
    "**Nota**: No hay que preocuparse del c√≥digo de error que aparece una vez que el minado alcanza estos l√≠mites, pues el paquete twarc se encarga de pausar la obtenci√≥n de tweets una vez que se llega al l√≠mite de 450 (o 300) requests en la ventana de 15 minutos; una vez que pasa un tiempo necesario, twarc reanuda el proceso. Si se llega al l√≠mite de tweets en un mes, la funci√≥n para."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-hardwood",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-question",
   "metadata": {},
   "source": [
    "### Operadores\n",
    "\n",
    "En esta secci√≥n vamos a ver de manera general qu√© son y c√≥mo funcionan los operadores, pero la lista y descripci√≥n completa de los mismos se puede [encontrar aqu√≠](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query).\n",
    "\n",
    "Existen dos tipos de operadores:\n",
    "- **Standalone**: Se pueden usar solos o en conjunci√≥n de otros. Por ejemplo, buscar tweets con un hashtag en espec√≠fico: `#migrantes`.\n",
    "- **Conjunction-required**: Es necesario que est√©n junto a m√≠nimo un operador *standalone*. Por ejemplo, buscar tweets con un hashtag en espec√≠fico, pero que incluyan im√°genes y sean retweets: `#migrantes has:media is:retweet`\n",
    "\n",
    "Adem√°s, como se mencion√≥ en la secci√≥n de \"Credenciales para la API de Twitter\", existen operadores *core*, que son accesibles con cualquier nivel de acceso, as√≠ como los operadores *advanced*, que solo se pueden utilizar con un acceso acad√©mico.\n",
    "\n",
    "#### Operadores L√≥gicos\n",
    "\n",
    "- **AND**: Obtiene tweets que cumplan con los dos operadores, se logra dejando un espacio en blanco entre ellos. \n",
    "    - Ejemplo, obtener tweets que contienen la palabra *pol√≠ticos* y el hashtag *#corruptos*: `politicos #corruptos`.\n",
    "- **OR**: Obtiene tweets que cumplan con alguno de los dos operadores. Hay que a√±adir el string \" OR \" entre los operadores. \n",
    "    - Ejemplo, tweets que contengan la palabra *migrantes* o *inmigrantes*: `migrantes OR inmigrantes`.\n",
    "- **NOT**: Obtiene tweets que no contengan el operador o la keyword negada. Se logra a√±adiendo un guion medio \"-\" antes del operador o keyword. \n",
    "    - Ejemplo, obtener tweets con la palabra *pol√≠ticos* pero sin la palabra *corruptos*: `politicos -corruptos`\n",
    "    - Ejemplo, obtener tweets con la palabra *migrantes, pero que no sean retweets*: `migrantes -is:retweets`\n",
    "- **Grouping**: Sirve para agrupar operadores l√≥gicos, y hay que encerrar los operadores entre parentesis. Un grupo no puede ser negado.\n",
    "    - Ejemplo, obtener tweets con la palabra *migrantes* y alguna de las palabras *llegan* o *salen*: `migrantes AND (llegan OR salen)`\n",
    "\n",
    "**Nota**: A menos de que haya par√©ntesis para especificar el orden de operadores, primero se resuelven aquellos que son *AND* y luego los *OR*. [M√°s aqu√≠](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#boolean).\n",
    "\n",
    "#### M√°s Operadores\n",
    "\n",
    "- **keyword**: Hace match con tweets que contengan un string en espec√≠fico. No es sensible a caracteres en may√∫sculas o min√∫sculas; acentos o caracteres especiales como √±. Ejemplo: `migrantes OR inmigrantes`.\n",
    "- **\"exact phrase\"**: Parecido al anterior, permite considerar espacios y m√∫ltiples tokens. Tiene que estar entre comillas dobles. Ejemplo: `\"ola migrante\"`.\n",
    "- **#**: Hace match a tweets que tengan el hashtag incluido. Ejemplo: `#migraresunderecho`.\n",
    "- **@**: Hace match a tweets que mencionen a los usuarios incluidos. Ejemplo: `@IADB`.\n",
    "- **place_country**: Obtiene tweets que sean geo-localizable a cierto pa√≠s. Hay que pasarle el c√≥digo [ISO del pa√≠s](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2). Ejemplo: `place_country:MX`.\n",
    "- **lang**: Obtiene tweets que est√©n escrito en el lenguaje indicado y tiene que estar junto con un standalone operator. Ejemplo: `migrantes lang:es`.\n",
    "- **is:retweet**: Obtiene solo tweets que sean retweets y tiene que estar junto con un standalone operator. Ejemplo: `migrantes is:retweet`.\n",
    "\n",
    "La lista completa de operadores se puede ver [aqu√≠](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-receptor",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-railway",
   "metadata": {},
   "source": [
    "### Ejercicio de Minado\n",
    "\n",
    "La siguiente funci√≥n recibe una lista de queries y un intervalo de tiempo para guardar los tweets en un archivo JSONL.\n",
    "\n",
    "**Nota**: Un archivo de extension .jsonl guarda un objeto json por cada l√≠nea separados por solo el salto de l√≠nea, no comas u otro separador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "limited-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(credentials_api,\n",
    "               queries_list,\n",
    "               output_file,\n",
    "               since_date, until_date,\n",
    "               is_academic=False):\n",
    "    \"\"\"Function in charge of scrape tweets from the \n",
    "    official Twitter API, using the library named Twarc.\n",
    "\n",
    "    Args:\n",
    "        credentials_api (dict): Dictionary with the Twitter API credentials.\n",
    "        queries_list (list[str]): List of queries to scrape.\n",
    "        output_file (str): Path to the file in which to store the results.\n",
    "        since_date (datetime): Start of the time span to scrape.\n",
    "        until_date (datetime): End of the time span to scrape.\n",
    "        is_academic (bool, optional): If the credentials has Research \n",
    "                                      Academic access level.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instiate the Twarc Client\n",
    "    twarc_client = Twarc2(**credentials_api)\n",
    "\n",
    "    # Make some tweaks for using the research credentials\n",
    "    max_size = 100\n",
    "    tweet_fields = TWEET_FIELDS.copy()\n",
    "    search_func = twarc_client.search_recent\n",
    "    if(is_academic):\n",
    "        search_func = twarc_client.search_all\n",
    "        max_size = 500\n",
    "\n",
    "        # Remove the context_annotations attr to\n",
    "        # scrape 500 tweets per request\n",
    "        tweet_fields.remove('context_annotations')\n",
    "\n",
    "    tweet_fields = ','.join(tweet_fields)\n",
    "\n",
    "    with open(output_file, 'a') as pages_file:\n",
    "        for query in tqdm(queries_list):\n",
    "\n",
    "            search_results = search_func(query=query,\n",
    "                                         start_time=since_date,\n",
    "                                         end_time=until_date,\n",
    "                                         tweet_fields=tweet_fields,\n",
    "                                         max_results=max_size)\n",
    "\n",
    "            # Write all the obtained tweets\n",
    "            for page in search_results:\n",
    "\n",
    "                # Write one by one the tweets\n",
    "                for tweet in ensure_flattened(page):\n",
    "                    json.dump(tweet, pages_file, ensure_ascii=False)\n",
    "                    pages_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-investment",
   "metadata": {},
   "source": [
    "Se declara la lista de queries, el intervalo de fecha (para fines de este notebook, es 6 h hacia atr√°s de la hora actual) y si las credenciales que se est√°n utilizando tienen un nivel de acceso acad√©mico. Esto √∫ltimo es √∫til para pedir que cada request tenga 500 tweets.\n",
    "\n",
    "\n",
    "Para este ejemplo, se minar√°n tweets que:\n",
    "- Contengan \"migrantes\" y \"migraci√≥n\", o alguna de sus variantes\n",
    "- Haya sido publicado en las √∫ltimas 3 horas\n",
    "\n",
    "Y se va a guardar en un archivo llamado `1_tweets_test.jsonl`.\n",
    "\n",
    "Algo a tomar en cuenta, es que la API de Twitter no acepta expresiones regulares o wildcards, es decir, no podemos pasarle un string como `(in|e)?migrantes` esperando que obtenga tweets con las palabras \"inmigrantes\", \"emigrantes\" o \"migrantes\". Hay que pasarle todas las variaciones de la keyword que esperamos obtengan informaci√≥n √∫til."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accurate-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_queries = ['migrante', 'inmigrante', 'emigrante', \n",
    "               'migrantes', 'inmigrantes', 'emigrantes',\n",
    "               'migraci√≥n', 'inmigracion', 'emigracion']\n",
    "\n",
    "# De las ultimas 24 hrs\n",
    "date_end = pendulum.today()\n",
    "date_start = date_end.subtract(hours=3)\n",
    "\n",
    "# # O un rango de fechas definido\n",
    "# date_start = pendulum.datetime(year=2022, month=11, day=14)\n",
    "# date_end = pendulum.datetime(year=2022, month=11, day=13)\n",
    "\n",
    "file_tweets = os.path.abspath(\"./files/1_tweets_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "obvious-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrar el archivo si ya existe\n",
    "if(os.path.exists(file_tweets)):\n",
    "    os.remove(file_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "static-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:43<00:00,  4.81s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-slovak",
   "metadata": {},
   "source": [
    "Con el siguiente comando (de bash) se puede revisar cuantos tweets obtuvimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "happy-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3388 ./files/1_tweets_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/1_tweets_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-springfield",
   "metadata": {},
   "source": [
    "Es posible que existan tweets que contengan dos o m√°s de las keywords, por lo que al hacer b√∫squedas diferentes, obtendremos N veces el mismo tweet. Por ejemplo, aquellos tweets que contengan las palabras \"migrante\" y \"migrantes\" los obtendremos dos veces. \n",
    "\n",
    "Esto se puede optimizar usando el operador l√≥gicos `OR`, as√≠ podremos hacer request que aprovechen la longitud m√°xima de los queries, reducir el n√∫mero de request y tiempo de minado, ya que estaremos obteniendo m√°s tweets por request. Los l√≠mites de la longitud en caracteres de los queries se vieron en la secci√≥n [Credenciales para la API de Twitter](#credentials_api) y los request/tiempos en [Endpoints & L√≠mites](#endpoints_limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "computational-visitor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'migrante OR inmigrante OR emigrante OR migrantes OR inmigrantes OR emigrantes OR migraci√≥n OR inmigracion OR emigracion'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Juntar las keywords en un solo string\n",
    "new_query = ' OR '.join(lst_queries)\n",
    "new_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-capital",
   "metadata": {},
   "source": [
    "En lugar de 9 queries ahora tenemos solo 1 compuesto por 80 caracteres (de los 512 o 1024 posibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ready-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que poner el nuevo query dentro de una lista dado que es lo que espera la funcion\n",
    "lst_queries_2 = [new_query]\n",
    "\n",
    "# Nuevo archivo\n",
    "file_tweets = os.path.abspath(\"./files/2_tweets_test.jsonl\")\n",
    "\n",
    "if(os.path.exists(file_tweets)):\n",
    "    os.remove(file_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "engaging-latex",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:35<00:00, 35.23s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries_2,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "august-survival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3243 ./files/2_tweets_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/2_tweets_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-separate",
   "metadata": {},
   "source": [
    "Esta optimizaci√≥n con operadores l√≥gicos se puede extender a cuando necesitamos tweets que contengan dos o m√°s posibles palabras. Por ejemplo, obtener los tweets que contengan alguna de las keywords: \"migrantes\", \"inmigrantes\" y la keyword \"bienvenidos\".\n",
    "\n",
    "El query ser√≠a `bienvenidos (migrantes OR inmigrantes)`, recordando que:\n",
    "- Los par√©ntesis agrupan el OR \n",
    "- El espacio entre la primera keyword y los parentesis es un `AND`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-vessel",
   "metadata": {},
   "source": [
    "Si se quiere a√±adir otra keyword opci√≥n a \"bienvenidos\", ser√≠a `(hermanos OR bienvenidos) (migrantes OR inmigrantes)`. Las combinaciones que forma son: `hermanos AND migrantes`, \"hermanos inmigrantes\", \"bienvenidos migrantes\" y \"bienvenidos inmigrantes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-modern",
   "metadata": {},
   "source": [
    "### Ejemplo final de queries\n",
    "\n",
    "Como un ejemplo m√°s completo, vamos a obtener tweets que:\n",
    "- Contengan alguna de las siguientes palabras: \"migrantes\", \"inmigrantes\" o \"emigrantes\"\n",
    "- O que tengan alguno de estos hashtags: #migraresunderecho,  #todossomomigrantes o #heramanomigrante\n",
    "- Que est√©n en espa√±ol\n",
    "- Que hayan sido publicados por alguien en M√©xico o en Argentina\n",
    "- Y fuesen publicados en los √∫ltimos 5 d√≠as\n",
    "\n",
    "\n",
    "Nota: Dado que se est√°n a√±adiendo filtros de lenguajes y sobre todo de pa√≠s, se espera obtener un volumen menor de tweets a que si solo fueran los keywords y hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "liberal-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_part = \"(migrantes OR inmigrantes OR emigrantes)\"\n",
    "hashtags_part = \"(#migraresunderecho OR #todossomomigrantes OR #hermanomigrante)\"\n",
    "language_part = \"lang:es\"\n",
    "country_part = \"(place_country:MX OR place_country:AR)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "enhanced-circus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((migrantes OR inmigrantes OR emigrantes) OR (#migraresunderecho OR #todossomomigrantes OR #hermanomigrante)) lang:es (place_country:MX OR place_country:AR)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_query = f\"({keywords_part} OR {hashtags_part}) {language_part} {country_part}\"\n",
    "final_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unable-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que poner el nuevo query dentro de una lista dado que es lo que espera la funcion\n",
    "lst_queries_3 = [final_query]\n",
    "\n",
    "# De las ultimas 5 dias\n",
    "date_end = pendulum.today()\n",
    "date_start = date_end.subtract(days=5)\n",
    "\n",
    "# Nuevo archivo\n",
    "file_tweets = os.path.abspath(\"./files/3_tweets_test.jsonl\")\n",
    "\n",
    "if(os.path.exists(file_tweets)):\n",
    "    os.remove(file_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "separated-worry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.06s/it]\n"
     ]
    }
   ],
   "source": [
    "get_tweets(credentials_api = CREDENTIALS_TWITTER_API,\n",
    "           queries_list = lst_queries_3,\n",
    "           output_file = file_tweets,\n",
    "           since_date = date_start, \n",
    "           until_date = date_end,\n",
    "           is_academic= IS_ACADEMIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "advised-plastic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 ./files/3_tweets_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./files/3_tweets_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-joyce",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-expense",
   "metadata": {},
   "source": [
    "### Objeto Tweet\n",
    "Antes de terminar esta notebook, hay que revisar que campos contiene el [objeto tweet](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet).\n",
    "\n",
    "Los campos m√°s importantes son:\n",
    "- **id**: Identificador unico de cada tweet\n",
    "- **conversation_id**: Identificador unico del tweet que inicio la conversaci√≥n\n",
    "- **created_at**: Fecha y hora en la que se public√≥ el tweet\n",
    "- **text**: Texto publicado en el tweet\n",
    "- **possibly_sensitive**: Si el texto incluye links a contenido posiblemente sensible\n",
    "- **lang**: Idioma en el que est√° escrito el tweet\n",
    "- **source**: S√≠ fue publicado desde un iPhone, Android, un buscador, etc.\n",
    "- **geo**: Objeto que contiene el lugar, pa√≠s y coordenadas que se asignaron al tweet\n",
    "- **author_id**: Identificador √∫nico del autor\n",
    "- **author**: Objeto que incluye atributos del autor\n",
    "- **public_metrics**: Objeto que tiene el conteo de retweets, replies, likes y quotes\n",
    "\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"1589089368777850880\",\n",
    "    \"conversation_id\": \"1589089368777850880\",\n",
    "    \"created_at\": \"2022-11-06T02:56:44.000Z\",\n",
    "    \"text\": \"Primer vuelo humanitario para repatriar a venezolanos que estaban en #M√©xico \\n\\n140 personas regresaron voluntariamente en un vuelo coordinado por @INAMI_mx \\n\\nSeg√∫n las autoridades mexicanas continuar√°n apoyando el regreso de migrantes a su pa√≠s. https://t.co/JuUjRB3oQk\",\n",
    "    \"possibly_sensitive\": false,\n",
    "    \"lang\": \"es\",\n",
    "    \"source\": \"Twitter for iPhone\",\n",
    "\n",
    "    \"geo\": {\n",
    "        \"place_id\": \"2a376531dff3d76a\",\n",
    "        \"full_name\": \"Tlalpan, Distrito Federal\",\n",
    "        \"name\": \"Tlalpan\",\n",
    "        \"place_type\": \"city\",\n",
    "        \"id\": \"2a376531dff3d76a\",\n",
    "        \"country\": \"M√©xico\",\n",
    "        \"geo\": {\n",
    "            \"type\": \"Feature\",\n",
    "            \"bbox\": [-99.315748, 19.087511, -99.1009804, 19.311459],\n",
    "            \"properties\": {}\n",
    "        },\n",
    "        \"country_code\": \"MX\"\n",
    "    },\n",
    "\n",
    "    \"author_id\": \"114573824\",\n",
    "    \"author\": {\n",
    "        \"verified\": false,\n",
    "        \"protected\": false,\n",
    "        \"profile_image_url\": \"https://pbs.twimg.com/profile_images/1569749213822570500/Xf8yCPws_normal.jpg\",\n",
    "        \"created_at\": \"2010-02-15T21:50:44.000Z\",\n",
    "        \"pinned_tweet_id\": \"1588664226729832448\",\n",
    "        \"url\": \"https://t.co/E1sEHNlBkT\",\n",
    "        \"description\": \"Periodista mexicano üóû @aztecanoticias\",\n",
    "        \"public_metrics\": {\n",
    "            \"followers_count\": 11893,\n",
    "            \"following_count\": 732,\n",
    "            \"tweet_count\": 24530,\n",
    "            \"listed_count\": 25\n",
    "        },\n",
    "        \"entities\": {\n",
    "            \"url\": {\n",
    "                \"urls\": [{\n",
    "                    \"start\": 0,\n",
    "                    \"end\": 23,\n",
    "                    \"url\": \"https://t.co/E1sEHNlBkT\",\n",
    "                    \"expanded_url\": \"http://www.facebook.com/OtonielMart√≠nez\",\n",
    "                    \"display_url\": \"facebook.com/OtonielMart√≠nez\"\n",
    "                }]\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"mentions\": [{\n",
    "                    \"start\": 22,\n",
    "                    \"end\": 37,\n",
    "                    \"username\": \"aztecanoticias\"\n",
    "                }]\n",
    "            }\n",
    "        },\n",
    "        \"name\": \"OTONIEL MART√çNEZ\",\n",
    "        \"username\": \"_otomartinez\",\n",
    "        \"location\": \"M√âXICO\",\n",
    "        \"id\": \"114573824\"\n",
    "    },\n",
    "\n",
    "    \"public_metrics\": {\n",
    "        \"retweet_count\": 4,\n",
    "        \"reply_count\": 1,\n",
    "        \"like_count\": 19,\n",
    "        \"quote_count\": 0\n",
    "    },\n",
    "    \"reply_settings\": \"everyone\",\n",
    "    \"attachments\": {\n",
    "        \"media_keys\": [\"3_1589089086119333889\", \"3_1589089086245146626\"],\n",
    "        \"media\": [{\n",
    "            \"type\": \"photo\",\n",
    "            \"media_key\": \"3_1589089086119333889\",\n",
    "            \"width\": 774,\n",
    "            \"url\": \"https://pbs.twimg.com/media/Fg2UGr6X0AEKzBx.jpg\",\n",
    "            \"height\": 1024\n",
    "        }, {\n",
    "            \"type\": \"photo\",\n",
    "            \"media_key\": \"3_1589089086245146626\",\n",
    "            \"width\": 731,\n",
    "            \"url\": \"https://pbs.twimg.com/media/Fg2UGsYXkAIC8S4.jpg\",\n",
    "            \"height\": 1024\n",
    "        }]\n",
    "    },\n",
    "\n",
    "    \"entities\": {\n",
    "        \"annotations\": [{\n",
    "            \"start\": 70,\n",
    "            \"end\": 75,\n",
    "            \"probability\": 0.9717,\n",
    "            \"type\": \"Organization\",\n",
    "            \"normalized_text\": \"M√©xico\"\n",
    "        }],\n",
    "        \"urls\": [{\n",
    "            \"start\": 246,\n",
    "            \"end\": 269,\n",
    "            \"url\": \"https://t.co/JuUjRB3oQk\",\n",
    "            \"expanded_url\": \"https://twitter.com/_otomartinez/status/1589089368777850880/photo/1\",\n",
    "            \"display_url\": \"pic.twitter.com/JuUjRB3oQk\",\n",
    "            \"media_key\": \"3_1589089086119333889\"\n",
    "        }, {\n",
    "            \"start\": 246,\n",
    "            \"end\": 269,\n",
    "            \"url\": \"https://t.co/JuUjRB3oQk\",\n",
    "            \"expanded_url\": \"https://twitter.com/_otomartinez/status/1589089368777850880/photo/1\",\n",
    "            \"display_url\": \"pic.twitter.com/JuUjRB3oQk\",\n",
    "            \"media_key\": \"3_1589089086245146626\"\n",
    "        }],\n",
    "        \"hashtags\": [{\n",
    "            \"start\": 69,\n",
    "            \"end\": 76,\n",
    "            \"tag\": \"M√©xico\"\n",
    "        }],\n",
    "        \"mentions\": [{\n",
    "            \"start\": 146,\n",
    "            \"end\": 155,\n",
    "            \"username\": \"INAMI_mx\",\n",
    "            \"id\": \"1300283125\",\n",
    "            \"verified\": true,\n",
    "            \"protected\": false,\n",
    "            \"profile_image_url\": \"https://pbs.twimg.com/profile_images/1542852123821424640/exw6RvmZ_normal.jpg\",\n",
    "            \"created_at\": \"2013-03-25T17:02:40.000Z\",\n",
    "            \"pinned_tweet_id\": \"1589088702302945280\",\n",
    "            \"url\": \"https://t.co/aMyjqh7MV3\",\n",
    "            \"description\": \"Instituto Nacional de Migraci√≥n\",\n",
    "            \"public_metrics\": {\n",
    "                \"followers_count\": 52464,\n",
    "                \"following_count\": 656,\n",
    "                \"tweet_count\": 27656,\n",
    "                \"listed_count\": 334\n",
    "            },\n",
    "            \"entities\": {\n",
    "                \"url\": {\n",
    "                    \"urls\": [{\n",
    "                        \"start\": 0,\n",
    "                        \"end\": 23,\n",
    "                        \"url\": \"https://t.co/aMyjqh7MV3\",\n",
    "                        \"expanded_url\": \"https://www.gob.mx/inm\",\n",
    "                        \"display_url\": \"gob.mx/inm\"\n",
    "                    }]\n",
    "                }\n",
    "            },\n",
    "            \"name\": \"INM\",\n",
    "            \"location\": \"M√©xico\"\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-citizenship",
   "metadata": {},
   "source": [
    "## Como manipular el JSONL a un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-reach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BID",
   "language": "python",
   "name": "bid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
